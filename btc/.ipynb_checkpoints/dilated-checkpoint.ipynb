{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "def convertTimeStampToUTC(data):\n",
    "    \n",
    "    return datetime.fromtimestamp(data)\n",
    "  \n",
    "def getURL(apiKey, cryptoSymbol, frequency, timestamp, maxRecords):\n",
    "    \n",
    "    API_KEY = '&api_key=' + str(apiKey)\n",
    "    limit = '&limit=' + str(maxRecords)\n",
    "    header = 'fsym=' + str(cryptoSymbol)\n",
    "    currency = '&tsym=USD'\n",
    "\n",
    "    if (timestamp == \"\"):\n",
    "        url = 'https://min-api.cryptocompare.com/data/v2/histo' + frequency + '?' + header\n",
    "    \n",
    "    else:  \n",
    "        url = 'https://min-api.cryptocompare.com/data/v2/histo' + frequency + '?' + header + '&toTs=' + str(timestamp)\n",
    "\n",
    "    return url + currency + API_KEY + limit  \n",
    "\n",
    "\n",
    "def getCryptoData(apiKey, crypto, frequency, numOfRequests, maxRecords):\n",
    "    \n",
    "    numOfRequests = 5\n",
    "    data = pd.DataFrame()\n",
    "    nextTimeStamp = 0\n",
    "\n",
    "    for i in range(0, numOfRequests):\n",
    "        if (nextTimeStamp == 0):\n",
    "            url = getURL(apiKey, crypto, frequency, \"\", maxRecords)\n",
    "    \n",
    "        else:\n",
    "            url = getURL(apiKey, crypto, frequency, nextTimeStamp, maxRecords)\n",
    "\n",
    "        response = requests.get(url).json()\n",
    "        partialData = pd.DataFrame(response.get('Data').get('Data'))\n",
    "        nextTimeStamp = partialData['time'].min() - 1\n",
    "        data = pd.concat([data, partialData])\n",
    "\n",
    "    data = data.sort_values(by = ['time'],ignore_index = True)\n",
    "    data['timeUTC'] = data['time'].apply(convertTimeStampToUTC)\n",
    "    data = data.reset_index(drop = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from pandas import Series\n",
    "import math\n",
    "import numpy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data = pd.read_csv('bitcoin2000.csv')\n",
    "data1 = data.set_index(pd.DatetimeIndex(data['timeUTC']))[['close']]\n",
    "data2=data.set_index(pd.DatetimeIndex(data['timeUTC']))[['open']]\n",
    "data3=data.set_index(pd.DatetimeIndex(data['timeUTC']))[['close','open','low','high']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-12 19:00:00</th>\n",
       "      <td>7166.14</td>\n",
       "      <td>7170.18</td>\n",
       "      <td>7149.15</td>\n",
       "      <td>7219.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 20:00:00</th>\n",
       "      <td>7199.79</td>\n",
       "      <td>7166.14</td>\n",
       "      <td>7166.03</td>\n",
       "      <td>7208.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 21:00:00</th>\n",
       "      <td>7199.97</td>\n",
       "      <td>7199.79</td>\n",
       "      <td>7194.87</td>\n",
       "      <td>7222.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 22:00:00</th>\n",
       "      <td>7201.38</td>\n",
       "      <td>7199.97</td>\n",
       "      <td>7187.29</td>\n",
       "      <td>7211.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 23:00:00</th>\n",
       "      <td>7173.50</td>\n",
       "      <td>7201.38</td>\n",
       "      <td>7152.84</td>\n",
       "      <td>7229.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 11:00:00</th>\n",
       "      <td>33675.45</td>\n",
       "      <td>33450.01</td>\n",
       "      <td>33257.39</td>\n",
       "      <td>33847.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 12:00:00</th>\n",
       "      <td>33590.20</td>\n",
       "      <td>33675.45</td>\n",
       "      <td>33505.50</td>\n",
       "      <td>33779.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 13:00:00</th>\n",
       "      <td>33576.82</td>\n",
       "      <td>33590.20</td>\n",
       "      <td>33404.87</td>\n",
       "      <td>33945.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 14:00:00</th>\n",
       "      <td>33803.02</td>\n",
       "      <td>33576.82</td>\n",
       "      <td>33467.87</td>\n",
       "      <td>33907.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 15:00:00</th>\n",
       "      <td>33897.06</td>\n",
       "      <td>33803.02</td>\n",
       "      <td>33739.63</td>\n",
       "      <td>33950.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10005 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close      open       low      high\n",
       "timeUTC                                                    \n",
       "2019-12-12 19:00:00   7166.14   7170.18   7149.15   7219.93\n",
       "2019-12-12 20:00:00   7199.79   7166.14   7166.03   7208.36\n",
       "2019-12-12 21:00:00   7199.97   7199.79   7194.87   7222.43\n",
       "2019-12-12 22:00:00   7201.38   7199.97   7187.29   7211.80\n",
       "2019-12-12 23:00:00   7173.50   7201.38   7152.84   7229.22\n",
       "...                       ...       ...       ...       ...\n",
       "2021-02-01 11:00:00  33675.45  33450.01  33257.39  33847.46\n",
       "2021-02-01 12:00:00  33590.20  33675.45  33505.50  33779.76\n",
       "2021-02-01 13:00:00  33576.82  33590.20  33404.87  33945.41\n",
       "2021-02-01 14:00:00  33803.02  33576.82  33467.87  33907.89\n",
       "2021-02-01 15:00:00  33897.06  33803.02  33739.63  33950.67\n",
       "\n",
       "[10005 rows x 4 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-12 19:00:00</th>\n",
       "      <td>7166.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 20:00:00</th>\n",
       "      <td>7199.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 21:00:00</th>\n",
       "      <td>7199.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 22:00:00</th>\n",
       "      <td>7201.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 23:00:00</th>\n",
       "      <td>7173.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 11:00:00</th>\n",
       "      <td>33675.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 12:00:00</th>\n",
       "      <td>33590.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 13:00:00</th>\n",
       "      <td>33576.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 14:00:00</th>\n",
       "      <td>33803.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 15:00:00</th>\n",
       "      <td>33897.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10005 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close\n",
       "timeUTC                      \n",
       "2019-12-12 19:00:00   7166.14\n",
       "2019-12-12 20:00:00   7199.79\n",
       "2019-12-12 21:00:00   7199.97\n",
       "2019-12-12 22:00:00   7201.38\n",
       "2019-12-12 23:00:00   7173.50\n",
       "...                       ...\n",
       "2021-02-01 11:00:00  33675.45\n",
       "2021-02-01 12:00:00  33590.20\n",
       "2021-02-01 13:00:00  33576.82\n",
       "2021-02-01 14:00:00  33803.02\n",
       "2021-02-01 15:00:00  33897.06\n",
       "\n",
       "[10005 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-12 19:00:00</th>\n",
       "      <td>7166.14</td>\n",
       "      <td>7219.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 20:00:00</th>\n",
       "      <td>7199.79</td>\n",
       "      <td>7208.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 21:00:00</th>\n",
       "      <td>7199.97</td>\n",
       "      <td>7222.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 22:00:00</th>\n",
       "      <td>7201.38</td>\n",
       "      <td>7211.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 23:00:00</th>\n",
       "      <td>7173.50</td>\n",
       "      <td>7229.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 11:00:00</th>\n",
       "      <td>33675.45</td>\n",
       "      <td>33847.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 12:00:00</th>\n",
       "      <td>33590.20</td>\n",
       "      <td>33779.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 13:00:00</th>\n",
       "      <td>33576.82</td>\n",
       "      <td>33945.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 14:00:00</th>\n",
       "      <td>33803.02</td>\n",
       "      <td>33907.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 15:00:00</th>\n",
       "      <td>33897.06</td>\n",
       "      <td>33950.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10005 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close      high\n",
       "timeUTC                                \n",
       "2019-12-12 19:00:00   7166.14   7219.93\n",
       "2019-12-12 20:00:00   7199.79   7208.36\n",
       "2019-12-12 21:00:00   7199.97   7222.43\n",
       "2019-12-12 22:00:00   7201.38   7211.80\n",
       "2019-12-12 23:00:00   7173.50   7229.22\n",
       "...                       ...       ...\n",
       "2021-02-01 11:00:00  33675.45  33847.46\n",
       "2021-02-01 12:00:00  33590.20  33779.76\n",
       "2021-02-01 13:00:00  33576.82  33945.41\n",
       "2021-02-01 14:00:00  33803.02  33907.89\n",
       "2021-02-01 15:00:00  33897.06  33950.67\n",
       "\n",
       "[10005 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3[['close','high']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "values = data1['close'].values.reshape(-1,1)\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07853526],\n",
       "       [0.07943861],\n",
       "       [0.07944345],\n",
       "       ...,\n",
       "       [0.787544  ],\n",
       "       [0.7936165 ],\n",
       "       [0.79614097]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004 2001\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(scaled) * 0.8)\n",
    "val_size = len(scaled) - train_size\n",
    "\n",
    "test_size = len(scaled) - train_size\n",
    "train, test = scaled[0:train_size,:], scaled[train_size:len(scaled),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07853526, 0.07943861, 0.07944345, 0.07948129, 0.07873285,\n",
       "        0.07950117],\n",
       "       [0.07943861, 0.07944345, 0.07948129, 0.07873285, 0.07950117,\n",
       "        0.07977526],\n",
       "       [0.07944345, 0.07948129, 0.07873285, 0.07950117, 0.07977526,\n",
       "        0.07980587],\n",
       "       ...,\n",
       "       [0.2969556 , 0.2913795 , 0.28947163, 0.29217225, 0.2951843 ,\n",
       "        0.29748446],\n",
       "       [0.2913795 , 0.28947163, 0.29217225, 0.2951843 , 0.29748446,\n",
       "        0.29883933],\n",
       "       [0.28947163, 0.29217225, 0.2951843 , 0.29748446, 0.29883933,\n",
       "        0.29901117]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    print(len(dataY))\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "values = scaled.copy()\n",
    "n_train_hours = int(len(values) * 0.8)\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7998\n",
      "1995\n"
     ]
    }
   ],
   "source": [
    "look_back = 6\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "#valX, valY = create_dataset(val, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7992\n",
      "1989\n"
     ]
    }
   ],
   "source": [
    "look_back = 12\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "#valX, valY = create_dataset(val, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7998, 6)\n",
      "(7998,)\n",
      "(1995, 6)\n",
      "(1995,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "#print(valX.shape)\n",
    "#print(valY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#valX = np.reshape(valX, (valX.shape[0], 1, valX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0],testX.shape[1], 1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7992 samples, validate on 1989 samples\n",
      "Epoch 1/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 0.0222\n",
      "Epoch 00001: val_loss improved from inf to 0.92866, saving model to saved_weights/dilated1.hdf5\n",
      "7992/7992 [==============================] - 6s 767us/sample - loss: 0.0219 - val_loss: 0.9287\n",
      "Epoch 2/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 3.7155e-04\n",
      "Epoch 00002: val_loss improved from 0.92866 to 0.46261, saving model to saved_weights/dilated1.hdf5\n",
      "7992/7992 [==============================] - 3s 391us/sample - loss: 3.6851e-04 - val_loss: 0.4626\n",
      "Epoch 3/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 3.8335e-04\n",
      "Epoch 00003: val_loss improved from 0.46261 to 0.02065, saving model to saved_weights/dilated1.hdf5\n",
      "7992/7992 [==============================] - 3s 400us/sample - loss: 3.8230e-04 - val_loss: 0.0207\n",
      "Epoch 4/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.9806e-04\n",
      "Epoch 00004: val_loss improved from 0.02065 to 0.00187, saving model to saved_weights/dilated1.hdf5\n",
      "7992/7992 [==============================] - 3s 391us/sample - loss: 1.9631e-04 - val_loss: 0.0019\n",
      "Epoch 5/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.5179e-04\n",
      "Epoch 00005: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 382us/sample - loss: 1.5652e-04 - val_loss: 0.0180\n",
      "Epoch 6/50\n",
      "7936/7992 [============================>.] - ETA: 0s - loss: 1.9042e-04\n",
      "Epoch 00006: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 427us/sample - loss: 1.9046e-04 - val_loss: 0.0068\n",
      "Epoch 7/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 1.7227e-04\n",
      "Epoch 00007: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 403us/sample - loss: 1.7738e-04 - val_loss: 0.0183\n",
      "Epoch 8/50\n",
      "7936/7992 [============================>.] - ETA: 0s - loss: 1.6836e-04\n",
      "Epoch 00008: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 397us/sample - loss: 1.6764e-04 - val_loss: 0.0076\n",
      "Epoch 9/50\n",
      "7968/7992 [============================>.] - ETA: 0s - loss: 1.5758e-04\n",
      "Epoch 00009: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 398us/sample - loss: 1.5958e-04 - val_loss: 0.0153\n",
      "Epoch 10/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 1.5571e-04\n",
      "Epoch 00010: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 394us/sample - loss: 1.5355e-04 - val_loss: 0.0155\n",
      "Epoch 11/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 1.1805e-04\n",
      "Epoch 00011: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 391us/sample - loss: 1.1940e-04 - val_loss: 0.0030\n",
      "Epoch 12/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.4807e-04\n",
      "Epoch 00012: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 378us/sample - loss: 1.4801e-04 - val_loss: 0.0028\n",
      "Epoch 13/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 2.1106e-04\n",
      "Epoch 00013: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 380us/sample - loss: 2.1023e-04 - val_loss: 0.0052\n",
      "Epoch 14/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.9182e-04\n",
      "Epoch 00014: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 425us/sample - loss: 1.9051e-04 - val_loss: 0.0072\n",
      "Epoch 15/50\n",
      "7904/7992 [============================>.] - ETA: 0s - loss: 1.6797e-04\n",
      "Epoch 00015: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 393us/sample - loss: 1.7062e-04 - val_loss: 0.0058\n",
      "Epoch 16/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.5299e-04\n",
      "Epoch 00016: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 386us/sample - loss: 1.5151e-04 - val_loss: 0.0134\n",
      "Epoch 17/50\n",
      "7904/7992 [============================>.] - ETA: 0s - loss: 1.3872e-04\n",
      "Epoch 00017: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 407us/sample - loss: 1.3827e-04 - val_loss: 0.0319\n",
      "Epoch 18/50\n",
      "7904/7992 [============================>.] - ETA: 0s - loss: 1.0154e-04\n",
      "Epoch 00018: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 396us/sample - loss: 1.0250e-04 - val_loss: 0.0121\n",
      "Epoch 19/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 1.0223e-04\n",
      "Epoch 00019: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 381us/sample - loss: 1.0182e-04 - val_loss: 0.0082\n",
      "Epoch 20/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.2360e-04\n",
      "Epoch 00020: val_loss did not improve from 0.00187\n",
      "7992/7992 [==============================] - 3s 418us/sample - loss: 1.2438e-04 - val_loss: 0.0201\n",
      "Epoch 21/50\n",
      "7936/7992 [============================>.] - ETA: 0s - loss: 1.3043e-04\n",
      "Epoch 00021: val_loss improved from 0.00187 to 0.00129, saving model to saved_weights/dilated1.hdf5\n",
      "7992/7992 [==============================] - 4s 499us/sample - loss: 1.3078e-04 - val_loss: 0.0013\n",
      "Epoch 22/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.6807e-04\n",
      "Epoch 00022: val_loss did not improve from 0.00129\n",
      "7992/7992 [==============================] - 3s 378us/sample - loss: 1.6739e-04 - val_loss: 0.0389\n",
      "Epoch 23/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.0562e-04\n",
      "Epoch 00023: val_loss improved from 0.00129 to 0.00121, saving model to saved_weights/dilated1.hdf5\n",
      "7992/7992 [==============================] - 3s 394us/sample - loss: 1.0758e-04 - val_loss: 0.0012\n",
      "Epoch 24/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.2881e-04\n",
      "Epoch 00024: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 385us/sample - loss: 1.2760e-04 - val_loss: 0.0043\n",
      "Epoch 25/50\n",
      "7904/7992 [============================>.] - ETA: 0s - loss: 1.4773e-04\n",
      "Epoch 00025: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 403us/sample - loss: 1.4641e-04 - val_loss: 0.0099\n",
      "Epoch 26/50\n",
      "7968/7992 [============================>.] - ETA: 0s - loss: 1.3307e-04\n",
      "Epoch 00026: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 386us/sample - loss: 1.3277e-04 - val_loss: 0.0048\n",
      "Epoch 27/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.4174e-04\n",
      "Epoch 00027: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 379us/sample - loss: 1.4082e-04 - val_loss: 0.0118\n",
      "Epoch 28/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.6930e-04\n",
      "Epoch 00028: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 379us/sample - loss: 1.6912e-04 - val_loss: 0.0154\n",
      "Epoch 29/50\n",
      "7968/7992 [============================>.] - ETA: 0s - loss: 1.6195e-04\n",
      "Epoch 00029: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 386us/sample - loss: 1.6157e-04 - val_loss: 0.0110\n",
      "Epoch 30/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.1880e-04\n",
      "Epoch 00030: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 390us/sample - loss: 1.1931e-04 - val_loss: 0.0120\n",
      "Epoch 31/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 1.2003e-04\n",
      "Epoch 00031: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 380us/sample - loss: 1.1877e-04 - val_loss: 0.0097\n",
      "Epoch 32/50\n",
      "7904/7992 [============================>.] - ETA: 0s - loss: 1.5294e-04\n",
      "Epoch 00032: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 396us/sample - loss: 1.5314e-04 - val_loss: 0.1283\n",
      "Epoch 33/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 1.7272e-04\n",
      "Epoch 00033: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 432us/sample - loss: 1.7081e-04 - val_loss: 0.0200\n",
      "Epoch 34/50\n",
      "7968/7992 [============================>.] - ETA: 0s - loss: 1.5734e-04\n",
      "Epoch 00034: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 401us/sample - loss: 1.5733e-04 - val_loss: 0.0281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.2001e-04\n",
      "Epoch 00035: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 388us/sample - loss: 1.1995e-04 - val_loss: 0.0276\n",
      "Epoch 36/50\n",
      "7936/7992 [============================>.] - ETA: 0s - loss: 8.3519e-05\n",
      "Epoch 00036: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 386us/sample - loss: 8.3549e-05 - val_loss: 0.0363\n",
      "Epoch 37/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 8.8477e-05\n",
      "Epoch 00037: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 389us/sample - loss: 8.8315e-05 - val_loss: 0.0778\n",
      "Epoch 38/50\n",
      "7936/7992 [============================>.] - ETA: 0s - loss: 7.6416e-05\n",
      "Epoch 00038: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 386us/sample - loss: 7.6197e-05 - val_loss: 0.0346\n",
      "Epoch 39/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 7.2113e-05\n",
      "Epoch 00039: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 377us/sample - loss: 7.2102e-05 - val_loss: 0.0416\n",
      "Epoch 40/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 6.7523e-05\n",
      "Epoch 00040: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 385us/sample - loss: 6.8340e-05 - val_loss: 0.0514\n",
      "Epoch 41/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 4.9833e-05\n",
      "Epoch 00041: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 395us/sample - loss: 4.9550e-05 - val_loss: 0.0727\n",
      "Epoch 42/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 8.0369e-05\n",
      "Epoch 00042: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 380us/sample - loss: 7.9239e-05 - val_loss: 0.0568\n",
      "Epoch 43/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 1.0349e-04\n",
      "Epoch 00043: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 380us/sample - loss: 1.0433e-04 - val_loss: 0.0419\n",
      "Epoch 44/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 4.5192e-05\n",
      "Epoch 00044: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 388us/sample - loss: 4.5325e-05 - val_loss: 0.0920\n",
      "Epoch 45/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 4.4810e-05\n",
      "Epoch 00045: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 420us/sample - loss: 4.4690e-05 - val_loss: 0.0625\n",
      "Epoch 46/50\n",
      "7968/7992 [============================>.] - ETA: 0s - loss: 5.1444e-05\n",
      "Epoch 00046: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 436us/sample - loss: 5.1709e-05 - val_loss: 0.0839\n",
      "Epoch 47/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 3.7416e-05\n",
      "Epoch 00047: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 386us/sample - loss: 3.8758e-05 - val_loss: 0.0935\n",
      "Epoch 48/50\n",
      "7840/7992 [============================>.] - ETA: 0s - loss: 3.2204e-05\n",
      "Epoch 00048: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 382us/sample - loss: 3.6339e-05 - val_loss: 0.1097\n",
      "Epoch 49/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 2.1419e-04\n",
      "Epoch 00049: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 381us/sample - loss: 2.1199e-04 - val_loss: 0.2150\n",
      "Epoch 50/50\n",
      "7872/7992 [============================>.] - ETA: 0s - loss: 5.3574e-05\n",
      "Epoch 00050: val_loss did not improve from 0.00121\n",
      "7992/7992 [==============================] - 3s 392us/sample - loss: 5.3253e-05 - val_loss: 0.1435\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "dilations = 6\n",
    "seq_length=6\n",
    "\n",
    "class TCNBlock(k.Model):\n",
    "    def __init__(self, dilation, seq_length):\n",
    "        super(TCNBlock, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.convolution0 = k.layers.Conv1D(32, kernel_size=3, strides=1, padding='causal', dilation_rate=dilation)\n",
    "        self.BatchNorm0 = k.layers.BatchNormalization()\n",
    "        self.relu0 = k.layers.ReLU()\n",
    "        self.dropout0 = k.layers.Dropout(rate=0.5)\n",
    "\n",
    "        self.convolution1 = k.layers.Conv1D(64, kernel_size=4, strides=1, padding='causal', dilation_rate=dilation)\n",
    "        self.BatchNorm1 = k.layers.BatchNormalization()\n",
    "        self.relu1 = k.layers.ReLU()\n",
    "        self.dropout1 = k.layers.Dropout(rate=0.5)\n",
    "        self.residual = k.layers.Conv1D(1, kernel_size=1, padding='same')\n",
    "\n",
    "\n",
    "    def build_block(self, dilation, training=False):\n",
    "        inputs = k.Input(shape=(12, 1))\n",
    "        output_layer1 = self.convolution0(inputs)\n",
    "        output_layer2 = self.BatchNorm0(output_layer1)\n",
    "        output_layer3 = self.relu0(output_layer2)\n",
    "        output_layer4 = self.dropout0(output_layer3, training)\n",
    "        output_layer5 = self.convolution1(output_layer4)\n",
    "        output_layer6 = self.BatchNorm1(output_layer5)\n",
    "        output_layer7 = self.relu1(output_layer6)\n",
    "        output = self.dropout1(output_layer7, training)\n",
    "        residual = self.residual(output)\n",
    "        outputs = k.layers.add([inputs, residual])\n",
    "\n",
    "\n",
    "        return k.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    mdl = k.models.Sequential()\n",
    "    for dilation in range(dilations):\n",
    "        dilation_actual = int(np.power(2, dilation))\n",
    "        block = TCNBlock(dilation_actual, seq_length).build_block(dilation_actual)\n",
    "        mdl.add(block)\n",
    "    mdl.add(MaxPooling1D(pool_size=2))\n",
    "    mdl.add(Flatten())\n",
    "    mdl.add(Dense(100))\n",
    "    mdl.add(Dense(1))\n",
    "\n",
    "    return mdl\n",
    "\n",
    "\n",
    "Model_complete = build_model()\n",
    "opt = k.optimizers.Adam(learning_rate=learning_rate)\n",
    "Model_complete.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "filepath=\"saved_weights/dilated1.hdf5\"\n",
    "mc = ModelCheckpoint(filepath, monitor='val_loss',mode='min',save_best_only='True', verbose=1)\n",
    "\n",
    "# Train Model\n",
    "training_process = Model_complete.fit(trainX, trainY, epochs=epochs, verbose=1, validation_data=(testX, testY),batch_size=32, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7998 samples, validate on 1995 samples\n",
      "Epoch 1/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.0419e-04\n",
      "Epoch 00001: val_loss improved from 0.00121 to 0.00031, saving model to saved_weights/dilated1.hdf5\n",
      "7998/7998 [==============================] - 4s 554us/sample - loss: 2.0255e-04 - val_loss: 3.0522e-04\n",
      "Epoch 2/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 5.9159e-05\n",
      "Epoch 00002: val_loss did not improve from 0.00031\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 5.8828e-05 - val_loss: 0.0034\n",
      "Epoch 3/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 6.7146e-05\n",
      "Epoch 00003: val_loss did not improve from 0.00031\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 6.8500e-05 - val_loss: 0.0029\n",
      "Epoch 4/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 7.5084e-05\n",
      "Epoch 00004: val_loss did not improve from 0.00031\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 7.4838e-05 - val_loss: 0.0021\n",
      "Epoch 5/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 5.2883e-05\n",
      "Epoch 00005: val_loss did not improve from 0.00031\n",
      "7998/7998 [==============================] - 3s 329us/sample - loss: 5.2492e-05 - val_loss: 0.0029\n",
      "Epoch 6/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 3.2475e-05\n",
      "Epoch 00006: val_loss improved from 0.00031 to 0.00024, saving model to saved_weights/dilated1.hdf5\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 3.2617e-05 - val_loss: 2.4449e-04\n",
      "Epoch 7/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.7538e-05\n",
      "Epoch 00007: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 3.7947e-05 - val_loss: 4.0880e-04\n",
      "Epoch 8/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 4.9127e-05\n",
      "Epoch 00008: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 4s 469us/sample - loss: 4.9278e-05 - val_loss: 0.0132\n",
      "Epoch 9/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.9275e-05\n",
      "Epoch 00009: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 321us/sample - loss: 6.8757e-05 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 4.0435e-05\n",
      "Epoch 00010: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 4s 539us/sample - loss: 4.0295e-05 - val_loss: 5.0372e-04\n",
      "Epoch 11/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 3.2688e-05\n",
      "Epoch 00011: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 3.2635e-05 - val_loss: 0.0021\n",
      "Epoch 12/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.1799e-05\n",
      "Epoch 00012: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 3.1800e-05 - val_loss: 0.0020\n",
      "Epoch 13/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.3040e-05\n",
      "Epoch 00013: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 3.4371e-05 - val_loss: 2.9427e-04\n",
      "Epoch 14/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 6.6351e-05\n",
      "Epoch 00014: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 6.5485e-05 - val_loss: 0.0034\n",
      "Epoch 15/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 3.0865e-05\n",
      "Epoch 00015: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 348us/sample - loss: 3.0970e-05 - val_loss: 0.0020\n",
      "Epoch 16/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.6476e-05\n",
      "Epoch 00016: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 4s 496us/sample - loss: 2.6396e-05 - val_loss: 6.6513e-04\n",
      "Epoch 17/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 2.7743e-05\n",
      "Epoch 00017: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 2.7648e-05 - val_loss: 8.1497e-04\n",
      "Epoch 18/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 4.2476e-05\n",
      "Epoch 00018: val_loss improved from 0.00024 to 0.00018, saving model to saved_weights/dilated1.hdf5\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 4.2230e-05 - val_loss: 1.7624e-04\n",
      "Epoch 19/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.2006e-05\n",
      "Epoch 00019: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 2.1960e-05 - val_loss: 2.7766e-04\n",
      "Epoch 20/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 3.7363e-05\n",
      "Epoch 00020: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 336us/sample - loss: 3.7213e-05 - val_loss: 0.0071\n",
      "Epoch 21/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.5129e-05\n",
      "Epoch 00021: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 2.5340e-05 - val_loss: 0.0029\n",
      "Epoch 22/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.9314e-05\n",
      "Epoch 00022: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 394us/sample - loss: 2.9297e-05 - val_loss: 2.0551e-04\n",
      "Epoch 23/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 2.1473e-05\n",
      "Epoch 00023: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 460us/sample - loss: 2.1593e-05 - val_loss: 4.8408e-04\n",
      "Epoch 24/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.3970e-05\n",
      "Epoch 00024: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 1.4063e-05 - val_loss: 2.8729e-04\n",
      "Epoch 25/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 4.8082e-05\n",
      "Epoch 00025: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 310us/sample - loss: 4.7645e-05 - val_loss: 0.0101\n",
      "Epoch 26/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.7677e-05\n",
      "Epoch 00026: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 1.7521e-05 - val_loss: 0.0032\n",
      "Epoch 27/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.4997e-05\n",
      "Epoch 00027: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 1.4975e-05 - val_loss: 3.6179e-04\n",
      "Epoch 28/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 2.2850e-05\n",
      "Epoch 00028: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 347us/sample - loss: 2.3112e-05 - val_loss: 0.0017\n",
      "Epoch 29/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.9817e-05\n",
      "Epoch 00029: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 322us/sample - loss: 1.9731e-05 - val_loss: 0.0032\n",
      "Epoch 30/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.8653e-05\n",
      "Epoch 00030: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.8662e-05 - val_loss: 5.8230e-04\n",
      "Epoch 31/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.7440e-05\n",
      "Epoch 00031: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 1.7320e-05 - val_loss: 0.0012\n",
      "Epoch 32/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 2.0995e-05\n",
      "Epoch 00032: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 357us/sample - loss: 2.1191e-05 - val_loss: 0.0086\n",
      "Epoch 33/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 3.3114e-05\n",
      "Epoch 00033: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 337us/sample - loss: 3.3138e-05 - val_loss: 0.0143\n",
      "Epoch 34/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.3519e-05\n",
      "Epoch 00034: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.3618e-05 - val_loss: 3.5333e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.8858e-05\n",
      "Epoch 00035: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.8886e-05 - val_loss: 0.0024\n",
      "Epoch 36/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.1307e-05\n",
      "Epoch 00036: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 331us/sample - loss: 2.1383e-05 - val_loss: 0.0014\n",
      "Epoch 37/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.2576e-05\n",
      "Epoch 00037: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 346us/sample - loss: 3.2456e-05 - val_loss: 0.0026\n",
      "Epoch 38/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.3723e-05\n",
      "Epoch 00038: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 630us/sample - loss: 2.4206e-05 - val_loss: 0.0033\n",
      "Epoch 39/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.6728e-05\n",
      "Epoch 00039: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 630us/sample - loss: 1.6952e-05 - val_loss: 4.5279e-04\n",
      "Epoch 40/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.3824e-05\n",
      "Epoch 00040: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 380us/sample - loss: 1.3650e-05 - val_loss: 3.3939e-04\n",
      "Epoch 41/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.2839e-05\n",
      "Epoch 00041: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 469us/sample - loss: 2.2818e-05 - val_loss: 8.5795e-04\n",
      "Epoch 42/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 2.3744e-05\n",
      "Epoch 00042: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 419us/sample - loss: 2.3481e-05 - val_loss: 3.4646e-04\n",
      "Epoch 43/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4699e-05\n",
      "Epoch 00043: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 1.4691e-05 - val_loss: 0.0012\n",
      "Epoch 44/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.4417e-05\n",
      "Epoch 00044: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 355us/sample - loss: 1.5016e-05 - val_loss: 0.0032\n",
      "Epoch 45/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.0805e-05\n",
      "Epoch 00045: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 632us/sample - loss: 2.0661e-05 - val_loss: 9.0863e-04\n",
      "Epoch 46/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.1515e-05\n",
      "Epoch 00046: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 577us/sample - loss: 2.1448e-05 - val_loss: 0.0025\n",
      "Epoch 47/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.4498e-05\n",
      "Epoch 00047: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 397us/sample - loss: 1.4435e-05 - val_loss: 0.0045\n",
      "Epoch 48/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.0385e-05\n",
      "Epoch 00048: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 2.0502e-05 - val_loss: 0.0071\n",
      "Epoch 49/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.7734e-05\n",
      "Epoch 00049: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 441us/sample - loss: 1.7653e-05 - val_loss: 0.0027\n",
      "Epoch 50/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.5485e-05\n",
      "Epoch 00050: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.5429e-05 - val_loss: 0.0057\n",
      "Epoch 51/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.7438e-05\n",
      "Epoch 00051: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 3.7114e-05 - val_loss: 0.0017\n",
      "Epoch 52/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.6991e-05\n",
      "Epoch 00052: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 1.6935e-05 - val_loss: 0.0042\n",
      "Epoch 53/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.7492e-05\n",
      "Epoch 00053: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 372us/sample - loss: 1.7486e-05 - val_loss: 0.0020\n",
      "Epoch 54/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.3509e-05\n",
      "Epoch 00054: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 1.4153e-05 - val_loss: 0.0059\n",
      "Epoch 55/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.8564e-05\n",
      "Epoch 00055: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 425us/sample - loss: 1.8413e-05 - val_loss: 0.0017\n",
      "Epoch 56/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.8002e-05\n",
      "Epoch 00056: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 568us/sample - loss: 1.7955e-05 - val_loss: 0.0042\n",
      "Epoch 57/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.9921e-05\n",
      "Epoch 00057: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 538us/sample - loss: 1.9910e-05 - val_loss: 0.0023\n",
      "Epoch 58/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.2090e-05\n",
      "Epoch 00058: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 480us/sample - loss: 2.1976e-05 - val_loss: 0.0075\n",
      "Epoch 59/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.1007e-05\n",
      "Epoch 00059: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 325us/sample - loss: 2.0956e-05 - val_loss: 0.0014\n",
      "Epoch 60/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.5247e-05\n",
      "Epoch 00060: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 360us/sample - loss: 1.5003e-05 - val_loss: 0.0055\n",
      "Epoch 61/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.5226e-05\n",
      "Epoch 00061: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.5156e-05 - val_loss: 0.0015\n",
      "Epoch 62/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.0380e-05\n",
      "Epoch 00062: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 330us/sample - loss: 1.0610e-05 - val_loss: 0.0024\n",
      "Epoch 63/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.7259e-05\n",
      "Epoch 00063: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 2.7254e-05 - val_loss: 9.0897e-04\n",
      "Epoch 64/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.4869e-05\n",
      "Epoch 00064: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 399us/sample - loss: 1.4955e-05 - val_loss: 0.0117\n",
      "Epoch 65/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.5659e-05\n",
      "Epoch 00065: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 325us/sample - loss: 1.5685e-05 - val_loss: 0.0048\n",
      "Epoch 66/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0925e-05\n",
      "Epoch 00066: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 401us/sample - loss: 1.1205e-05 - val_loss: 0.0015\n",
      "Epoch 67/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.5771e-05\n",
      "Epoch 00067: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 1.5716e-05 - val_loss: 0.0083\n",
      "Epoch 68/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.1588e-05\n",
      "Epoch 00068: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 365us/sample - loss: 1.1806e-05 - val_loss: 0.0057\n",
      "Epoch 69/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0754e-05\n",
      "Epoch 00069: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.0798e-05 - val_loss: 0.0057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2388e-05\n",
      "Epoch 00070: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.2374e-05 - val_loss: 0.0020\n",
      "Epoch 71/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.7430e-05\n",
      "Epoch 00071: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 325us/sample - loss: 1.7133e-05 - val_loss: 0.0030\n",
      "Epoch 72/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.6602e-05\n",
      "Epoch 00072: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 331us/sample - loss: 1.6492e-05 - val_loss: 0.0014\n",
      "Epoch 73/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.7608e-05\n",
      "Epoch 00073: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.7542e-05 - val_loss: 7.8225e-04\n",
      "Epoch 74/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.3600e-05\n",
      "Epoch 00074: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 1.3574e-05 - val_loss: 0.0034\n",
      "Epoch 75/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.3307e-05\n",
      "Epoch 00075: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 1.3281e-05 - val_loss: 2.8590e-04\n",
      "Epoch 76/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.2562e-05\n",
      "Epoch 00076: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 369us/sample - loss: 1.2481e-05 - val_loss: 0.0012\n",
      "Epoch 77/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.2703e-05\n",
      "Epoch 00077: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 1.2583e-05 - val_loss: 0.0016\n",
      "Epoch 78/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.5095e-05\n",
      "Epoch 00078: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 459us/sample - loss: 1.5622e-05 - val_loss: 4.8788e-04\n",
      "Epoch 79/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.6666e-05\n",
      "Epoch 00079: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 321us/sample - loss: 1.6500e-05 - val_loss: 8.1704e-04\n",
      "Epoch 80/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.2377e-05\n",
      "Epoch 00080: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 350us/sample - loss: 1.2469e-05 - val_loss: 0.0018\n",
      "Epoch 81/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.1203e-06\n",
      "Epoch 00081: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 9.1725e-06 - val_loss: 0.0030\n",
      "Epoch 82/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.6298e-05\n",
      "Epoch 00082: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 345us/sample - loss: 1.6227e-05 - val_loss: 0.0027\n",
      "Epoch 83/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.3868e-05\n",
      "Epoch 00083: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 359us/sample - loss: 2.3768e-05 - val_loss: 5.8867e-04\n",
      "Epoch 84/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 9.7626e-06\n",
      "Epoch 00084: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 489us/sample - loss: 9.7445e-06 - val_loss: 0.0048\n",
      "Epoch 85/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.5860e-06\n",
      "Epoch 00085: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 8.6368e-06 - val_loss: 0.0045\n",
      "Epoch 86/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.3037e-05\n",
      "Epoch 00086: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 320us/sample - loss: 1.3000e-05 - val_loss: 3.3652e-04\n",
      "Epoch 87/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.6086e-05\n",
      "Epoch 00087: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.6447e-05 - val_loss: 6.1233e-04\n",
      "Epoch 88/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.1834e-05\n",
      "Epoch 00088: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 1.1714e-05 - val_loss: 0.0014\n",
      "Epoch 89/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2101e-05\n",
      "Epoch 00089: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 1.2059e-05 - val_loss: 0.0015\n",
      "Epoch 90/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.0254e-05\n",
      "Epoch 00090: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 324us/sample - loss: 1.0290e-05 - val_loss: 5.0951e-04\n",
      "Epoch 91/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.8960e-05\n",
      "Epoch 00091: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.9067e-05 - val_loss: 0.0026\n",
      "Epoch 92/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.1464e-05\n",
      "Epoch 00092: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 3.1145e-05 - val_loss: 0.0043\n",
      "Epoch 93/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.0891e-06\n",
      "Epoch 00093: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 320us/sample - loss: 9.0764e-06 - val_loss: 0.0024\n",
      "Epoch 94/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.0120e-05\n",
      "Epoch 00094: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 551us/sample - loss: 1.0251e-05 - val_loss: 0.0029\n",
      "Epoch 95/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.2290e-05\n",
      "Epoch 00095: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 395us/sample - loss: 1.2225e-05 - val_loss: 0.0041\n",
      "Epoch 96/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 8.5684e-06\n",
      "Epoch 00096: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 450us/sample - loss: 8.5380e-06 - val_loss: 0.0024\n",
      "Epoch 97/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4166e-05\n",
      "Epoch 00097: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.4072e-05 - val_loss: 0.0021\n",
      "Epoch 98/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0611e-05\n",
      "Epoch 00098: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 1.0676e-05 - val_loss: 0.0078\n",
      "Epoch 99/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 9.2852e-06\n",
      "Epoch 00099: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 9.3910e-06 - val_loss: 0.0050\n",
      "Epoch 100/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.3393e-05\n",
      "Epoch 00100: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 1.3344e-05 - val_loss: 6.3234e-04\n",
      "Epoch 101/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.1575e-05\n",
      "Epoch 00101: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 1.1538e-05 - val_loss: 0.0035\n",
      "Epoch 102/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.9172e-06\n",
      "Epoch 00102: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 9.8760e-06 - val_loss: 8.6416e-04\n",
      "Epoch 103/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2296e-05\n",
      "Epoch 00103: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 310us/sample - loss: 1.2240e-05 - val_loss: 0.0020\n",
      "Epoch 104/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2849e-05\n",
      "Epoch 00104: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 1.2860e-05 - val_loss: 0.0071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.4864e-05\n",
      "Epoch 00105: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 324us/sample - loss: 1.4759e-05 - val_loss: 0.0065\n",
      "Epoch 106/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 2.3480e-05\n",
      "Epoch 00106: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 321us/sample - loss: 2.3290e-05 - val_loss: 0.0018\n",
      "Epoch 107/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0578e-05\n",
      "Epoch 00107: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.0522e-05 - val_loss: 0.0031\n",
      "Epoch 108/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 8.7072e-06\n",
      "Epoch 00108: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 338us/sample - loss: 8.8121e-06 - val_loss: 0.0015\n",
      "Epoch 109/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.1903e-05\n",
      "Epoch 00109: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 418us/sample - loss: 1.2349e-05 - val_loss: 0.0026\n",
      "Epoch 110/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.1265e-05\n",
      "Epoch 00110: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 479us/sample - loss: 1.1226e-05 - val_loss: 0.0014\n",
      "Epoch 111/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0931e-05\n",
      "Epoch 00111: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.0841e-05 - val_loss: 0.0049\n",
      "Epoch 112/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.5013e-06\n",
      "Epoch 00112: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 398us/sample - loss: 8.4348e-06 - val_loss: 0.0016\n",
      "Epoch 113/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0813e-05\n",
      "Epoch 00113: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.0739e-05 - val_loss: 0.0026\n",
      "Epoch 114/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1698e-05\n",
      "Epoch 00114: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 408us/sample - loss: 1.1630e-05 - val_loss: 0.0012\n",
      "Epoch 115/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.1725e-06\n",
      "Epoch 00115: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 9.1447e-06 - val_loss: 4.2308e-04\n",
      "Epoch 116/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.6922e-06\n",
      "Epoch 00116: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 6.6868e-06 - val_loss: 8.9466e-04\n",
      "Epoch 117/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.7445e-06\n",
      "Epoch 00117: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 9.6712e-06 - val_loss: 0.0014\n",
      "Epoch 118/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1030e-05\n",
      "Epoch 00118: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.1026e-05 - val_loss: 0.0048\n",
      "Epoch 119/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4836e-05\n",
      "Epoch 00119: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.4732e-05 - val_loss: 0.0029\n",
      "Epoch 120/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1583e-05\n",
      "Epoch 00120: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.1561e-05 - val_loss: 0.0027\n",
      "Epoch 121/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 9.4714e-06\n",
      "Epoch 00121: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 361us/sample - loss: 9.4421e-06 - val_loss: 8.2732e-04\n",
      "Epoch 122/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.9284e-06\n",
      "Epoch 00122: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 8.9919e-06 - val_loss: 0.0029\n",
      "Epoch 123/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4720e-05\n",
      "Epoch 00123: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.4655e-05 - val_loss: 0.0073\n",
      "Epoch 124/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.1636e-05\n",
      "Epoch 00124: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 1.1631e-05 - val_loss: 0.0019\n",
      "Epoch 125/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.5117e-06\n",
      "Epoch 00125: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 632us/sample - loss: 7.5053e-06 - val_loss: 8.2899e-04\n",
      "Epoch 126/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 9.4874e-06\n",
      "Epoch 00126: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 382us/sample - loss: 9.4444e-06 - val_loss: 0.0028\n",
      "Epoch 127/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.7841e-06\n",
      "Epoch 00127: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 9.8038e-06 - val_loss: 0.0059\n",
      "Epoch 128/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2778e-05\n",
      "Epoch 00128: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 1.2743e-05 - val_loss: 0.0078\n",
      "Epoch 129/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.5853e-06\n",
      "Epoch 00129: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 7.5640e-06 - val_loss: 0.0056\n",
      "Epoch 130/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.5563e-06\n",
      "Epoch 00130: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 8.5659e-06 - val_loss: 0.0020\n",
      "Epoch 131/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.8936e-06\n",
      "Epoch 00131: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 341us/sample - loss: 7.8645e-06 - val_loss: 0.0014\n",
      "Epoch 132/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.0730e-05\n",
      "Epoch 00132: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 347us/sample - loss: 1.0672e-05 - val_loss: 0.0023\n",
      "Epoch 133/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.2509e-06\n",
      "Epoch 00133: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 8.2830e-06 - val_loss: 0.0015\n",
      "Epoch 134/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.5211e-06\n",
      "Epoch 00134: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 7.5478e-06 - val_loss: 0.0015\n",
      "Epoch 135/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0787e-05\n",
      "Epoch 00135: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.0722e-05 - val_loss: 0.0019\n",
      "Epoch 136/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.9811e-06\n",
      "Epoch 00136: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 336us/sample - loss: 8.9408e-06 - val_loss: 0.0027\n",
      "Epoch 137/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.9540e-06\n",
      "Epoch 00137: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 8.8829e-06 - val_loss: 0.0020\n",
      "Epoch 138/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.9233e-06\n",
      "Epoch 00138: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 322us/sample - loss: 7.8757e-06 - val_loss: 0.0034\n",
      "Epoch 139/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.3279e-06\n",
      "Epoch 00139: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 329us/sample - loss: 9.2417e-06 - val_loss: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.0014e-05\n",
      "Epoch 00140: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 331us/sample - loss: 1.0215e-05 - val_loss: 0.0037\n",
      "Epoch 141/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.1342e-05\n",
      "Epoch 00141: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.1281e-05 - val_loss: 0.0037\n",
      "Epoch 142/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.6402e-06\n",
      "Epoch 00142: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 8.6425e-06 - val_loss: 0.0106\n",
      "Epoch 143/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.0518e-05\n",
      "Epoch 00143: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 346us/sample - loss: 1.0507e-05 - val_loss: 0.0056\n",
      "Epoch 144/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 6.6539e-06\n",
      "Epoch 00144: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 6.6344e-06 - val_loss: 0.0042\n",
      "Epoch 145/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4861e-05\n",
      "Epoch 00145: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 324us/sample - loss: 1.4801e-05 - val_loss: 0.0056\n",
      "Epoch 146/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.0062e-05\n",
      "Epoch 00146: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 402us/sample - loss: 1.0011e-05 - val_loss: 0.0083\n",
      "Epoch 147/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 7.7323e-06\n",
      "Epoch 00147: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 478us/sample - loss: 7.7335e-06 - val_loss: 0.0066\n",
      "Epoch 148/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 6.9360e-06\n",
      "Epoch 00148: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 402us/sample - loss: 6.9391e-06 - val_loss: 0.0028\n",
      "Epoch 149/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.2794e-06\n",
      "Epoch 00149: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 435us/sample - loss: 8.2446e-06 - val_loss: 0.0073\n",
      "Epoch 150/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 9.5598e-06\n",
      "Epoch 00150: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 322us/sample - loss: 9.5354e-06 - val_loss: 0.0056\n",
      "Epoch 151/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.4849e-06\n",
      "Epoch 00151: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 374us/sample - loss: 9.3870e-06 - val_loss: 0.0077\n",
      "Epoch 152/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.4844e-05\n",
      "Epoch 00152: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 1.4875e-05 - val_loss: 0.0094\n",
      "Epoch 153/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.1569e-06\n",
      "Epoch 00153: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 9.1055e-06 - val_loss: 0.0082\n",
      "Epoch 154/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.3227e-06\n",
      "Epoch 00154: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 7.3540e-06 - val_loss: 0.0068\n",
      "Epoch 155/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.2860e-06\n",
      "Epoch 00155: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 6.3072e-06 - val_loss: 0.0072\n",
      "Epoch 156/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.1871e-06\n",
      "Epoch 00156: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 8.2972e-06 - val_loss: 0.0204\n",
      "Epoch 157/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 8.7131e-06\n",
      "Epoch 00157: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 8.6754e-06 - val_loss: 0.0078\n",
      "Epoch 158/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1608e-05\n",
      "Epoch 00158: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 1.1512e-05 - val_loss: 0.0112\n",
      "Epoch 159/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.6222e-06\n",
      "Epoch 00159: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 359us/sample - loss: 7.5938e-06 - val_loss: 0.0048\n",
      "Epoch 160/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 9.6964e-06\n",
      "Epoch 00160: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 9.6575e-06 - val_loss: 0.0038\n",
      "Epoch 161/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 7.5615e-06\n",
      "Epoch 00161: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 342us/sample - loss: 7.5672e-06 - val_loss: 0.0046\n",
      "Epoch 162/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2593e-05\n",
      "Epoch 00162: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 1.2831e-05 - val_loss: 0.0052\n",
      "Epoch 163/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1732e-05\n",
      "Epoch 00163: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.1662e-05 - val_loss: 0.0032\n",
      "Epoch 164/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1510e-05\n",
      "Epoch 00164: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.1443e-05 - val_loss: 0.0057\n",
      "Epoch 165/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.6996e-06\n",
      "Epoch 00165: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 347us/sample - loss: 8.6884e-06 - val_loss: 0.0086\n",
      "Epoch 166/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.9318e-06\n",
      "Epoch 00166: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 6.9917e-06 - val_loss: 0.0053\n",
      "Epoch 167/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 9.0159e-06\n",
      "Epoch 00167: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 361us/sample - loss: 9.0123e-06 - val_loss: 0.0060\n",
      "Epoch 168/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.4872e-06\n",
      "Epoch 00168: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 8.5520e-06 - val_loss: 0.0170\n",
      "Epoch 169/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.1107e-05\n",
      "Epoch 00169: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 341us/sample - loss: 1.1108e-05 - val_loss: 0.0024\n",
      "Epoch 170/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.0714e-06\n",
      "Epoch 00170: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 7.0386e-06 - val_loss: 0.0030\n",
      "Epoch 171/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.9295e-06\n",
      "Epoch 00171: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 6.9887e-06 - val_loss: 0.0041\n",
      "Epoch 172/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.6422e-06\n",
      "Epoch 00172: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 9.6825e-06 - val_loss: 0.0096\n",
      "Epoch 173/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.3818e-06\n",
      "Epoch 00173: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 7.4525e-06 - val_loss: 0.0064\n",
      "Epoch 174/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.0970e-06\n",
      "Epoch 00174: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 7.0667e-06 - val_loss: 0.0037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.3074e-06\n",
      "Epoch 00175: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 8.2836e-06 - val_loss: 0.0064\n",
      "Epoch 176/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 8.7822e-06\n",
      "Epoch 00176: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 325us/sample - loss: 8.7690e-06 - val_loss: 0.0059\n",
      "Epoch 177/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 8.0099e-06\n",
      "Epoch 00177: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 341us/sample - loss: 8.0008e-06 - val_loss: 0.0043\n",
      "Epoch 178/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 7.6857e-06\n",
      "Epoch 00178: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 324us/sample - loss: 7.6950e-06 - val_loss: 0.0035\n",
      "Epoch 179/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 8.0994e-06\n",
      "Epoch 00179: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 328us/sample - loss: 8.0947e-06 - val_loss: 0.0080\n",
      "Epoch 180/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.0835e-06\n",
      "Epoch 00180: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 370us/sample - loss: 9.0625e-06 - val_loss: 0.0073\n",
      "Epoch 181/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.9132e-06\n",
      "Epoch 00181: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.0065e-05 - val_loss: 0.0105\n",
      "Epoch 182/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.2797e-06\n",
      "Epoch 00182: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 9.2524e-06 - val_loss: 0.0041\n",
      "Epoch 183/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.8247e-06\n",
      "Epoch 00183: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 321us/sample - loss: 8.8176e-06 - val_loss: 0.0043\n",
      "Epoch 184/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 6.4441e-06\n",
      "Epoch 00184: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 6.4801e-06 - val_loss: 0.0037\n",
      "Epoch 185/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.2758e-06\n",
      "Epoch 00185: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 8.1814e-06 - val_loss: 0.0054\n",
      "Epoch 186/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.6831e-06\n",
      "Epoch 00186: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 9.7678e-06 - val_loss: 0.0043\n",
      "Epoch 187/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.1240e-06\n",
      "Epoch 00187: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 9.2574e-06 - val_loss: 0.0071\n",
      "Epoch 188/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.6066e-06\n",
      "Epoch 00188: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 334us/sample - loss: 9.5087e-06 - val_loss: 0.0054\n",
      "Epoch 189/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 6.7323e-06\n",
      "Epoch 00189: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 487us/sample - loss: 6.7740e-06 - val_loss: 0.0021\n",
      "Epoch 190/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.4481e-06\n",
      "Epoch 00190: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 8.3769e-06 - val_loss: 0.0055\n",
      "Epoch 191/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.5691e-06\n",
      "Epoch 00191: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 6.5778e-06 - val_loss: 0.0017\n",
      "Epoch 192/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.1873e-06\n",
      "Epoch 00192: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 8.2554e-06 - val_loss: 0.0098\n",
      "Epoch 193/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 7.5533e-06\n",
      "Epoch 00193: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 355us/sample - loss: 7.5318e-06 - val_loss: 0.0033\n",
      "Epoch 194/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 8.5234e-06\n",
      "Epoch 00194: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 425us/sample - loss: 8.5105e-06 - val_loss: 0.0039\n",
      "Epoch 195/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 6.0899e-06\n",
      "Epoch 00195: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 322us/sample - loss: 6.1086e-06 - val_loss: 6.7817e-04\n",
      "Epoch 196/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 6.7642e-06\n",
      "Epoch 00196: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 320us/sample - loss: 6.8359e-06 - val_loss: 4.4758e-04\n",
      "Epoch 197/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 9.1157e-06\n",
      "Epoch 00197: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 337us/sample - loss: 9.0922e-06 - val_loss: 0.0044\n",
      "Epoch 198/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 7.1899e-06\n",
      "Epoch 00198: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 6s 764us/sample - loss: 7.1981e-06 - val_loss: 0.0022\n",
      "Epoch 199/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 7.0518e-06\n",
      "Epoch 00199: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 372us/sample - loss: 7.0191e-06 - val_loss: 0.0043\n",
      "Epoch 200/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.9664e-06\n",
      "Epoch 00200: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 7.0075e-06 - val_loss: 0.0027\n"
     ]
    }
   ],
   "source": [
    "training_process = model.fit(trainX, trainY, epochs=200, verbose=1, validation_data=(testX, testY),batch_size=32, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('dilated_rmse660_model.h5')\n",
    "\n",
    "#model.load_model('dilated_rmse660_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_262 (Model)            (None, 6, 1)              8833      \n",
      "_________________________________________________________________\n",
      "model_263 (Model)            (None, 6, 1)              8833      \n",
      "_________________________________________________________________\n",
      "model_264 (Model)            (None, 6, 1)              8833      \n",
      "_________________________________________________________________\n",
      "model_265 (Model)            (None, 6, 1)              8833      \n",
      "_________________________________________________________________\n",
      "model_266 (Model)            (None, 6, 1)              8833      \n",
      "_________________________________________________________________\n",
      "model_267 (Model)            (None, 6, 1)              8833      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 3, 1)              0         \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 53,499\n",
      "Trainable params: 52,347\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model_complete.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model_complete.load_weights('saved_weights/dilated.hdf5')\n",
    "model.load_weights('saved_weights/dilated1.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_complete.save('dilated_rmse660_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7494, 6, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nbeats_kerasmodel import NBeatsNet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = NBeatsNet(backcast_length=look_back, forecast_length=look_back,\n",
    "                      stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK), nb_blocks_per_stack=2,\n",
    "                      thetas_dim=(4, 4), share_weights_in_stack=True, hidden_layer_units=64)\n",
    "\n",
    "    # Definition of the objective function and the optimizer.\n",
    "model.compile_model(loss='mae', learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history=model.fit(trainX, trainY, validation_data=(testX, testY), epochs=200, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save('n_beats_model.h5')\n",
    "\n",
    "    # Predict on the testing set.\n",
    "predictions = model.predict(testX)\n",
    "print(predictions.shape)\n",
    "\n",
    "    # Load the model.\n",
    "model2 = NBeatsNet.load('n_beats_model.h5')\n",
    "\n",
    "predictions2 = model2.predict(testX)\n",
    "np.testing.assert_almost_equal(predictions, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VdW5//HPkznMCMEqg2ClKk6ogDjWWagt2DphtcVWpV6rta1atbdq6+2gvb869TpRxbFOxaq0YtFWcAYJiAIKMggSUEGQeUzy/P5Y+yQnh5PkJJwkmP19v155ZZ+1h7PWycl+9hr22ubuiIiI5LR0BkREZOeggCAiIoACgoiIRBQQREQEUEAQEZGIAoKIiAAKCCIiElFAEBERQAFBREQieS2dgYbo2rWr9+7du6WzISLypTJt2rTP3b2kvu2+VAGhd+/elJaWtnQ2RES+VMxscSbbqclIRESADAOCmQ0xs7lmNt/Mrkmz/udm9r6ZvWdm/zGzPZLWjTSzedHPyKT0Q81sZnTMO8zMslMkERFpjHoDgpnlAncCQ4F+wDlm1i9ls3eAAe5+IDAW+GO07y7ADcBhwCDgBjPrHO1zN3AR0Df6GbLDpRERkUbLpA9hEDDf3RcCmNkTwHDg/cQG7j4xafvJwHnR8inAS+6+Ktr3JWCImU0COrj75Cj9YeA04IUdKo2ISIpt27ZRVlbG5s2bWzorTa6oqIgePXqQn5/fqP0zCQjdgSVJr8sIV/y1uYDqE3u6fbtHP2Vp0kVEsqqsrIz27dvTu3dvWnPLtLuzcuVKysrK6NOnT6OOkdVOZTM7DxgA/G8WjznKzErNrHTFihXZOqyIxMTmzZvp0qVLqw4GAGZGly5ddqgmlElAWAr0THrdI0pLzcyJwH8Dw9x9Sz37Lo2W6zwmgLuPdvcB7j6gpKTeYbQiIttp7cEgYUfLmUlAmAr0NbM+ZlYAjADGpWTiYOBeQjBYnrRqAnCymXWOOpNPBia4+yfAWjMbHI0u+j7w3A6VpC7vPglT72+yw4uItAb1BgR3LwcuJZzcPwCecvfZZnajmQ2LNvtfoB3wNzObYWbjon1XAf9DCCpTgRsTHczAJcB9wHxgAU3ZoTz7GZj2QJMdXkSkNqtXr+auu+5q8H7f+MY3WL16dRPkqHYZ3ans7uOB8Slp1yctn1jHvmOAMWnSS4H9M87pjijqCMtnN8tbiYgkSwSESy65pEZ6eXk5eXm1n4LHjx9f67qm8qWauqLRijrA5rUtnQsRiaFrrrmGBQsW0L9/f/Lz8ykqKqJz587MmTOHDz/8kNNOO40lS5awefNmLr/8ckaNGgVUT9Wzfv16hg4dylFHHcWbb75J9+7dee655yguLs56XmMSEDrClrXgDjHpXBKR7f3mH7N5f1l2Lw777d6BG761X63rb7rpJmbNmsWMGTOYNGkSp556KrNmzaoaGjpmzBh22WUXNm3axMCBAzn99NPp0qVLjWPMmzePxx9/nL/85S+cddZZPP3005x33nnp3m6HxGMuo8IO4JWwdX1L50REYm7QoEE17hO44447OOiggxg8eDBLlixh3rx52+3Tp08f+vfvD8Chhx7KokWLmiRv8akhAGxeA4XtWzYvItJi6rqSby5t27atWp40aRL//ve/eeutt2jTpg3HHnts2vsICgsLq5Zzc3PZtGlTk+QtHjWEog7ht/oRRKSZtW/fnnXr1qVdt2bNGjp37kybNm2YM2cOkydPbubc1RS/GoKISDPq0qULRx55JPvvvz/FxcXsuuuuVeuGDBnCPffcw7777svee+/N4MGDWzCncQsIW1RDEJHm99hjj6VNLyws5IUX0t+Clegn6Nq1K7NmzapKv/LKK7Oev4R4NBkVqoYgIlKfeAQENRmJiNQrJgEh0amsgCAiUpt4BIS8QsgrUkAQEalDPAIChJvT1KksIlKr+ASEoo6qIYiI1CFGAUET3IlI82vs9NcAt912Gxs3bsxyjmoXo4CgGoKINL8vU0CIx41pEPoQ1pS1dC5EJGaSp78+6aST6NatG0899RRbtmzh29/+Nr/5zW/YsGEDZ511FmVlZVRUVHDdddfx2WefsWzZMo477ji6du3KxIkTmzyvGQUEMxsC3A7kAve5+00p648BbgMOBEa4+9go/Tjg1qRN94nWP2tmDwJfBxKX7ee7+4wdKEvdVEMQkReugU9nZveYXzkAht5U6+rk6a9ffPFFxo4dy9tvv427M2zYMF599VVWrFjB7rvvzvPPPw+EOY46duzILbfcwsSJE+natWt281yLepuMzCwXuBMYCvQDzjGzfimbfQycD9S4P9vdJ7p7f3fvDxwPbAReTNrkqsT6Jg0GEAUE9SGISMt58cUXefHFFzn44IM55JBDmDNnDvPmzeOAAw7gpZde4uqrr+a1116jY8eOLZK/TGoIg4D57r4QwMyeAIYD7yc2cPdF0brKOo5zBvCCuzdfg1iyog5QvgnKt4T7EkQkfuq4km8O7s61117Lj370o+3WTZ8+nfHjx/OrX/2KE044geuvvz7NEZpWJp3K3YElSa/LorSGGgE8npL2OzN7z8xuNbOmPUsXdQq/VUsQkWaUPP31KaecwpgxY1i/Pjysa+nSpSxfvpxly5bRpk0bzjvvPK666iqmT5++3b7NoVk6lc1sN+AAYEJS8rXAp0ABMBq4Grgxzb6jgFEAvXr1anwmCqPpK7ashXYljT+OiEgDJE9/PXToUL773e9y+OGHA9CuXTseffRR5s+fz1VXXUVOTg75+fncfffdAIwaNYohQ4aw++677zSdykuBnkmve0RpDXEW8Iy7b0skuPsn0eIWM3sASDunq7uPJgQMBgwY4A1832pVE9ytbvQhREQaI3X668svv7zG669+9auccsop2+132WWXcdlllzVp3pJl0mQ0FehrZn3MrIDQ9DOuge9zDinNRVGtATMz4DRgVpr9skdPTRMRqVO9AcHdy4FLCc09HwBPuftsM7vRzIYBmNlAMysDzgTuNbPZif3NrDehhvFKyqH/amYzgZlAV+C3O16cOmgKbBGROmXUh+Du44HxKWnXJy1PJTQlpdt3EWk6od39+IZkdIcl9yGISKy4O6ExonVzb3yrOsRt6gpQDUEkZoqKili5cuUOnyx3du7OypUrKSoqavQx4jN1RUE7sBz1IYjETI8ePSgrK2PFihUtnZUmV1RURI8eaRtrMhKfgJCTA4XtVUMQiZn8/Hz69OnT0tn4UohPkxGEZiP1IYiIpBWvgFCoCe5ERGoTr4CgGU9FRGoVs4Cgp6aJiNQmZgFBNQQRkdrEKyAUdoAtCggiIunEKyAkHpJTWddjG0RE4ilmAaED4LB1fUvnRERkpxOzgKDpK0REahPPgKCb00REthOvgJCY8VQ1BBGR7cQrIKjJSESkVjENCGoyEhFJFdOAoBqCiEiqjAKCmQ0xs7lmNt/Mrkmz/hgzm25m5WZ2Rsq6CjObEf2MS0rvY2ZTomM+GT2vuWlVPTVNAUFEJFW9AcHMcoE7gaFAP+AcM+uXstnHwPnAY2kOscnd+0c/w5LSbwZudfe9gC+ACxqR/4bJK4C8YtUQRETSyKSGMAiY7+4L3X0r8AQwPHkDd1/k7u8BGd0CbOHhpscDY6Okh4DTMs71jtAEdyIiaWUSELoDS5Jel0VpmSoys1Izm2xmiZN+F2C1u5fXd0wzGxXtX5qVR+BpgjsRkbSa4xGae7j7UjPbE3jZzGYCGZ+R3X00MBpgwIABO/6U7MIOujFNRCSNTGoIS4GeSa97RGkZcfel0e+FwCTgYGAl0MnMEgGpQcfcIaohiIiklUlAmAr0jUYFFQAjgHH17AOAmXU2s8JouStwJPC+uzswEUiMSBoJPNfQzDdKYsZTERGpod6AELXzXwpMAD4AnnL32WZ2o5kNAzCzgWZWBpwJ3Gtms6Pd9wVKzexdQgC4yd3fj9ZdDfzczOYT+hTuz2bBalXUQTUEEZE0MupDcPfxwPiUtOuTlqcSmn1S93sTOKCWYy4kjGBqXmoyEhFJK153KkPoVK7YAts2t3RORER2KvELCJoCW0QkrfgGBHUsi4jUEOOAoH4EEZFk8QsImuBORCSt+AUE1RBERNKKYUBIPEZTfQgiIsliGBBUQxARSSd+AaGgHViOhp2KiKSIX0AwCx3LqiGIiNQQv4AAmr5CRCSNmAYEPTVNRCRVTANCJ9UQRERSxDMg6KlpIiLbiWdAUB+CiMh2YhoQ1IcgIpIqo4BgZkPMbK6ZzTeza9KsP8bMpptZuZmdkZTe38zeMrPZZvaemZ2dtO5BM/vIzGZEP/2zU6QMFHUMTUaVlc32liIiO7t6n5hmZrnAncBJQBkw1czGJT0KE+Bj4HzgypTdNwLfd/d5ZrY7MM3MJrj76mj9Ve4+dkcL0WCFHQCHreuq71wWEYm5TB6hOQiYHz3yEjN7AhgOVAUEd18Uratxye3uHyYtLzOz5UAJsJqWlDx9hQKCiAiQWZNRd2BJ0uuyKK1BzGwQUAAsSEr+XdSUdKuZFday3ygzKzWz0hUrVjT0bdPTQ3JERLbTLJ3KZrYb8AjwA3dP1CKuBfYBBgK7AFen29fdR7v7AHcfUFJSkp0MVc14qpFGIiIJmQSEpUDPpNc9orSMmFkH4Hngv919ciLd3T/xYAvwAKFpqnloxlMRke1kEhCmAn3NrI+ZFQAjgHGZHDza/hng4dTO46jWgJkZcBowqyEZ3yFVT01Tk5GISEK9AcHdy4FLgQnAB8BT7j7bzG40s2EAZjbQzMqAM4F7zWx2tPtZwDHA+WmGl/7VzGYCM4GuwG+zWrK6FHUKv1VDEBGpkskoI9x9PDA+Je36pOWphKak1P0eBR6t5ZjHNyin2aSnpomIbCeedyrn5kN+G9jcsqNfRUR2JvEMCKAJ7kREUsQ3IGiCOxGRGmIcEDTBnYhIshgHBNUQRESSKSCIiAgQ54CgTmURkRriGxBUQxARqSHGAaEDVGyFbZtbOiciIjuFGAcETXAnIpIsvgGhMAoI6kcQEQHiHBBUQxARqSHGAUEPyRERSRbjgKAagohIsvgGBD0kR0SkhvgGBNUQRERqyCggmNkQM5trZvPN7Jo0648xs+lmVm5mZ6SsG2lm86KfkUnph5rZzOiYd0SP0mw+BW3BchUQREQi9QYEM8sF7gSGAv2Ac8ysX8pmHwPnA4+l7LsLcANwGDAIuMHMOker7wYuAvpGP0MaXYrGMNOMpyIiSTKpIQwC5rv7QnffCjwBDE/ewN0Xuft7QGXKvqcAL7n7Knf/AngJGGJmuwEd3H2yuzvwMHDajhamwTR9hYhIlUwCQndgSdLrsigtE7Xt2z1arveYZjbKzErNrHTFihUZvm2GNMGdiEiVnb5T2d1Hu/sAdx9QUlKS3YOrhiAiUiWTgLAU6Jn0ukeUlona9l0aLTfmmNlT1FF9CCIikUwCwlSgr5n1MbMCYAQwLsPjTwBONrPOUWfyycAEd/8EWGtmg6PRRd8HnmtE/neMaggiIlXqDQjuXg5cSji5fwA85e6zzexGMxsGYGYDzawMOBO418xmR/uuAv6HEFSmAjdGaQCXAPcB84EFwAtZLVkm1IcgIlIlL5ON3H08MD4l7fqk5anUbAJK3m4MMCZNeimwf0Mym3VFHUNAqKyAnNwWzYqISEvb6TuVm1Rxp/BbzUYiIjEPCEWJgLC6ZfMhIrITiHdASNQQNn3RsvkQEdkJxDsgJGoIm1RDEBGJd0AoVpORiEhCvAOCaggiIlXiHRBUQxARqRLvgJBfDHlFqiGIiBD3gACh2Ug1BBERBQSKO6mGICKCAkKoIeg+BBERBQSK1WQkIgIKCFENQXMZiYgoIKiGICICKCBAcefqKbBFRGJMAaFIU2CLiECGAcHMhpjZXDObb2bXpFlfaGZPRuunmFnvKP1cM5uR9FNpZv2jdZOiYybWdctmwTKmGU9FRIAMAoKZ5QJ3AkOBfsA5ZtYvZbMLgC/cfS/gVuBmAHf/q7v3d/f+wPeAj9x9RtJ+5ybWu/vyLJSn4fRMBBERILMawiBgvrsvdPetwBPA8JRthgMPRctjgRPMzFK2OSfad+eiGoKICJBZQOgOLEl6XRalpd3G3cuBNUCXlG3OBh5PSXsgai66Lk0AaR6a8VREBGimTmUzOwzY6O6zkpLPdfcDgKOjn+/Vsu8oMys1s9IVK1ZkP3Oa8VREBMgsICwFeia97hGlpd3GzPKAjsDKpPUjSKkduPvS6Pc64DFC09R23H20uw9w9wElJSUZZLeBVEMQEQEyCwhTgb5m1sfMCggn93Ep24wDRkbLZwAvu7sDmFkOcBZJ/QdmlmdmXaPlfOCbwCxaQn4R5BWrhiAisZdX3wbuXm5mlwITgFxgjLvPNrMbgVJ3HwfcDzxiZvOBVYSgkXAMsMTdFyalFQITomCQC/wb+EtWStQYmvFURKT+gADg7uOB8Slp1yctbwbOrGXfScDglLQNwKENzGvT0TMRRER0pzKgGoKICAoIQZECgoiIAgJoxlMRERQQAtUQREQUEIAwBfbWdVBR3tI5ERFpMQoIkHS3sqbAFpH4UkAAzXgqIoICQlCs6StERBQQIGk+I02BLSLxpYAAmvFURAQFhEA1BBERBQRANQQRERQQgrxCyG+jTmURiTUFhATNeCoiMaeAkKAZT0Uk5hQQEoo66U5lEYm1jAKCmQ0xs7lmNt/MrkmzvtDMnozWTzGz3lF6bzPbZGYzop97kvY51MxmRvvcYWaWrUI1SnEnjTISkVirNyCYWS5wJzAU6AecY2b9Uja7APjC3fcCbgVuTlq3wN37Rz8XJ6XfDVwE9I1+hjS+GFmgGU9FJOYyqSEMAua7+0J33wo8AQxP2WY48FC0PBY4oa4rfjPbDejg7pPd3YGHgdManPts0jMRRCTmMgkI3YElSa/LorS027h7ObAG6BKt62Nm75jZK2Z2dNL2ZfUcs3kVd4at66FiW4tmQ0SkpeQ18fE/AXq5+0ozOxR41sz2a8gBzGwUMAqgV69eTZDFSFHSFNhtuzbd+4iI7KQyqSEsBXomve4RpaXdxszygI7ASnff4u4rAdx9GrAA+Fq0fY96jkm032h3H+DuA0pKSjLIbiNpxlMRiblMAsJUoK+Z9TGzAmAEMC5lm3HAyGj5DOBld3czK4k6pTGzPQmdxwvd/RNgrZkNjvoavg88l4XyNJ6eiSAiMVdvk5G7l5vZpcAEIBcY4+6zzexGoNTdxwH3A4+Y2XxgFSFoABwD3Ghm24BK4GJ3XxWtuwR4ECgGXoh+Wo5qCCIScxn1Ibj7eGB8Str1ScubgTPT7Pc08HQtxywF9m9IZpuUZjwVkZjTncoJmvFURGJOASGhSE1GIhJvCggJeQWQ31Y1BBGJLQWEZJrxVERiTAEhmZ6JICIxpoCQTDUEEYkxBYRkRZoCW0TiSwEhmWY8FZEYU0BIpmciiEiMKSAkK+4M2zZoCmwRiSUFhGSaz0hEYkwBIZlmPBWRGFNASKYagojEmAJCMtUQRCTGFBCSFWsKbBGJLwWEZJrxVERiLKOAYGZDzGyumc03s2vSrC80syej9VPMrHeUfpKZTTOzmdHv45P2mRQdc0b00y1bhWo0PRNBRGKs3iemRc9EvhM4CSgDpprZOHd/P2mzC4Av3H0vMxsB3AycDXwOfMvdl5nZ/oTHcHZP2u/c6MlpO4fcfChopxqCiOw8KisAg5ymb9DJ5B0GAfPdfaG7bwWeAIanbDMceChaHgucYGbm7u+4+7IofTZQbGaF2ch4k9GMp/GzbRM88m2Y/++WzsmOqdgGS6a2dC4k2+b/B/7YBz6b3eRvlUlA6A4sSXpdRs2r/BrbuHs5sAbokrLN6cB0d9+SlPZA1Fx0nZlZg3LeVDTjafy89xQseBnGX/Xlvkv9nUfh/hNh6bSWzolk0+I3YOsG6Nynyd+qWTqVzWw/QjPSj5KSz3X3A4Cjo5/v1bLvKDMrNbPSFStWNH1mVUOIF3eYfHf4u69aCO880tI5arwF/wm/Z/29ZfMh2bX4Teh+CBS0afK3yiQgLAV6Jr3uEaWl3cbM8oCOwMrodQ/gGeD77r4gsYO7L41+rwMeIzRNbcfdR7v7AHcfUFJSkkmZdoxqCPGycBKs+ABO+T30HAyTboatG1s6Vw1XWQEfvRqWZz8DlZUtmx/Jjq0bYdl02OOIZnm7TALCVKCvmfUxswJgBDAuZZtxwMho+QzgZXd3M+sEPA9c4+5vJDY2szwz6xot5wPfBGbtWFGyRM9EiJfJd0PbEjjgDDjx17D+U5hyT0vnquE+mQGb10DfU2DtUih7u6VzJNmwtBQqy2GPI5vl7eoNCFGfwKWEEUIfAE+5+2wzu9HMhkWb3Q90MbP5wM+BxNDUS4G9gOtThpcWAhPM7D1gBqGG8ZdsFqzR9EyE+Fi5AOZNgAEXQF4h7HE4fG0IvH4bbFzV0rlrmIWvhN9Db4K8IjUbtRaL3wTLgZ5pG1Cyrt5hpwDuPh4Yn5J2fdLyZuDMNPv9FvhtLYc9NPNsNqPiTrBtI5RvhbyCls6NNKUp90BuAQz4YXXaCdfD3UfC67fCyf+Tfr+Kcnjxv0PT4nfubZ681mfhJOi2H+yyJ/Q9Cd5/Fob8AXJyWzpnsiMWvwG77g9FHZvl7XSncirNZxQPm1bDO3+F/c+A9rtWp++6Hxx4Nrw9GtakdpURhqg+9f0QTN57ApbPab4812bbJvh4Mux5bHi9/+mw/rNwddmauMP65S2di+ZTvjUMI26m5iJQQNhecefwWx3Lrds7j4SHIQ2+ePt1x/0SvBJeualm+uY18OjpMHc8HHstWG4ICi1tyRSo2FIdEPqeAvltYdbTLZmr7Jv4e7ilX3zutfhkBpRvarYOZVBA2J5qCK1fRTlMGQ17HAW7HbT9+s57hH6Fdx6FFR+GtPXL4cFTw8n39Pvg2GtgrxPCPQyVFc2b/1QLJ0FOXvWJo6AN7D0EPhgXytoabF4LU+6Fym3w9AUhOLd2i6NxOAoILUjPRGj95j4Paz5OXztIOPoKyG8DL/8PrPoI7j85dEJ/98kwIgngoBFhRM+i15on37VZOAl6DITCdtVp+30HNq6Ej15p3DG3bdq5ht9OexC2rIGhf4Q1ZfD8FaEJqTVb/CZ03Rvadm22t1RASFU142kMh566w4cToHxL/ds2p9dvC3cSZ8vku6FTL9j7G7Vv064EjrgsXGX/5fhQYxz5D9jrxOpt9v4GFHaAd1uw2WjTF7BsRnVzUcJeJ4a8zW7EaKOK8lAbenj4znHSLd8Ck++CPsfAYT8KzXUz/9ayn3tTq6wI/ULNWDsABYTtxXnG01lPw2NnwRt3tHROqn06C/59Q7gizPRmq2kPws194B+XQ1lpzZPa0unw8Vtw2MX1j8A5/MfQthvkF8MPJ0CPATXX5xfDfqfB++Ngy/oGFStrPnoNcOjz9Zrp+UUhYH3wj9A52RBT7gnTX5S9vXNMgzHzb7DuEzjyp+H10T8PzX3jrwy1ttbos1mwZW2zdiiDAsL2EsO74tZkVLENXo5GCE++q+VOcKne+r/we9VC+PBf9W9fvjXcbZxXGNr37zsB7jwsBLn1y8PJrqAdHHxe/ccqbA8/egX+6w0o2Tv9NgedEzqn5/wz8zJl08JJoTypwQpg/++EtvaFEzM/3heLYOLv4KvHh+NOvS9bOW2cykp443b4ygEhTxAC+XfuDf0mT1/Q8ID3ZZAYIbbH4c36tgoIqXLzoaB9/GoI0x+GLz6CY66CTavCVXZTqNgWrjozaYpYszRcHQ68EDr2DIGqPjOfgnXLYPj/wRVz4Vt3hFrfS9fBn/aBmWNDMMh0XHeH3atHnqXTczB02gPefTyz42XbR6+Eq8jc/O3X7XlcaALN9CY1d/jnz8KNUMP+HPpIZv0dNqzMbp4b4sN/wecfhtpB8vyXHXuEv/Gyd2Bibbc6fYktfiN8rzr2aNa3VUBIJ27zGW3dCK/8EXodDsf9d2irffMO2LY5u+9TUQ5jfxDa5GeOrX/7KfeEk9QRP4FBo0Ln7bIZtW9f42ryBCjqAIeOhAtehB9PhSMuhe6HhqagbMnJCSfOha/A2mX1b59Nq5fAyvmw59fTr88rgH2/CXOez+xvmZj19YQbwolo4IVhOOs7D2c33w3xxm2hv6ffaduv2/dbcOgPwt88m31MLc091BCaubkIFBDSi9uMp2/fG+bwOeGGcBV29JXhxqYZj9a/70s3wD9+Wv+IlMpKeO6S0Kbdtlu407euoYOb14Zayn6nhWGgh3w/NGHUVUuYOz791SRAydfgpBvhwpfCCSabDjwb8HBCbU6JEUR7Hlv7Nvt9B7auq/9ZDxtWwoRrw2ilgReEtG77Qu+jYeqYlhla+/HkMMz38Msgt5ZJFU75PZTsA89cDBs+b978NZXPPwwjxJq5uQgUENKLUw1h0xdhmoa+p1R/AfscE04Mr99e9/MBZjwWruCmPQBjTglXrOm4w/gr4L0n4fhfhaGb65fDxD/UfuxpD4ZOtSMuC6+LO4WmnllPp78Sd4+uJvdIfzXZlLp8FXoeFpqNGjMqZ/mcxg3xXPhKmJivW7/at+nzdWjTpf6b1Cb8MgThb91Rs7N94IVhiO68Fxuevx31+m1QvAscfG7t2xS0gdPvD3NPvXB18+WtKVX1H6iGsHMo6hifGsIbd4Qr9ROuq05L1BLWfBza8NNZPieM/Ol9NIx4PHRGjj4WFr1Rczt3ePFXUDoGjvpZOG73Q2DAD0LN5NOZ2x+7fGsYGtr7aNj94Or0wy4OV6pvp5kHcfGbUDY1BJDariab0kEjYMUc+OTdhu039T64a3B4YltDgoJ76FDu8/Xta0PJcvOg3/DQFr91Q/pt5v8n3HF91E9h15Tgss+p0H635u9cXj4HPnwhDDMtaFv3tl/ZP/R9zRoLc19onvw1pcVvQrtdw7xUzSwWAWHa4lVMnNOAOVCKYzIF9rpPw4n3gDNDu3uyr50Cux4Ar92yfXPB1o2hLyC/DXznL7DPN+Cil0Pn68PDwskjcaX8ys1hpNCgUdVNUgDHXxe2f/7K7YeTzv6kBsAFAAAQ60lEQVR76Bg+8vKa6bv0CW3ipWO2P7m9fiu06ZrZ6KGmsN+3w0R5mY6Ndw/9Ns9fEQLkkikw9oeZ31m8/APYsLzu5qKE/U8PEzbeORjGXRael5CYzXXrhtCR3KVvCNapcvPh0PNDk1NzDvF88w7IK4aBF2W2/VE/C5P7/fNnX+7avXvoUN7jiLoDfRNp9QHB3fnfCXP50aPTePXDDJ+4VhSTJqNX/zdMBXDcL7dfZwbHXAEr54Wbs5L962pY/n4Y+tdht5DWtS9c9J/Qmfv8FeEegNdugUl/gP7nwpCba37B2+wCJ/4GlkyuOULHPdRaSvateRNYwuAfh9pb8j6fzoL5L4U7j/OLG/957IjizrD30FCjqu8xnJWV8K9rwvDOA0eEexxO/X/hivifl2fW7JRJ/0HCHkfCt0fDbgfC7Gfhb+fDH/cMNbrHR8DqxfCt28O9C+kcen4Y4lk6pv73SlW+BabeD5/Py3yfNUtDf8wh34e2qU/irUVeQRh1tP6zMKLsy2r1x+Hu9xZoLoIYBAQz457zDuWrJe0Y9Ugpby3IYAhdcecwqVTyHbtrlsKkm+CuI+DZH4eT0JfZqoWhnf6QkbVXTfcdFq4cX/1T9Unqvb+FIapH/Wz7E3ZRRzjn8TDtw/SH4D+/CVfOw/4cRuOk6n8u9BgEL11fXSNb8DIsnx2aftJdIfUaDLsfAm/dVV2zeOP20OE88MJGfRRZc+AI2Ph5aIKpTcU2ePbiMIJq8CVw2t3hKnzghXDML8L8SS9nMIxy4aTwd+vUs95NMYODzoYRf4VffAQXvBTu9s0rCs0TAy+E3nWcgNp/JYzoeeeRhjVrrSmDB4bC8z8PzWIv/ir0U9RlxYfwj5+EyQUbOhqs+yHhezP94fD5NJdNX2RvRF5V/0Hz3qGc0AKNrc2vU5sCHr1gECNGT+aCh6by8A8HMaD3LrXvkLhbeeOq0MY97YHQBuuVofNw9t/DCJw+x4Qr1r4npz/hZdPyOTDlbmi/e2gC6LrXjh1v4h8gJx++/ovat8nJDXeFPvtfYUqLLnvBP38axt4f96va9znhetitf2jTP/662u8IzsmBU/8Eo78eToKn/ik0FbTfLTRjpWMWThRPXxA6OrvtGzpMB/9X3fcLNIe9TgwduO8+HiaXS7V1Y7g6nzchdK4ffWXNoHfcL8MV7mv/D9p1C+3n6VRsg0Wvw4FnNTyPuXnhYSs9B8GxV4cTWW4Gz/0YeFFoapo1Nly512fhpNAEVr4Fht8ZRgy9+X/w7pPhyXQHnVPzf2bpdHj9FvjgnyFQnXhDGF3WUMdeG44x7idwyVv19z/siI2rwl3006NhucWdw/9nh93Cd7hD93DDYO+ja699pVr8RmihKNm36fJdB/MMqqdmNgS4HcgF7nP3m1LWFwIPEx56sxI4290XReuuBS4AKoCfuPuETI6ZzoABA7y0tDTjwqVavm4zI+6dzPJ1W3j0wsPo37NT+g1njg0nnDZdwvCvtiWhbfqQkaEde+OqcAU8ZXRo6+6yV+jw7HtSaMcuaJu99r81S2HS78OIntxCKN8MeGjz3//0MKywIf84XywOTUAvXhc6EU/8dd3bV2yDPx8SPoOKreGq7+LXs3vDzPhfhOcPDLsjtHGf+OtQA6krT7cfFK6Qu+0LpQ/A5e9Cx+7Zy1Njjf9F6CxvWxJOEEWdwgVGcWdYMTd0Op/6p+qhnakqyuFvI8O9A2eMCXcbp/p4Cow5Gc56OHQYNwd3uPuI0HT0o1dr/35XVsIbt4YA36UvnP1oGPIL4aT/wtVhSozuh4aJ6rZtDE2LCydCYUc4bFT4X9qRCd0WvQEPfgMO+6/wBLlsq6yEdx8L/0Ob14S/Zdtu4Vyw9pPq3xtWAB762vY8Llwk9D051Lhqc8ch4a74c7J7o6OZTXP3NLezp2xXX0Aws1zgQ+AkoIzwjOVz3P39pG0uAQ5094vNbATwbXc/28z6AY8Dg4DdgX8D0bej7mOms6MBAeCTNZs4+97JrN64lccuGsz+3dPcsfrpTLj3mFADOPQHYU6YdE9Pq9gG7z8Hb90ZHoSdkFsYvtBtdgkBomP30EH7lQPCiIhM7pJNDAedcm+omQy8KDTFVGwJ7cCzng7PWwXoPiDcnNS5TwhYnfuEK5ScnPCP/Ol74QQzZzx8Fo3q2f0Q+N7fM7uqnnpf6BcAOOfJ9Fe/O2LzGvjzgNBJWtAOfja7upZWmzduD01NuQVwwFlw2p3ZzVNjrfs0jILauDL8DTevDr83rQ7Pxj35t+lP8sm2bYJHvhNqWEf9NATiTaujY60OI7pWfwy/WBi+Y81l6v2h+eeCf0PPgduv37Q61Cbnjg8XKsP+XHMGVgjfx/eeCn+79Z+GtLbdQq1vwA/DzYTZ8PwVIb8/nAC9Dqt7281rQif9Z7Pgs9mh1tT9kFCL6rZfzVFrn70fPoOP3wqtBafeEv6n09m2KdTkPvwXzP0XrC0L6bsfHAJEz0Hhf7ddSUhf9yn8ae/wHUkMt86SbAaEw4Ffu/sp0etrAdz9D0nbTIi2ecvM8oBPgRKiZysntk1sF+1W5zHTyUZAACj7YiNn3zuZjVvLeXzUYPb5SpovYWVF5o8fdA9XPys+CCeCDZ+HWsTGz8PyF4vCckKnXvCVA0NHbF5xCDa5iZ98WPcZTL4ztLceNCI0JaS7meqLRVE1/u/hi+xJo4FyC0PNYevG8EW0nPAF3ufUEOC6fDXzD2zb5nBF+rWhcNy1me/XEO8+Cc+MCk1wQ35f//abVoeHpWzbAD9+u/a5hr6sNq0OI7Y+eTd8LxI1jcTvXoPDBUJz2rIO/rRvuF/lgDOjILUmBKrNq8NEe2uXhhPaYRfXXUvesi6csIs6huajTJtUGpLXuw4PgwzOGBPyumlV+L9M/F61MASB1R9X71fUKfwPbogGoOS3DcGhx8BQO397dJhF9qQbQx9Ypk3F7mEgxtwXQvPrsunhAgGgc+9w/LzC0Id00cuhBpVF2QwIZwBD3P3C6PX3gMPc/dKkbWZF25RFrxcAhxFO/pPd/dEo/X4gMVC4zmOmk62AALB45QbOuvctPlsbOo7zcoycHCMvx8i1sFzpTmWlU+FOpUNlpVPpjlnYxgxyc6qXzQx3xwEcqj9Zp8RWsy+L2McWs7d/xN4spiefkkv6GTxf5xD+nPNd5lvvesviQG7lNrqxku7+KT34lJ7+Gd35DMd41Q/mVRvAajpU5TPHICcqZ44RlcHIzTEcr+qv9ajs7pVV/+CGVf2vW/T+FdFnU+nRcmX4HHIMcqo+IyM3J7zv9oVwTqh8k7dyDmajtam1rEbIP8AZ5f+kq3/BvQXfS/+5eHX+K6O/SyZNpNXvFC0lldWsZvndwfHod6LvPekvn/R2XpXmScvV63OS/jZg5FJJAdvYQgGYkZzzRNkcqr6nVW8d/W0t8TeOjpdcjuRypctrctlzzKo+98u33c9ZFTUn8ttEIetpy+fWmdvzfsh7OftG+1a/QXJe3aGyxudkUdmrP9vEnuk+p3QSn2ji7wBwWMU7/F9l+g76jV7IMkr40PZgvvVmQc4efJTbm1U5XckxYzeWs3/lHParmMt+lXPZq/Ijcqnk+fwT+UvBSNbmdCD547M03+navmsOFPoW+lYsoF/lXPYtD+/R1Vexhvac3vYhKqOL0eq/lTFm5EB6dan9f6MumQaEnb5T2cxGAaMAevXK3pQDe3Rpy9iLj+DZd5ayraKSCnfKoxNZeWX40ib+GXJzrOofK8eiE2YUIMKJMHzRa+a7+ssdvqQ9cT+Ale687vBadIwcryDXy8n1beSxjdzKctyM9fld6Qv0TTpmIk9pPyeMHOsVgpJBmRmJJwIXAScln7iSTpLJga4iOrHkRP+RluYfNXECTOQnITcKqLmJzynHMKKgGv1UVIb3rqj0tOXYynDqui5K/md3hzJGsgRnUNqNq0+uVSc1q3myqfV9Ut4zkRo+u5rlr3ESSzrx1jxZJC1Ha5K3Sb6QSD5huqf7nKoTqoJ6FEgSeUn8nauP5dEJuLp0iXKlfqeSl5PLWxktl1b8hLIt32JLTls25bVjc257KnPyq3LWHjgyzWcY/gbRdyqn+nOqzku4CEkOrqmBy+r5y1VtZ4ncDGH0ht1oX7GSzXmd2JzXkU35ndic35HKnMKqC5nyykraVMI+lZVV5wCnM5+zN5McJgL5lZspLl/LmvwSeqSUr+aL1EzVnlxJCbMYzCxgLNCpfAWGs09e55qHjRYK8pp+UGgmAWEpkDy2rUeUlm6bsqjJqCOhc7mufes7JgDuPhoYDaGGkEF+M9ZzlzZcdkLf+jcUkST1tMnvVA6ofxOpkknImQr0NbM+ZlYAjABS7lRiHDAyWj4DeNlDfWkcMMLMCs2sD+GC9+0MjykiIs2o3hqCu5eb2aXABMIQ0THuPtvMbgRK3X0ccD/wiJnNB1YRTvBE2z0FvA+UAz92Dz2f6Y6Z/eKJiEimMroPYWeRzU5lEZG4yLRTudVPXSEiIplRQBAREUABQUREIgoIIiICKCCIiEjkSzXKyMxWAIsbuXtXoJU8hbtBVO54iWu5Ib5lz6Tce7h7SX0H+lIFhB1hZqWZDLtqbVTueIlruSG+Zc9mudVkJCIigAKCiIhE4hQQRrd0BlqIyh0vcS03xLfsWSt3bPoQRESkbnGqIYiISB1iERDMbIiZzTWz+WZ2TUvnp6mY2RgzWx49wS6RtouZvWRm86LfGTxE+cvFzHqa2UQze9/MZpvZ5VF6qy67mRWZ2dtm9m5U7t9E6X3MbEr0fX8ymmK+1TGzXDN7x8z+Gb1u9eU2s0VmNtPMZphZaZSWte95qw8IZpYL3AkMBfoB55hZv5bNVZN5EBiSknYN8B937wv8J3rd2pQDV7h7P2Aw8OPob9zay74FON7dDwL6A0PMbDBwM3Cru+8FfAFc0IJ5bEqXAx8kvY5LuY9z9/5JQ02z9j1v9QEBGATMd/eF7r4VeAIY3sJ5ahLu/irheRTJhgMPRcsPAac1a6aagbt/4u7To+V1hJNEd1p52T1YH73Mj34cOJ7wVEZoheUGMLMewKnAfdFrIwblrkXWvudxCAjdgSVJr8uitLjY1d0/iZY/BXZtycw0NTPrDRwMTCEGZY+aTWYAy4GXgAXAancvjzZprd/324BfAJXR6y7Eo9wOvGhm06LnzUMWv+eZPFNZWgl3dzNrtcPKzKwd8DTwU3dfa0lPjG+tZY+eQNjfzDoBzwD7tHCWmpyZfRNY7u7TzOzYls5PMzvK3ZeaWTfgJTObk7xyR7/ncaghLAV6Jr3uEaXFxWdmthtA9Ht5C+enSZhZPiEY/NXd/x4lx6LsAO6+GpgIHA50MrPExV5r/L4fCQwzs0WEJuDjgdtp/eXG3ZdGv5cTLgAGkcXveRwCwlSgbzQCoYDwvOdxLZyn5jQOGBktjwSea8G8NImo/fh+4AN3vyVpVasuu5mVRDUDzKwYOInQfzIROCParNWV292vdfce7t6b8P/8srufSysvt5m1NbP2iWXgZGAWWfyex+LGNDP7BqHNMRcY4+6/a+EsNQkzexw4ljD74WfADcCzwFNAL8JMsWe5e2rH85eamR0FvAbMpLpN+ZeEfoRWW3YzO5DQiZhLuLh7yt1vNLM9CVfOuwDvAOe5+5aWy2nTiZqMrnT3b7b2ckfleyZ6mQc85u6/M7MuZOl7HouAICIi9YtDk5GIiGRAAUFERAAFBBERiSggiIgIoIAgIiIRBQQREQEUEEREJKKAICIiAPx/o3qtDDOJ4KwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(training_process.history['loss'], label='train')\n",
    "pyplot.plot(training_process.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = Model_complete.predict(testX)\n",
    "pyplot.plot(yhat, label='predict')\n",
    "pyplot.plot(testY, label='true')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = model.predict(testX)\n",
    "pyplot.plot(yhat, label='predict')\n",
    "pyplot.plot(testY, label='true')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 660.207\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "yhat_inverse = scaler.inverse_transform(yhat.reshape(-1,1))\n",
    "testY_inverse = scaler.inverse_transform(testY.reshape(-1, 1))\n",
    "rmse = sqrt(mean_squared_error(testY_inverse, yhat_inverse))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 494.510\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "yhat_inverse = scaler.inverse_transform(yhat.reshape(-1,1))\n",
    "testY_inverse = scaler.inverse_transform(testY.reshape(-1, 1))\n",
    "rmse = sqrt(mean_squared_error(testY_inverse, yhat_inverse))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shifting 하는거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1e647b3640>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_date = pd.Timestamp('01-01-2021')\n",
    "# 2021/1/1 까지의 데이터를 트레이닝셋.\n",
    "# 그 이후 데이터를 테스트셋으로 한다.\n",
    "\n",
    "train = data1.loc[:split_date]\n",
    "test = data1.loc[split_date:]\n",
    "# Feature는 Unadjusted 한 개\n",
    "\n",
    "ax = train.plot()\n",
    "test.plot(ax=ax)\n",
    "plt.legend(['train', 'test'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0019861 ],\n",
       "       [0.00317089],\n",
       "       [0.00241308],\n",
       "       ...,\n",
       "       [0.96353706],\n",
       "       [0.95444165],\n",
       "       [0.97917115]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "train_sc = sc.fit_transform(train)\n",
    "test_sc = sc.transform(test)\n",
    "\n",
    "train_sc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-20 05:00:00</th>\n",
       "      <td>0.001986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 06:00:00</th>\n",
       "      <td>0.003171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 07:00:00</th>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 08:00:00</th>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 09:00:00</th>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Scaled\n",
       "timeUTC                      \n",
       "2020-10-20 05:00:00  0.001986\n",
       "2020-10-20 06:00:00  0.003171\n",
       "2020-10-20 07:00:00  0.002413\n",
       "2020-10-20 08:00:00  0.003235\n",
       "2020-10-20 09:00:00  0.001791"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sc_df = pd.DataFrame(train_sc, columns=['Scaled'], index=train.index)\n",
    "test_sc_df = pd.DataFrame(test_sc, columns=['Scaled'], index=test.index)\n",
    "train_sc_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaled</th>\n",
       "      <th>shift_1</th>\n",
       "      <th>shift_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-20 05:00:00</th>\n",
       "      <td>0.001986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 06:00:00</th>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 07:00:00</th>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.001986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 08:00:00</th>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.003171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 09:00:00</th>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 10:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 11:00:00</th>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Scaled   shift_1   shift_2\n",
       "timeUTC                                          \n",
       "2020-10-20 05:00:00  0.001986       NaN       NaN\n",
       "2020-10-20 06:00:00  0.003171  0.001986       NaN\n",
       "2020-10-20 07:00:00  0.002413  0.003171  0.001986\n",
       "2020-10-20 08:00:00  0.003235  0.002413  0.003171\n",
       "2020-10-20 09:00:00  0.001791  0.003235  0.002413\n",
       "2020-10-20 10:00:00  0.000000  0.001791  0.003235\n",
       "2020-10-20 11:00:00  0.001553  0.000000  0.001791"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s in range(1, 3):\n",
    "    train_sc_df['shift_{}'.format(s)] = train_sc_df['Scaled'].shift(s)\n",
    "    test_sc_df['shift_{}'.format(s)] = test_sc_df['Scaled'].shift(s)\n",
    "\n",
    "train_sc_df.head(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_sc_df.dropna().drop('Scaled', axis=1)\n",
    "y_train = train_sc_df.dropna()[['Scaled']]\n",
    "\n",
    "X_test = test_sc_df.dropna().drop('Scaled', axis=1)\n",
    "y_test = test_sc_df.dropna()[['Scaled']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shift_1</th>\n",
       "      <th>shift_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-20 07:00:00</th>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.001986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 08:00:00</th>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.003171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 09:00:00</th>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 10:00:00</th>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 11:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      shift_1   shift_2\n",
       "timeUTC                                \n",
       "2020-10-20 07:00:00  0.003171  0.001986\n",
       "2020-10-20 08:00:00  0.002413  0.003171\n",
       "2020-10-20 09:00:00  0.003235  0.002413\n",
       "2020-10-20 10:00:00  0.001791  0.003235\n",
       "2020-10-20 11:00:00  0.000000  0.001791"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-20 07:00:00</th>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 08:00:00</th>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 09:00:00</th>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 10:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 11:00:00</th>\n",
       "      <td>0.001553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Scaled\n",
       "timeUTC                      \n",
       "2020-10-20 07:00:00  0.002413\n",
       "2020-10-20 08:00:00  0.003235\n",
       "2020-10-20 09:00:00  0.001791\n",
       "2020-10-20 10:00:00  0.000000\n",
       "2020-10-20 11:00:00  0.001553"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1746, 2)\n",
      "[[0.00317089 0.0019861 ]\n",
      " [0.00241308 0.00317089]\n",
      " [0.00323499 0.00241308]\n",
      " ...\n",
      " [0.97030869 0.98571841]\n",
      " [0.96353706 0.97030869]\n",
      " [0.95444165 0.96353706]]\n",
      "(1746, 1)\n",
      "[[0.00241308]\n",
      " [0.00323499]\n",
      " [0.0017915 ]\n",
      " ...\n",
      " [0.96353706]\n",
      " [0.95444165]\n",
      " [0.97917115]]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.values\n",
    "X_test= X_test.values\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "print(X_train.shape)\n",
    "print(X_train)\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train.reshape(X_train.shape[0], 2, 1) \n",
    "X_test_t = X_test.reshape(X_test.shape[0], 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 DATA\n",
      "(1746, 2, 1)\n",
      "[[[0.00317089]\n",
      "  [0.0019861 ]]\n",
      "\n",
      " [[0.00241308]\n",
      "  [0.00317089]]\n",
      "\n",
      " [[0.00323499]\n",
      "  [0.00241308]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.97030869]\n",
      "  [0.98571841]]\n",
      "\n",
      " [[0.96353706]\n",
      "  [0.97030869]]\n",
      "\n",
      " [[0.95444165]\n",
      "  [0.96353706]]]\n",
      "[[0.00241308]\n",
      " [0.00323499]\n",
      " [0.0017915 ]\n",
      " ...\n",
      " [0.96353706]\n",
      " [0.95444165]\n",
      " [0.97917115]]\n"
     ]
    }
   ],
   "source": [
    "print(\"최종 DATA\")\n",
    "print(X_train_t.shape)\n",
    "print(X_train_t)\n",
    "print(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 40,901\n",
      "Trainable params: 40,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "K.clear_session() \n",
    "model = Sequential() # Sequeatial Model \n",
    "model.add(LSTM(100, input_shape=(X_train_t.shape[1], X_train_t.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam') \n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.0359\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 6.2887e-04\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 2.1661e-04\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 1.7869e-04\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 1.4293e-04\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 1.2872e-04\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 9.8637e-05\n",
      "Epoch 8/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 8.3698e-05\n",
      "Epoch 9/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.6040e-05\n",
      "Epoch 10/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.5892e-05\n",
      "Epoch 11/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.4798e-05\n",
      "Epoch 12/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.0063e-05\n",
      "Epoch 13/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.2674e-05\n",
      "Epoch 14/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 6.9875e-05\n",
      "Epoch 15/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.1360e-05\n",
      "Epoch 16/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.4119e-05\n",
      "Epoch 17/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.1897e-05\n",
      "Epoch 18/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.3760e-05\n",
      "Epoch 19/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.2926e-05\n",
      "Epoch 20/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.8779e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc0e5820d0>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1)\n",
    "\n",
    "model.fit(X_train_t, y_train, epochs=20,\n",
    "          batch_size=16, verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(X_test_t)\n",
    "pyplot.plot(y_pred, label='predict')\n",
    "pyplot.plot(y_test, label='true')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_X = test_X.reshape((test_X.shape[0], 4))\n",
    "\n",
    "print(test_X.shape)\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 0:3]), axis=1)\n",
    "print(inv_yhat.shape)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 1)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (756,1) doesn't match the broadcast shape (756,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-03005c0f8d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscaler2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0myhat_inverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtestY_inverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrmse_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_inverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    459\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (756,1) doesn't match the broadcast shape (756,5)"
     ]
    }
   ],
   "source": [
    "#history = model.fit(X_train_t, y_train, epochs=30, batch_size=10, validation_data=(X_test_t, y_test), verbose=0, shuffle=False)\n",
    "yhat = model.predict(X_test_t)\n",
    "#yhat_inverse = yhat.ravel()\n",
    "#testY_inverse = y_test.ravel()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler2.min_, scaler2.scale_ = scaler.min_[0], scaler.scale_[0]\n",
    "\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "testY_inverse = scaler.inverse_transform(testY)\n",
    "rmse_sent = sqrt(mean_squared_error(testY_inverse, yhat_inverse))\n",
    "print (\"Done\")\n",
    "print ('Test RMSE: %.3f' % rmse_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9667460117283561,\n",
       " 0.9846662102636626,\n",
       " 0.9864486591660434,\n",
       " 0.999857561727005,\n",
       " 0.9900084899966671,\n",
       " 0.9975278796177024,\n",
       " 0.9887738372983345,\n",
       " 0.9930807653157739,\n",
       " 1.0165487375352438,\n",
       " 1.003704521092124,\n",
       " 1.0095495572590596,\n",
       " 1.005676700026123,\n",
       " 1.003131953014512,\n",
       " 1.0024681794024124,\n",
       " 0.9987985641321285,\n",
       " 0.9935925297037284,\n",
       " 1.0050894940232227,\n",
       " 1.0062318151928151,\n",
       " 1.0114384126183422,\n",
       " 1.0072857458135531,\n",
       " 1.0202600145928855,\n",
       " 1.0123611649086142,\n",
       " 1.0044020745318116,\n",
       " 1.010961554052228,\n",
       " 0.9983318395142909,\n",
       " 0.9976320340860981,\n",
       " 0.9961626115860304,\n",
       " 1.007211993189987,\n",
       " 1.0039623737760444,\n",
       " 1.0093778431354887,\n",
       " 1.01222491960401,\n",
       " 1.004485398106528,\n",
       " 1.0148839550322934,\n",
       " 1.0125964977074755,\n",
       " 1.013957261762136,\n",
       " 1.0143750056299714,\n",
       " 1.0273661643230971,\n",
       " 1.0342217803320333,\n",
       " 1.0368695558177872,\n",
       " 1.0404378316053076,\n",
       " 1.0283992640501562,\n",
       " 1.0318194715934168,\n",
       " 1.0361725653752263,\n",
       " 1.0831321556934772,\n",
       " 1.131379883437075,\n",
       " 1.121758262545828,\n",
       " 1.146191774837181,\n",
       " 1.1884706070460216,\n",
       " 1.2108942195959034,\n",
       " 1.215630151423707,\n",
       " 1.2198295469899922,\n",
       " 1.0899303459955139,\n",
       " 1.1431211885094537,\n",
       " 1.1689070198985703,\n",
       " 1.1706917207894572,\n",
       " 1.186644244367777,\n",
       " 1.193386134957167,\n",
       " 1.2021840910523993,\n",
       " 1.1939778449370793,\n",
       " 1.2297664462720581,\n",
       " 1.2573122066784967,\n",
       " 1.2711174592164882,\n",
       " 1.299479565456298,\n",
       " 1.2933417707836021,\n",
       " 1.2839194508751226,\n",
       " 1.2661067847909706,\n",
       " 1.296755222361345,\n",
       " 1.2787269283777576,\n",
       " 1.2672727518398745,\n",
       " 1.266097776836947,\n",
       " 1.2489899831551257,\n",
       " 1.1963339879113257,\n",
       " 1.2102754857539209,\n",
       " 1.2032847504346336,\n",
       " 1.2026727725581687,\n",
       " 1.2567886193508868,\n",
       " 1.248544652428094,\n",
       " 1.2238820003062707,\n",
       " 1.219100465711223,\n",
       " 1.2082723419779664,\n",
       " 1.227758235521966,\n",
       " 1.2436696603100534,\n",
       " 1.2473674254366602,\n",
       " 1.2338121436228189,\n",
       " 1.206282147135921,\n",
       " 1.2354048624935818,\n",
       " 1.1655791438840497,\n",
       " 1.1625153135218396,\n",
       " 1.103522785619702,\n",
       " 1.0625529217298872,\n",
       " 1.0898031086449333,\n",
       " 1.1175701269220721,\n",
       " 1.1258366137299234,\n",
       " 1.1674499833352852,\n",
       " 1.1458078107969336,\n",
       " 1.120124444884833,\n",
       " 1.1024429571311467,\n",
       " 1.1129529874879518,\n",
       " 1.1410200832334954,\n",
       " 1.1181601479106051,\n",
       " 1.1048413248898776,\n",
       " 1.1228814418131208,\n",
       " 1.1609468035275148,\n",
       " 1.1832437417239423,\n",
       " 1.2041900498139855,\n",
       " 1.1788974039076505,\n",
       " 1.1147675272265412,\n",
       " 1.0972149658148145,\n",
       " 1.070926940988893,\n",
       " 1.1106762271085366,\n",
       " 1.0932503400502642,\n",
       " 1.1144359219190543,\n",
       " 1.135406438885536,\n",
       " 1.1228420320142685,\n",
       " 1.1484386963688937,\n",
       " 1.1385778016988999,\n",
       " 1.1311192157675225,\n",
       " 1.17992881464333,\n",
       " 1.166785646726059,\n",
       " 1.1895059587615866,\n",
       " 1.2049118121301108,\n",
       " 1.24211691423527,\n",
       " 1.2632478853827926,\n",
       " 1.2767232216046769,\n",
       " 1.2603107293740372,\n",
       " 1.2888220298523594,\n",
       " 1.27398198859593,\n",
       " 1.2700173628313802,\n",
       " 1.2642624062046788,\n",
       " 1.2683300604433714,\n",
       " 1.2832556772630235,\n",
       " 1.3528989848035815,\n",
       " 1.3322685181014835,\n",
       " 1.3400446344121857,\n",
       " 1.3130331582787598,\n",
       " 1.3072888985974616,\n",
       " 1.2702701485411616,\n",
       " 1.300761509913253,\n",
       " 1.3301296920180516,\n",
       " 1.3289851188599533,\n",
       " 1.307613184942304,\n",
       " 1.289507197355265,\n",
       " 1.3091451001234091,\n",
       " 1.3103701818705917,\n",
       " 1.3271784610811346,\n",
       " 1.3463614621710969,\n",
       " 1.322979628511976,\n",
       " 1.3898952149748225,\n",
       " 1.3803569176582924,\n",
       " 1.388562037779359,\n",
       " 1.432272571680794,\n",
       " 1.4535009413311952,\n",
       " 1.4382865069856683,\n",
       " 1.443428359741652,\n",
       " 1.4721789339987206,\n",
       " 1.4712736346193687,\n",
       " 1.4672482051651605,\n",
       " 1.4545543089548065,\n",
       " 1.4347396250889535,\n",
       " 1.4538849053714427,\n",
       " 1.470207881058975,\n",
       " 1.4448324745750494,\n",
       " 1.4901492392782827,\n",
       " 1.4989815381982288,\n",
       " 1.5106215037878443,\n",
       " 1.5251237467683962,\n",
       " 1.5530050534622069,\n",
       " 1.5621025240287172,\n",
       " 1.5967932809670935,\n",
       " 1.5363437165016705,\n",
       " 1.5581407132497995,\n",
       " 1.5613025051120135,\n",
       " 1.5943605703836483,\n",
       " 1.592717181771504,\n",
       " 1.5803154810697846,\n",
       " 1.544150797654329,\n",
       " 1.5361528604758,\n",
       " 1.4532402736616432,\n",
       " 1.518619440966373,\n",
       " 1.5375170025132192,\n",
       " 1.4956311422986492,\n",
       " 1.5279432363777214,\n",
       " 1.5511330880167185,\n",
       " 1.5335495617630364,\n",
       " 1.5738550890436254,\n",
       " 1.6522862187311396,\n",
       " 1.6833912469710755,\n",
       " 1.6841608640429495,\n",
       " 1.6854833442930106,\n",
       " 1.6935584120778642,\n",
       " 1.658433584354985,\n",
       " 1.6870642402241174,\n",
       " 1.6681222019042812,\n",
       " 1.6046403349157305,\n",
       " 1.6299059569599952,\n",
       " 1.5563785322439712,\n",
       " 1.6116316332321436,\n",
       " 1.6498732130471203,\n",
       " 1.645444677650365,\n",
       " 1.6268832253880174,\n",
       " 1.6210601461090142,\n",
       " 1.6087693558412073,\n",
       " 1.6238368479367282,\n",
       " 1.6233087566321056,\n",
       " 1.5715107690090346,\n",
       " 1.5529087809535818,\n",
       " 1.5898357624782684,\n",
       " 1.6210685910659106,\n",
       " 1.6289201489915595,\n",
       " 1.6327749903164492,\n",
       " 1.6434922036157924,\n",
       " 1.6590922909929469,\n",
       " 1.6718351679533026,\n",
       " 1.6595201688090584,\n",
       " 1.6450213038112653,\n",
       " 1.6145811751776815,\n",
       " 1.6378537873942691,\n",
       " 1.6496868609982611,\n",
       " 1.6500449271706912,\n",
       " 1.6491705926332947,\n",
       " 1.6570446704440016,\n",
       " 1.634292267572266,\n",
       " 1.62294843847117,\n",
       " 1.6472378234981484,\n",
       " 1.6789238647725941,\n",
       " 1.6697363146658497,\n",
       " 1.6352459847044938,\n",
       " 1.634987006026321,\n",
       " 1.639962774629998,\n",
       " 1.6664394034932846,\n",
       " 1.6668374424616936,\n",
       " 1.659552822642393,\n",
       " 1.6563876527974197,\n",
       " 1.6380941871672685,\n",
       " 1.5665929891093833,\n",
       " 1.58620499400971,\n",
       " 1.5995733607775664,\n",
       " 1.5788100267536231,\n",
       " 1.5936495050129262,\n",
       " 1.5694980542819303,\n",
       " 1.5324748002486195,\n",
       " 1.5010280327529206,\n",
       " 1.445651072396926,\n",
       " 1.480025424950231,\n",
       " 1.4998181519281526,\n",
       " 1.5224281165268931,\n",
       " 1.5084021691153287,\n",
       " 1.4500683478511522,\n",
       " 1.4542384675668614,\n",
       " 1.4032590777656666,\n",
       " 1.348245250556241,\n",
       " 1.3572526415825172,\n",
       " 1.3230494401556574,\n",
       " 1.2081681875095711,\n",
       " 1.2938023024330483,\n",
       " 1.3528460630736943,\n",
       " 1.3694972660859537,\n",
       " 1.3483584129786603,\n",
       " 1.280674898435318,\n",
       " 1.2830349823894496,\n",
       " 1.1951505679515013,\n",
       " 1.2298097970507955,\n",
       " 1.1345596912073361,\n",
       " 1.1417519794978968,\n",
       " 1.2052169565726536,\n",
       " 1.1929064614054208,\n",
       " 1.1732578616918738,\n",
       " 1.237044310125841,\n",
       " 1.2745162728689432,\n",
       " 1.2970294019619324,\n",
       " 1.3560523317088986,\n",
       " 1.3255097375982992,\n",
       " 1.2876954726023078,\n",
       " 1.2952311891400106,\n",
       " 1.327555106158738,\n",
       " 1.3452895156423121,\n",
       " 1.3771844288506747,\n",
       " 1.3861017403367168,\n",
       " 1.4096147523263043,\n",
       " 1.3725025447470114,\n",
       " 1.379677380126652,\n",
       " 1.3321356507796382,\n",
       " 1.3497152360534352,\n",
       " 1.3289457090611005,\n",
       " 1.231767901056633,\n",
       " 1.2387028996604004,\n",
       " 1.26260381667012,\n",
       " 1.3285290911875185,\n",
       " 1.3276767135380538,\n",
       " 1.3284119877852145,\n",
       " 1.3102874212930016,\n",
       " 1.2889481412086874,\n",
       " 1.3114066595804093,\n",
       " 1.2568972777962943,\n",
       " 1.2751141758172468,\n",
       " 1.184225608712493,\n",
       " 1.2221198193004423,\n",
       " 1.215764144739805,\n",
       " 1.2596137389314763,\n",
       " 1.2246887751884912,\n",
       " 1.2319356742003187,\n",
       " 1.2587056245664918,\n",
       " 1.3274244908253987,\n",
       " 1.3017810977092772,\n",
       " 1.2917344139875508,\n",
       " 1.3236096222964875,\n",
       " 1.2931818795996866,\n",
       " 1.2860042292344138,\n",
       " 1.3049963742985056,\n",
       " 1.285436165133813,\n",
       " 1.3078051669624275,\n",
       " 1.3146619089656166,\n",
       " 1.3167388053651372,\n",
       " 1.370030424364714,\n",
       " 1.384637947807914,\n",
       " 1.397860498320016,\n",
       " 1.456493834055471,\n",
       " 1.4504686388080672,\n",
       " 1.4621671560988352,\n",
       " 1.4849736066947115,\n",
       " 1.4664144064208693,\n",
       " 1.464212524659274,\n",
       " 1.44515225694288,\n",
       " 1.468654008989938,\n",
       " 1.4715663931251295,\n",
       " 1.4884247790799274,\n",
       " 1.5113708529631662,\n",
       " 1.519763451127345,\n",
       " 1.4923094592525197,\n",
       " 1.523888531072937,\n",
       " 1.5184173249979729,\n",
       " 1.5127107861241473,\n",
       " 1.5537217488041941,\n",
       " 1.587230211776999,\n",
       " 1.5811881266158019,\n",
       " 1.5663007936007496,\n",
       " 1.575466949816688,\n",
       " 1.5702941322187494,\n",
       " 1.6009543927287795,\n",
       " 1.5716807941412267,\n",
       " 1.5404119337374897,\n",
       " 1.550241300568402,\n",
       " 1.5620839451235438,\n",
       " 1.5774036599317198,\n",
       " 1.5606561844108344,\n",
       " 1.5664955906065057,\n",
       " 1.535609005251637,\n",
       " 1.5196142568888327,\n",
       " 1.4867340987091597,\n",
       " 1.4969007008188226,\n",
       " 1.5204598785727796,\n",
       " 1.5107183392935961,\n",
       " 1.5337015709871813,\n",
       " 1.5208545395584299,\n",
       " 1.5003895940115122,\n",
       " 1.483666327367065,\n",
       " 1.4773934133840179,\n",
       " 1.4347064082584917,\n",
       " 1.3595372839217026,\n",
       " 1.3939245854089157,\n",
       " 1.3829523344112853,\n",
       " 1.3518084593696233,\n",
       " 1.3369014214551451,\n",
       " 1.362160287533892,\n",
       " 1.3989560907281127,\n",
       " 1.4180946150450846,\n",
       " 1.4289120418329384,\n",
       " 1.4505024186356552,\n",
       " 1.4418891255979025,\n",
       " 1.395707597308423,\n",
       " 1.4144570906110094,\n",
       " 1.4157598659616437,\n",
       " 1.3768021538018065,\n",
       " 1.3911400016214315,\n",
       " 1.4178328213812792,\n",
       " 1.4668940799726156,\n",
       " 1.4654499923432391,\n",
       " 1.4645064091592874,\n",
       " 1.4783673984127983,\n",
       " 1.4621969949465377,\n",
       " 1.4500418869862086,\n",
       " 1.453618607730626,\n",
       " 1.4681878473692267,\n",
       " 1.453655202543846,\n",
       " 1.4363728797528217,\n",
       " 1.414593335915613,\n",
       " 1.4072372154612522,\n",
       " 1.4263008611604047,\n",
       " 1.3820036842531955,\n",
       " 1.3929764982479527,\n",
       " 1.3858478286326825,\n",
       " 1.406861133380775,\n",
       " 1.4241654130597317,\n",
       " 1.408113238990028,\n",
       " 1.3960026078026893,\n",
       " 1.3769344581265255,\n",
       " 1.3602162584562167,\n",
       " 1.3510602361885544,\n",
       " 1.3144248871753756,\n",
       " 1.3113818877068448,\n",
       " 1.2803005053462204,\n",
       " 1.3196117797014764,\n",
       " 1.3449032996135588,\n",
       " 1.3353689432769134,\n",
       " 1.3199011602244783,\n",
       " 1.3160710907731525,\n",
       " 1.3691532748416853,\n",
       " 1.3795726626611295,\n",
       " 1.3743367893850271,\n",
       " 1.366714371289849,\n",
       " 1.3644955996144592,\n",
       " 1.388627908443155,\n",
       " 1.414817971769072,\n",
       " 1.3950066658859774,\n",
       " 1.3753901570086384,\n",
       " 1.3827839982704724,\n",
       " 1.3914761109059297,\n",
       " 1.37095261365786,\n",
       " 1.3330645960383016,\n",
       " 1.335185969210813,\n",
       " 1.3486720023781,\n",
       " 1.3386095547368324,\n",
       " 1.3903574356156483,\n",
       " 1.3969152261446856,\n",
       " 1.4146901714213649,\n",
       " 1.396393890805581,\n",
       " 1.4084380883319971,\n",
       " 1.4419510552818133,\n",
       " 1.428986920450758,\n",
       " 1.409242048228586,\n",
       " 1.4094852629872174,\n",
       " 1.397149432949294,\n",
       " 1.3678364245628885,\n",
       " 1.3731331015286496,\n",
       " 1.3836954906182157,\n",
       " 1.3913415545927048,\n",
       " 1.3974112266130994,\n",
       " 1.4114782728148954,\n",
       " 1.419502670858368,\n",
       " 1.4242340987091602,\n",
       " 1.4374746651293089,\n",
       " 1.4330354327871508,\n",
       " 1.4141271742949022,\n",
       " 1.4044728995703206,\n",
       " 1.4019112626449153,\n",
       " 1.4227933890625417,\n",
       " 1.4535555520524621,\n",
       " 1.4539367011070774,\n",
       " 1.4464116815147774,\n",
       " 1.4478180483366812,\n",
       " 1.4555772747335896,\n",
       " 1.43067365984164,\n",
       " 1.4492964787907718,\n",
       " 1.4423147514255086,\n",
       " 1.4602540918631153,\n",
       " 1.4626907434264456,\n",
       " 1.4514009620494894,\n",
       " 1.4120164980677936,\n",
       " 1.4013955572770755,\n",
       " 1.4109845243349874,\n",
       " 1.4122450749011373,\n",
       " 1.4226869826056405,\n",
       " 1.3806417942042821,\n",
       " 1.404660940610559,\n",
       " 1.3926251880410399,\n",
       " 1.3890890030897283,\n",
       " 1.3307777017106104,\n",
       " 1.3438808968319025,\n",
       " 1.3553801131399021,\n",
       " 1.3430155702485296,\n",
       " 1.367448519542756,\n",
       " 1.3559842090565968,\n",
       " 1.3173102474484968,\n",
       " 1.3131390017385351,\n",
       " 1.2947346256744705,\n",
       " 1.3121762766522838,\n",
       " 1.3259516903425723,\n",
       " 1.3215237179429438,\n",
       " 1.2841350787745576,\n",
       " 1.3128271013304746,\n",
       " 1.3200486654716115,\n",
       " 1.3415377027915651,\n",
       " 1.3389901407943214,\n",
       " 1.326516939457541,\n",
       " 1.3228878599803626,\n",
       " 1.3279047273742712,\n",
       " 1.3563535351715563,\n",
       " 1.3458716546710745,\n",
       " 1.3165541423076577,\n",
       " 1.305575135344509,\n",
       " 1.3055880842784178,\n",
       " 1.2850285552142542,\n",
       " 1.304196355381802,\n",
       " 1.309713727221136,\n",
       " 1.3066498968589264,\n",
       " 1.228963049372596,\n",
       " 1.2094805338113552,\n",
       " 1.205406123607145,\n",
       " 1.168034374352553,\n",
       " 1.186447195373515,\n",
       " 1.1764810202408729,\n",
       " 1.11040035851657,\n",
       " 1.1238785097240864,\n",
       " 1.1048779197030978,\n",
       " 1.1533356453748658,\n",
       " 1.1451817579923071,\n",
       " 1.186655504310306,\n",
       " 1.1522028951564232,\n",
       " 1.115374438128868,\n",
       " 1.0960816525992452,\n",
       " 1.0939360705502956,\n",
       " 1.0160701899777504,\n",
       " 1.051043571473611,\n",
       " 1.0876862394494338,\n",
       " 1.101230261320746,\n",
       " 1.0991725068235252,\n",
       " 1.1489943745327125,\n",
       " 1.1336476358624665,\n",
       " 1.091757834668012,\n",
       " 1.1321134686928556,\n",
       " 1.1264936313765053,\n",
       " 1.1249290623620656,\n",
       " 1.1446322727968794,\n",
       " 1.1354244547935828,\n",
       " 1.170123656688856,\n",
       " 1.1910716537702792,\n",
       " 1.1871999225315952,\n",
       " 1.1827359183158728,\n",
       " 1.1797875023645878,\n",
       " 1.233601582697522,\n",
       " 1.2517272751839874,\n",
       " 1.246523492744093,\n",
       " 1.2339973696774251,\n",
       " 1.208272904975093,\n",
       " 1.2159780836478609,\n",
       " 1.2004782097592175,\n",
       " 1.20071016457532,\n",
       " 1.1753679749218562,\n",
       " 1.1826435867871328,\n",
       " 1.1936895904083307,\n",
       " 1.2373365056344752,\n",
       " 1.2083078107969336,\n",
       " 1.214215902642033,\n",
       " 1.2148560303748206,\n",
       " 1.1882003684253197,\n",
       " 1.1804281930945026,\n",
       " 1.141909618693306,\n",
       " 1.1407847504346336,\n",
       " 1.1542426337455973,\n",
       " 1.1449875239836778,\n",
       " 1.1534279769036058,\n",
       " 1.1683924405249835,\n",
       " 1.1742346617062864,\n",
       " 1.171453455901561,\n",
       " 1.1814016151261564,\n",
       " 1.1716972336573193,\n",
       " 1.1799654094565502,\n",
       " 1.1638952194787997,\n",
       " 1.1652672434759892,\n",
       " 1.1535355093547603,\n",
       " 1.1546417987082593,\n",
       " 1.166118495131201,\n",
       " 1.1870952050660732,\n",
       " 1.206855841207786,\n",
       " 1.208635475124535,\n",
       " 1.2022375757794133,\n",
       " 1.204766558871483,\n",
       " 1.196526532928576,\n",
       " 1.1982909659229097,\n",
       " 1.2001319665264427,\n",
       " 1.2078647320584075,\n",
       " 1.1826542837325356,\n",
       " 1.1747064532982625,\n",
       " 1.1654029257834666,\n",
       " 1.1591880005044453,\n",
       " 1.147916798032663,\n",
       " 1.1545410222226224,\n",
       " 1.1516945087512274,\n",
       " 1.1221669984596399,\n",
       " 1.1429928251646202,\n",
       " 1.1558651914640627,\n",
       " 1.1684960319962525,\n",
       " 1.1752694504247252,\n",
       " 1.1942486465549078,\n",
       " 1.208902335762478,\n",
       " 1.204394980768018,\n",
       " 1.2478651148964537,\n",
       " 1.2380312440885302,\n",
       " 1.2353840315999025,\n",
       " 1.2430768243358883,\n",
       " 1.2366980668930667,\n",
       " 1.2414064118616737,\n",
       " 1.2155862376478428,\n",
       " 1.2233685469269364,\n",
       " 1.2265511696828297,\n",
       " 1.2835259158837253,\n",
       " 1.303730193761091,\n",
       " 1.2978862835884084,\n",
       " 1.2970429138929673,\n",
       " 1.2660020673254486,\n",
       " 1.2588401808797167,\n",
       " 1.2534489203967105,\n",
       " 1.238682068766721,\n",
       " 1.2407381342725623,\n",
       " 1.1980449361786456,\n",
       " 1.185213668669435,\n",
       " 1.1747244692063092,\n",
       " 1.1873761406321783,\n",
       " 1.20151862844892,\n",
       " 1.1783344067811878,\n",
       " 1.1339798041670797,\n",
       " 1.1611720023780996,\n",
       " 1.164779124967346,\n",
       " 1.1390450893138642,\n",
       " 1.1409924963742983,\n",
       " 1.1536295298748795,\n",
       " 1.1500848999666706,\n",
       " 1.1737268382982173,\n",
       " 1.1497161368488373,\n",
       " 1.121557272571681,\n",
       " 1.1457205462423317,\n",
       " 1.1193266779566358,\n",
       " 1.146330272130291,\n",
       " 1.1599773224757461,\n",
       " 1.171937633430319,\n",
       " 1.1751225081747183,\n",
       " 1.172115540522281,\n",
       " 1.160393377352202,\n",
       " 1.160588174357958,\n",
       " 1.202340604253556,\n",
       " 1.188306774882221,\n",
       " 1.1801444425427654,\n",
       " 1.1664850062605279,\n",
       " 1.1490928990298435,\n",
       " 1.150063506075865,\n",
       " 1.1552824894381737,\n",
       " 1.1335333474457947,\n",
       " 1.1432275949663553,\n",
       " 1.149211691423527,\n",
       " 1.1148598587552807,\n",
       " 1.117876397358868,\n",
       " 1.1262233927558034,\n",
       " 1.085069991802762,\n",
       " 1.0913671146622468,\n",
       " 1.064400678298938,\n",
       " 1.0212655274607476,\n",
       " 1.0691022673020276,\n",
       " 1.0802473584174828,\n",
       " 1.0780285867420933,\n",
       " 1.0327365939124244,\n",
       " 1.0916362272886957,\n",
       " 1.140801077351301,\n",
       " 1.1039686793438608,\n",
       " 1.0799219460783873,\n",
       " 1.070445015448641,\n",
       " 1.0880426166304846,\n",
       " 1.099174758812031,\n",
       " 1.1063991379387996,\n",
       " 1.1306412312071559,\n",
       " 1.1240828776809924,\n",
       " 1.1467699728860583,\n",
       " 1.1374332285408015,\n",
       " 1.1140879896949007,\n",
       " 1.108595389729131,\n",
       " 1.1305387657301398,\n",
       " 1.132189473304928,\n",
       " 1.129192076603641,\n",
       " 1.1311383576698222,\n",
       " 1.1476606343401223,\n",
       " 1.1529201534955367,\n",
       " 1.157108289119292,\n",
       " 1.1859906047039535,\n",
       " 1.1778237683874861,\n",
       " 1.1980742120292218,\n",
       " 1.1954191175808235,\n",
       " 1.194985046796321,\n",
       " 1.22955926332952,\n",
       " 1.2459604956176302,\n",
       " 1.2405163134047363,\n",
       " 1.2846569771107887,\n",
       " 1.271243007575689,\n",
       " 1.2754851909235851,\n",
       " 1.261565086971796,\n",
       " 1.2223686640303386,\n",
       " 1.2180352751479555,\n",
       " 1.1895836523650383,\n",
       " 1.1774521902840207,\n",
       " 1.4062829353318982,\n",
       " 1.4392616180087017,\n",
       " 1.4243489501229587,\n",
       " 1.4486878788970659,\n",
       " 1.4942698152468625,\n",
       " 1.5166393800726037,\n",
       " 1.451869938655833,\n",
       " 1.4488100492735083,\n",
       " 1.3890315773828288,\n",
       " 1.3685221550629203,\n",
       " 1.3395649608604399,\n",
       " 1.2889914919874248,\n",
       " 1.305132056605983,\n",
       " 1.3079915190112872,\n",
       " 1.2997531820597588,\n",
       " 1.2860183041625755,\n",
       " 1.2876971615936874,\n",
       " 1.2791852080386978,\n",
       " 1.264314201940313,\n",
       " 1.2309594371830328,\n",
       " 1.2438678352985688,\n",
       " 1.244460671272734,\n",
       " 1.2621325880752705,\n",
       " 1.2742606721735292,\n",
       " 1.26673396358985,\n",
       " 1.2874584508120672,\n",
       " 1.2903809688955348,\n",
       " 1.253191630709917,\n",
       " 1.2595580022159565,\n",
       " 1.2869883482114703,\n",
       " 1.2787438182915514]"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_inverse.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41064307],\n",
       "       [0.41071   ],\n",
       "       [0.41054648],\n",
       "       [0.4120195 ],\n",
       "       [0.4077616 ],\n",
       "       [0.40563706],\n",
       "       [0.4101848 ],\n",
       "       [0.41610965],\n",
       "       [0.41439262],\n",
       "       [0.4079402 ],\n",
       "       [0.40789166],\n",
       "       [0.40882316],\n",
       "       [0.4137763 ],\n",
       "       [0.41385075],\n",
       "       [0.41248858],\n",
       "       [0.4118966 ],\n",
       "       [0.40847337],\n",
       "       [0.39506638],\n",
       "       [0.39874795],\n",
       "       [0.4012557 ],\n",
       "       [0.39736018],\n",
       "       [0.40014467],\n",
       "       [0.39561188],\n",
       "       [0.38865435],\n",
       "       [0.38273656],\n",
       "       [0.3722981 ],\n",
       "       [0.3787802 ],\n",
       "       [0.38250872],\n",
       "       [0.38676453],\n",
       "       [0.3841249 ],\n",
       "       [0.37313148],\n",
       "       [0.37391818],\n",
       "       [0.3642933 ],\n",
       "       [0.35388842],\n",
       "       [0.35559326],\n",
       "       [0.34911725],\n",
       "       [0.3273221 ],\n",
       "       [0.34357464],\n",
       "       [0.35475928],\n",
       "       [0.35790998],\n",
       "       [0.35390988],\n",
       "       [0.34108543],\n",
       "       [0.34153304],\n",
       "       [0.3248486 ],\n",
       "       [0.3314326 ],\n",
       "       [0.31332725],\n",
       "       [0.3146956 ],\n",
       "       [0.3267614 ],\n",
       "       [0.32442212],\n",
       "       [0.32068732],\n",
       "       [0.33280623],\n",
       "       [0.33991736],\n",
       "       [0.34418643],\n",
       "       [0.35536608],\n",
       "       [0.34958336],\n",
       "       [0.34241676],\n",
       "       [0.34384552],\n",
       "       [0.34997076],\n",
       "       [0.3533289 ],\n",
       "       [0.35936403],\n",
       "       [0.36105025],\n",
       "       [0.36549416],\n",
       "       [0.35847846],\n",
       "       [0.3598355 ],\n",
       "       [0.35083827],\n",
       "       [0.35416672],\n",
       "       [0.3502341 ],\n",
       "       [0.33180442],\n",
       "       [0.33312112],\n",
       "       [0.3376574 ],\n",
       "       [0.3501552 ],\n",
       "       [0.34999377],\n",
       "       [0.35013303],\n",
       "       [0.34669927],\n",
       "       [0.34265432],\n",
       "       [0.34691143],\n",
       "       [0.33657458],\n",
       "       [0.34003076],\n",
       "       [0.3227723 ],\n",
       "       [0.32997224],\n",
       "       [0.328765  ],\n",
       "       [0.33709005],\n",
       "       [0.3304601 ],\n",
       "       [0.3318363 ],\n",
       "       [0.33691773],\n",
       "       [0.34994596],\n",
       "       [0.3450871 ],\n",
       "       [0.3431826 ],\n",
       "       [0.34922338],\n",
       "       [0.34345704],\n",
       "       [0.34209606],\n",
       "       [0.34569654],\n",
       "       [0.34198833],\n",
       "       [0.34622887],\n",
       "       [0.34752825],\n",
       "       [0.3479217 ],\n",
       "       [0.35801086],\n",
       "       [0.3607735 ],\n",
       "       [0.36327305],\n",
       "       [0.3743436 ],\n",
       "       [0.37320703],\n",
       "       [0.37541357],\n",
       "       [0.37971252],\n",
       "       [0.3762144 ],\n",
       "       [0.37579924],\n",
       "       [0.37220398],\n",
       "       [0.37663668],\n",
       "       [0.37718576],\n",
       "       [0.38036272],\n",
       "       [0.3846837 ],\n",
       "       [0.38626313],\n",
       "       [0.38109455],\n",
       "       [0.38703927],\n",
       "       [0.38600984],\n",
       "       [0.38493595],\n",
       "       [0.39264843],\n",
       "       [0.39894035],\n",
       "       [0.3978065 ],\n",
       "       [0.39501145],\n",
       "       [0.3967326 ],\n",
       "       [0.39576134],\n",
       "       [0.4015147 ],\n",
       "       [0.39602172],\n",
       "       [0.39014682],\n",
       "       [0.39199445],\n",
       "       [0.39421946],\n",
       "       [0.39709613],\n",
       "       [0.39395127],\n",
       "       [0.3950481 ],\n",
       "       [0.38924375],\n",
       "       [0.3862351 ],\n",
       "       [0.38004425],\n",
       "       [0.38195926],\n",
       "       [0.3863942 ],\n",
       "       [0.3845609 ],\n",
       "       [0.38888502],\n",
       "       [0.38646844],\n",
       "       [0.3826163 ],\n",
       "       [0.3794662 ],\n",
       "       [0.37828413],\n",
       "       [0.37023255],\n",
       "       [0.35602558],\n",
       "       [0.36252913],\n",
       "       [0.36045474],\n",
       "       [0.35456288],\n",
       "       [0.3517408 ],\n",
       "       [0.35652187],\n",
       "       [0.36348012],\n",
       "       [0.367096  ],\n",
       "       [0.36913872],\n",
       "       [0.37321344],\n",
       "       [0.3715882 ],\n",
       "       [0.3628662 ],\n",
       "       [0.36640894],\n",
       "       [0.36665496],\n",
       "       [0.3592917 ],\n",
       "       [0.3620028 ],\n",
       "       [0.3670465 ],\n",
       "       [0.3763049 ],\n",
       "       [0.3760326 ],\n",
       "       [0.37585464],\n",
       "       [0.37846765],\n",
       "       [0.3754192 ],\n",
       "       [0.37312654],\n",
       "       [0.37380132],\n",
       "       [0.3765488 ],\n",
       "       [0.37380818],\n",
       "       [0.37054715],\n",
       "       [0.36643463],\n",
       "       [0.36504498],\n",
       "       [0.36864564],\n",
       "       [0.36027542],\n",
       "       [0.3623499 ],\n",
       "       [0.36100224],\n",
       "       [0.36497393],\n",
       "       [0.36824247],\n",
       "       [0.36521047],\n",
       "       [0.36292195],\n",
       "       [0.35931677],\n",
       "       [0.35615405],\n",
       "       [0.3544213 ],\n",
       "       [0.3474833 ],\n",
       "       [0.3469067 ],\n",
       "       [0.34101442],\n",
       "       [0.34846607],\n",
       "       [0.35325578],\n",
       "       [0.35145056],\n",
       "       [0.34852087],\n",
       "       [0.3477952 ],\n",
       "       [0.35784495],\n",
       "       [0.3598157 ],\n",
       "       [0.35882545],\n",
       "       [0.35738352],\n",
       "       [0.35696378],\n",
       "       [0.36152783],\n",
       "       [0.36647704],\n",
       "       [0.36273366],\n",
       "       [0.35902467],\n",
       "       [0.36042297],\n",
       "       [0.36206633],\n",
       "       [0.35818535],\n",
       "       [0.3510142 ],\n",
       "       [0.35141593],\n",
       "       [0.35396925],\n",
       "       [0.35206422],\n",
       "       [0.36185482],\n",
       "       [0.3630944 ],\n",
       "       [0.36645296],\n",
       "       [0.36299583],\n",
       "       [0.36527184],\n",
       "       [0.3715999 ],\n",
       "       [0.36915284],\n",
       "       [0.3654237 ],\n",
       "       [0.3654697 ],\n",
       "       [0.36313862],\n",
       "       [0.35759583],\n",
       "       [0.3585978 ],\n",
       "       [0.3605953 ],\n",
       "       [0.36204085],\n",
       "       [0.36318815],\n",
       "       [0.36584625],\n",
       "       [0.36736193],\n",
       "       [0.36825538],\n",
       "       [0.3707551 ],\n",
       "       [0.36991715],\n",
       "       [0.3663466 ],\n",
       "       [0.36452264],\n",
       "       [0.36403856],\n",
       "       [0.36798337],\n",
       "       [0.3737894 ],\n",
       "       [0.37386128],\n",
       "       [0.37244162],\n",
       "       [0.37270698],\n",
       "       [0.37417075],\n",
       "       [0.36947128],\n",
       "       [0.37298587],\n",
       "       [0.37166855],\n",
       "       [0.37505278],\n",
       "       [0.37551233],\n",
       "       [0.37338296],\n",
       "       [0.36594793],\n",
       "       [0.36394116],\n",
       "       [0.3657529 ],\n",
       "       [0.36599106],\n",
       "       [0.3679633 ],\n",
       "       [0.36001787],\n",
       "       [0.3645582 ],\n",
       "       [0.36228353],\n",
       "       [0.361615  ],\n",
       "       [0.3505811 ],\n",
       "       [0.35306224],\n",
       "       [0.35523888],\n",
       "       [0.35289842],\n",
       "       [0.3575224 ],\n",
       "       [0.3553532 ],\n",
       "       [0.34803   ],\n",
       "       [0.34723964],\n",
       "       [0.34375137],\n",
       "       [0.3470572 ],\n",
       "       [0.349667  ],\n",
       "       [0.34882826],\n",
       "       [0.34174162],\n",
       "       [0.34718058],\n",
       "       [0.34854883],\n",
       "       [0.35261866],\n",
       "       [0.35213628],\n",
       "       [0.3497741 ],\n",
       "       [0.3490867 ],\n",
       "       [0.35003698],\n",
       "       [0.35542306],\n",
       "       [0.35343912],\n",
       "       [0.3478867 ],\n",
       "       [0.3458062 ],\n",
       "       [0.34580874],\n",
       "       [0.34191105],\n",
       "       [0.34554496],\n",
       "       [0.34659058],\n",
       "       [0.34600994],\n",
       "       [0.3312718 ],\n",
       "       [0.32757136],\n",
       "       [0.32679728],\n",
       "       [0.31969413],\n",
       "       [0.32319453],\n",
       "       [0.3213001 ],\n",
       "       [0.30872983],\n",
       "       [0.3112949 ],\n",
       "       [0.3076787 ],\n",
       "       [0.31689894],\n",
       "       [0.31534803],\n",
       "       [0.32323408],\n",
       "       [0.3166835 ],\n",
       "       [0.30967653],\n",
       "       [0.3060042 ],\n",
       "       [0.3055957 ],\n",
       "       [0.29076353],\n",
       "       [0.29742718],\n",
       "       [0.30440578],\n",
       "       [0.3069843 ],\n",
       "       [0.3065926 ],\n",
       "       [0.31607324],\n",
       "       [0.31315377],\n",
       "       [0.305181  ],\n",
       "       [0.3128619 ],\n",
       "       [0.31179255],\n",
       "       [0.31149486],\n",
       "       [0.3152435 ],\n",
       "       [0.31349185],\n",
       "       [0.32009143],\n",
       "       [0.32407346],\n",
       "       [0.32333758],\n",
       "       [0.3224891 ],\n",
       "       [0.32192862],\n",
       "       [0.3321526 ],\n",
       "       [0.33559343],\n",
       "       [0.33460572],\n",
       "       [0.3322277 ],\n",
       "       [0.327342  ],\n",
       "       [0.32880569],\n",
       "       [0.325861  ],\n",
       "       [0.32590508],\n",
       "       [0.32108846],\n",
       "       [0.3224715 ],\n",
       "       [0.32457098],\n",
       "       [0.3328617 ],\n",
       "       [0.32734865],\n",
       "       [0.32847095],\n",
       "       [0.32859257],\n",
       "       [0.32352772],\n",
       "       [0.32205042],\n",
       "       [0.31472558],\n",
       "       [0.31451157],\n",
       "       [0.31707147],\n",
       "       [0.31531107],\n",
       "       [0.31691653],\n",
       "       [0.31976223],\n",
       "       [0.320873  ],\n",
       "       [0.32034424],\n",
       "       [0.32223547],\n",
       "       [0.3203906 ],\n",
       "       [0.32196245],\n",
       "       [0.3189071 ],\n",
       "       [0.31916803],\n",
       "       [0.31693694],\n",
       "       [0.31714737],\n",
       "       [0.31932986],\n",
       "       [0.3233177 ],\n",
       "       [0.32707274],\n",
       "       [0.32741088],\n",
       "       [0.3261953 ],\n",
       "       [0.3266758 ],\n",
       "       [0.3251101 ],\n",
       "       [0.32544538],\n",
       "       [0.3257952 ],\n",
       "       [0.32726443],\n",
       "       [0.32247356],\n",
       "       [0.3209627 ],\n",
       "       [0.31919378],\n",
       "       [0.31801194],\n",
       "       [0.3158683 ],\n",
       "       [0.31712818],\n",
       "       [0.31658685],\n",
       "       [0.3109692 ],\n",
       "       [0.31493163],\n",
       "       [0.31738   ],\n",
       "       [0.31978193],\n",
       "       [0.32106972],\n",
       "       [0.3246772 ],\n",
       "       [0.32746157],\n",
       "       [0.32660523],\n",
       "       [0.33486035],\n",
       "       [0.33299363],\n",
       "       [0.33249104],\n",
       "       [0.3339515 ],\n",
       "       [0.33274052],\n",
       "       [0.33363438],\n",
       "       [0.32873124],\n",
       "       [0.33020937],\n",
       "       [0.3308138 ],\n",
       "       [0.3416261 ],\n",
       "       [0.3454566 ],\n",
       "       [0.34434882],\n",
       "       [0.344189  ],\n",
       "       [0.3383022 ],\n",
       "       [0.3369433 ],\n",
       "       [0.33592016],\n",
       "       [0.33311716],\n",
       "       [0.3335075 ],\n",
       "       [0.32539862],\n",
       "       [0.32296008],\n",
       "       [0.3209661 ],\n",
       "       [0.32337108],\n",
       "       [0.3260587 ],\n",
       "       [0.3216524 ],\n",
       "       [0.31321698],\n",
       "       [0.3183893 ],\n",
       "       [0.31907517],\n",
       "       [0.3141806 ],\n",
       "       [0.3145511 ],\n",
       "       [0.31695485],\n",
       "       [0.31628063],\n",
       "       [0.32077646],\n",
       "       [0.31621054],\n",
       "       [0.31085318],\n",
       "       [0.31545055],\n",
       "       [0.3104287 ],\n",
       "       [0.3155665 ],\n",
       "       [0.31816205],\n",
       "       [0.3204363 ],\n",
       "       [0.32104182],\n",
       "       [0.32047012],\n",
       "       [0.31824118],\n",
       "       [0.31827822],\n",
       "       [0.32621482],\n",
       "       [0.323548  ],\n",
       "       [0.32199645],\n",
       "       [0.31939954],\n",
       "       [0.31609198],\n",
       "       [0.3162766 ],\n",
       "       [0.3172692 ],\n",
       "       [0.31313202],\n",
       "       [0.3149763 ],\n",
       "       [0.31611454],\n",
       "       [0.3095786 ],\n",
       "       [0.3101527 ],\n",
       "       [0.3117411 ],\n",
       "       [0.30390766],\n",
       "       [0.30510664],\n",
       "       [0.29997146],\n",
       "       [0.29175356],\n",
       "       [0.30086693],\n",
       "       [0.30298936],\n",
       "       [0.30256686],\n",
       "       [0.2939394 ],\n",
       "       [0.30515784],\n",
       "       [0.3145147 ],\n",
       "       [0.30750564],\n",
       "       [0.30292737],\n",
       "       [0.30112264],\n",
       "       [0.30447364],\n",
       "       [0.30659303],\n",
       "       [0.30796823],\n",
       "       [0.31258172],\n",
       "       [0.3113338 ],\n",
       "       [0.31565017],\n",
       "       [0.313874  ],\n",
       "       [0.3094317 ],\n",
       "       [0.30838633],\n",
       "       [0.31256226],\n",
       "       [0.31287634],\n",
       "       [0.31230602],\n",
       "       [0.31267637],\n",
       "       [0.31581956],\n",
       "       [0.31681994],\n",
       "       [0.31761646],\n",
       "       [0.32310772],\n",
       "       [0.32155532],\n",
       "       [0.3254042 ],\n",
       "       [0.32489967],\n",
       "       [0.32481718],\n",
       "       [0.331385  ],\n",
       "       [0.33449888],\n",
       "       [0.33346543],\n",
       "       [0.3418406 ],\n",
       "       [0.33929643],\n",
       "       [0.34010112],\n",
       "       [0.3374603 ],\n",
       "       [0.33001953],\n",
       "       [0.32919642],\n",
       "       [0.32379064],\n",
       "       [0.3214847 ],\n",
       "       [0.36486468],\n",
       "       [0.37109232],\n",
       "       [0.36827713],\n",
       "       [0.3728711 ],\n",
       "       [0.3814638 ],\n",
       "       [0.38567525],\n",
       "       [0.37347135],\n",
       "       [0.3728941 ],\n",
       "       [0.36160412],\n",
       "       [0.3577255 ],\n",
       "       [0.35224512],\n",
       "       [0.34266254],\n",
       "       [0.34572226],\n",
       "       [0.34626424],\n",
       "       [0.34470278],\n",
       "       [0.3420987 ],\n",
       "       [0.34241706],\n",
       "       [0.3408029 ],\n",
       "       [0.33798194],\n",
       "       [0.3316509 ],\n",
       "       [0.33410165],\n",
       "       [0.33421418],\n",
       "       [0.337568  ],\n",
       "       [0.3398688 ],\n",
       "       [0.33844104],\n",
       "       [0.34237185],\n",
       "       [0.342926  ],\n",
       "       [0.3358714 ],\n",
       "       [0.33707947],\n",
       "       [0.34228268]], dtype=float32)"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_inverse.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [718, 500]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-655-339fed75d7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY_inverse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_inverse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;36m0.825\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \"\"\"\n\u001b[0;32m--> 335\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    336\u001b[0m         y_true, y_pred, multioutput)\n\u001b[1;32m    337\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0margument\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [718, 500]"
     ]
    }
   ],
   "source": [
    "mean_squared_error(testY_inverse.tolist(), yhat_inverse.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96674601, 0.98466621, 0.98644866, 0.99985756, 0.99000849,\n",
       "       0.99752788, 0.98877384, 0.99308077, 1.01654874, 1.00370452,\n",
       "       1.00954956, 1.0056767 , 1.00313195, 1.00246818, 0.99879856,\n",
       "       0.99359253, 1.00508949, 1.00623182, 1.01143841, 1.00728575,\n",
       "       1.02026001, 1.01236116, 1.00440207, 1.01096155, 0.99833184,\n",
       "       0.99763203, 0.99616261, 1.00721199, 1.00396237, 1.00937784,\n",
       "       1.01222492, 1.0044854 , 1.01488396, 1.0125965 , 1.01395726,\n",
       "       1.01437501, 1.02736616, 1.03422178, 1.03686956, 1.04043783,\n",
       "       1.02839926, 1.03181947, 1.03617257, 1.08313216, 1.13137988,\n",
       "       1.12175826, 1.14619177, 1.18847061, 1.21089422, 1.21563015,\n",
       "       1.21982955, 1.08993035, 1.14312119, 1.16890702, 1.17069172,\n",
       "       1.18664424, 1.19338613, 1.20218409, 1.19397784, 1.22976645,\n",
       "       1.25731221, 1.27111746, 1.29947957, 1.29334177, 1.28391945,\n",
       "       1.26610678, 1.29675522, 1.27872693, 1.26727275, 1.26609778,\n",
       "       1.24898998, 1.19633399, 1.21027549, 1.20328475, 1.20267277,\n",
       "       1.25678862, 1.24854465, 1.223882  , 1.21910047, 1.20827234,\n",
       "       1.22775824, 1.24366966, 1.24736743, 1.23381214, 1.20628215,\n",
       "       1.23540486, 1.16557914, 1.16251531, 1.10352279, 1.06255292,\n",
       "       1.08980311, 1.11757013, 1.12583661, 1.16744998, 1.14580781,\n",
       "       1.12012444, 1.10244296, 1.11295299, 1.14102008, 1.11816015,\n",
       "       1.10484132, 1.12288144, 1.1609468 , 1.18324374, 1.20419005,\n",
       "       1.1788974 , 1.11476753, 1.09721497, 1.07092694, 1.11067623,\n",
       "       1.09325034, 1.11443592, 1.13540644, 1.12284203, 1.1484387 ,\n",
       "       1.1385778 , 1.13111922, 1.17992881, 1.16678565, 1.18950596,\n",
       "       1.20491181, 1.24211691, 1.26324789, 1.27672322, 1.26031073,\n",
       "       1.28882203, 1.27398199, 1.27001736, 1.26426241, 1.26833006,\n",
       "       1.28325568, 1.35289898, 1.33226852, 1.34004463, 1.31303316,\n",
       "       1.3072889 , 1.27027015, 1.30076151, 1.33012969, 1.32898512,\n",
       "       1.30761318, 1.2895072 , 1.3091451 , 1.31037018, 1.32717846,\n",
       "       1.34636146, 1.32297963, 1.38989521, 1.38035692, 1.38856204,\n",
       "       1.43227257, 1.45350094, 1.43828651, 1.44342836, 1.47217893,\n",
       "       1.47127363, 1.46724821, 1.45455431, 1.43473963, 1.45388491,\n",
       "       1.47020788, 1.44483247, 1.49014924, 1.49898154, 1.5106215 ,\n",
       "       1.52512375, 1.55300505, 1.56210252, 1.59679328, 1.53634372,\n",
       "       1.55814071, 1.56130251, 1.59436057, 1.59271718, 1.58031548,\n",
       "       1.5441508 , 1.53615286, 1.45324027, 1.51861944, 1.537517  ,\n",
       "       1.49563114, 1.52794324, 1.55113309, 1.53354956, 1.57385509,\n",
       "       1.65228622, 1.68339125, 1.68416086, 1.68548334, 1.69355841,\n",
       "       1.65843358, 1.68706424, 1.6681222 , 1.60464033, 1.62990596,\n",
       "       1.55637853, 1.61163163, 1.64987321, 1.64544468, 1.62688323,\n",
       "       1.62106015, 1.60876936, 1.62383685, 1.62330876, 1.57151077,\n",
       "       1.55290878, 1.58983576, 1.62106859, 1.62892015, 1.63277499,\n",
       "       1.6434922 , 1.65909229, 1.67183517, 1.65952017, 1.6450213 ,\n",
       "       1.61458118, 1.63785379, 1.64968686, 1.65004493, 1.64917059,\n",
       "       1.65704467, 1.63429227, 1.62294844, 1.64723782, 1.67892386,\n",
       "       1.66973631, 1.63524598, 1.63498701, 1.63996277, 1.6664394 ,\n",
       "       1.66683744, 1.65955282, 1.65638765, 1.63809419, 1.56659299,\n",
       "       1.58620499, 1.59957336, 1.57881003, 1.59364951, 1.56949805,\n",
       "       1.5324748 , 1.50102803, 1.44565107, 1.48002542, 1.49981815,\n",
       "       1.52242812, 1.50840217, 1.45006835, 1.45423847, 1.40325908,\n",
       "       1.34824525, 1.35725264, 1.32304944, 1.20816819, 1.2938023 ,\n",
       "       1.35284606, 1.36949727, 1.34835841, 1.2806749 , 1.28303498,\n",
       "       1.19515057, 1.2298098 , 1.13455969, 1.14175198, 1.20521696,\n",
       "       1.19290646, 1.17325786, 1.23704431, 1.27451627, 1.2970294 ,\n",
       "       1.35605233, 1.32550974, 1.28769547, 1.29523119, 1.32755511,\n",
       "       1.34528952, 1.37718443, 1.38610174, 1.40961475, 1.37250254,\n",
       "       1.37967738, 1.33213565, 1.34971524, 1.32894571, 1.2317679 ,\n",
       "       1.2387029 , 1.26260382, 1.32852909, 1.32767671, 1.32841199,\n",
       "       1.31028742, 1.28894814, 1.31140666, 1.25689728, 1.27511418,\n",
       "       1.18422561, 1.22211982, 1.21576414, 1.25961374, 1.22468878,\n",
       "       1.23193567, 1.25870562, 1.32742449, 1.3017811 , 1.29173441,\n",
       "       1.32360962, 1.29318188, 1.28600423, 1.30499637, 1.28543617,\n",
       "       1.30780517, 1.31466191, 1.31673881, 1.37003042, 1.38463795,\n",
       "       1.3978605 , 1.45649383, 1.45046864, 1.46216716, 1.48497361,\n",
       "       1.46641441, 1.46421252, 1.44515226, 1.46865401, 1.47156639,\n",
       "       1.48842478, 1.51137085, 1.51976345, 1.49230946, 1.52388853,\n",
       "       1.51841732, 1.51271079, 1.55372175, 1.58723021, 1.58118813,\n",
       "       1.56630079, 1.57546695, 1.57029413, 1.60095439, 1.57168079,\n",
       "       1.54041193, 1.5502413 , 1.56208395, 1.57740366, 1.56065618,\n",
       "       1.56649559, 1.53560901, 1.51961426, 1.4867341 , 1.4969007 ,\n",
       "       1.52045988, 1.51071834, 1.53370157, 1.52085454, 1.50038959,\n",
       "       1.48366633, 1.47739341, 1.43470641, 1.35953728, 1.39392459,\n",
       "       1.38295233, 1.35180846, 1.33690142, 1.36216029, 1.39895609,\n",
       "       1.41809462, 1.42891204, 1.45050242, 1.44188913, 1.3957076 ,\n",
       "       1.41445709, 1.41575987, 1.37680215, 1.39114   , 1.41783282,\n",
       "       1.46689408, 1.46544999, 1.46450641, 1.4783674 , 1.46219699,\n",
       "       1.45004189, 1.45361861, 1.46818785, 1.4536552 , 1.43637288,\n",
       "       1.41459334, 1.40723722, 1.42630086, 1.38200368, 1.3929765 ,\n",
       "       1.38584783, 1.40686113, 1.42416541, 1.40811324, 1.39600261,\n",
       "       1.37693446, 1.36021626, 1.35106024, 1.31442489, 1.31138189,\n",
       "       1.28030051, 1.31961178, 1.3449033 , 1.33536894, 1.31990116,\n",
       "       1.31607109, 1.36915327, 1.37957266, 1.37433679, 1.36671437,\n",
       "       1.3644956 , 1.38862791, 1.41481797, 1.39500667, 1.37539016,\n",
       "       1.382784  , 1.39147611, 1.37095261, 1.3330646 , 1.33518597,\n",
       "       1.348672  , 1.33860955, 1.39035744, 1.39691523, 1.41469017,\n",
       "       1.39639389, 1.40843809, 1.44195106, 1.42898692, 1.40924205,\n",
       "       1.40948526, 1.39714943, 1.36783642, 1.3731331 , 1.38369549,\n",
       "       1.39134155, 1.39741123, 1.41147827, 1.41950267, 1.4242341 ,\n",
       "       1.43747467, 1.43303543, 1.41412717, 1.4044729 , 1.40191126,\n",
       "       1.42279339, 1.45355555, 1.4539367 , 1.44641168, 1.44781805,\n",
       "       1.45557727, 1.43067366, 1.44929648, 1.44231475, 1.46025409,\n",
       "       1.46269074, 1.45140096, 1.4120165 , 1.40139556, 1.41098452,\n",
       "       1.41224507, 1.42268698, 1.38064179, 1.40466094, 1.39262519,\n",
       "       1.389089  , 1.3307777 , 1.3438809 , 1.35538011, 1.34301557,\n",
       "       1.36744852, 1.35598421, 1.31731025, 1.313139  , 1.29473463,\n",
       "       1.31217628, 1.32595169, 1.32152372, 1.28413508, 1.3128271 ,\n",
       "       1.32004867, 1.3415377 , 1.33899014, 1.32651694, 1.32288786,\n",
       "       1.32790473, 1.35635354, 1.34587165, 1.31655414, 1.30557514,\n",
       "       1.30558808, 1.28502856, 1.30419636, 1.30971373, 1.3066499 ,\n",
       "       1.22896305, 1.20948053, 1.20540612, 1.16803437, 1.1864472 ,\n",
       "       1.17648102, 1.11040036, 1.12387851, 1.10487792, 1.15333565,\n",
       "       1.14518176, 1.1866555 , 1.1522029 , 1.11537444, 1.09608165,\n",
       "       1.09393607, 1.01607019, 1.05104357, 1.08768624, 1.10123026,\n",
       "       1.09917251, 1.14899437, 1.13364764, 1.09175783, 1.13211347,\n",
       "       1.12649363, 1.12492906, 1.14463227, 1.13542445, 1.17012366,\n",
       "       1.19107165, 1.18719992, 1.18273592, 1.1797875 , 1.23360158,\n",
       "       1.25172728, 1.24652349, 1.23399737, 1.2082729 , 1.21597808,\n",
       "       1.20047821, 1.20071016, 1.17536797, 1.18264359, 1.19368959,\n",
       "       1.23733651, 1.20830781, 1.2142159 , 1.21485603, 1.18820037,\n",
       "       1.18042819, 1.14190962, 1.14078475, 1.15424263, 1.14498752,\n",
       "       1.15342798, 1.16839244, 1.17423466, 1.17145346, 1.18140162,\n",
       "       1.17169723, 1.17996541, 1.16389522, 1.16526724, 1.15353551,\n",
       "       1.1546418 , 1.1661185 , 1.18709521, 1.20685584, 1.20863548,\n",
       "       1.20223758, 1.20476656, 1.19652653, 1.19829097, 1.20013197,\n",
       "       1.20786473, 1.18265428, 1.17470645, 1.16540293, 1.159188  ,\n",
       "       1.1479168 , 1.15454102, 1.15169451, 1.122167  , 1.14299283,\n",
       "       1.15586519, 1.16849603, 1.17526945, 1.19424865, 1.20890234,\n",
       "       1.20439498, 1.24786511, 1.23803124, 1.23538403, 1.24307682,\n",
       "       1.23669807, 1.24140641, 1.21558624, 1.22336855, 1.22655117,\n",
       "       1.28352592, 1.30373019, 1.29788628, 1.29704291, 1.26600207,\n",
       "       1.25884018, 1.25344892, 1.23868207, 1.24073813, 1.19804494,\n",
       "       1.18521367, 1.17472447, 1.18737614, 1.20151863, 1.17833441,\n",
       "       1.1339798 , 1.161172  , 1.16477912, 1.13904509, 1.1409925 ,\n",
       "       1.15362953, 1.1500849 , 1.17372684, 1.14971614, 1.12155727,\n",
       "       1.14572055, 1.11932668, 1.14633027, 1.15997732, 1.17193763,\n",
       "       1.17512251, 1.17211554, 1.16039338, 1.16058817, 1.2023406 ,\n",
       "       1.18830677, 1.18014444, 1.16648501, 1.1490929 , 1.15006351,\n",
       "       1.15528249, 1.13353335, 1.14322759, 1.14921169, 1.11485986,\n",
       "       1.1178764 , 1.12622339, 1.08506999, 1.09136711, 1.06440068,\n",
       "       1.02126553, 1.06910227, 1.08024736, 1.07802859, 1.03273659,\n",
       "       1.09163623, 1.14080108, 1.10396868, 1.07992195, 1.07044502,\n",
       "       1.08804262, 1.09917476, 1.10639914, 1.13064123, 1.12408288,\n",
       "       1.14676997, 1.13743323, 1.11408799, 1.10859539, 1.13053877,\n",
       "       1.13218947, 1.12919208, 1.13113836, 1.14766063, 1.15292015,\n",
       "       1.15710829, 1.1859906 , 1.17782377, 1.19807421, 1.19541912,\n",
       "       1.19498505, 1.22955926, 1.2459605 , 1.24051631, 1.28465698,\n",
       "       1.27124301, 1.27548519, 1.26156509, 1.22236866, 1.21803528,\n",
       "       1.18958365, 1.17745219, 1.40628294, 1.43926162, 1.42434895,\n",
       "       1.44868788, 1.49426982, 1.51663938, 1.45186994, 1.44881005,\n",
       "       1.38903158, 1.36852216, 1.33956496, 1.28899149, 1.30513206,\n",
       "       1.30799152, 1.29975318, 1.2860183 , 1.28769716, 1.27918521,\n",
       "       1.2643142 , 1.23095944, 1.24386784, 1.24446067, 1.26213259,\n",
       "       1.27426067, 1.26673396, 1.28745845, 1.29038097, 1.25319163,\n",
       "       1.259558  , 1.28698835, 1.27874382])"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41064307, 0.41071   , 0.41054648, 0.4120195 , 0.4077616 ,\n",
       "       0.40563706, 0.4101848 , 0.41610965, 0.41439262, 0.4079402 ,\n",
       "       0.40789166, 0.40882316, 0.4137763 , 0.41385075, 0.41248858,\n",
       "       0.4118966 , 0.40847337, 0.39506638, 0.39874795, 0.4012557 ,\n",
       "       0.39736018, 0.40014467, 0.39561188, 0.38865435, 0.38273656,\n",
       "       0.3722981 , 0.3787802 , 0.38250872, 0.38676453, 0.3841249 ,\n",
       "       0.37313148, 0.37391818, 0.3642933 , 0.35388842, 0.35559326,\n",
       "       0.34911725, 0.3273221 , 0.34357464, 0.35475928, 0.35790998,\n",
       "       0.35390988, 0.34108543, 0.34153304, 0.3248486 , 0.3314326 ,\n",
       "       0.31332725, 0.3146956 , 0.3267614 , 0.32442212, 0.32068732,\n",
       "       0.33280623, 0.33991736, 0.34418643, 0.35536608, 0.34958336,\n",
       "       0.34241676, 0.34384552, 0.34997076, 0.3533289 , 0.35936403,\n",
       "       0.36105025, 0.36549416, 0.35847846, 0.3598355 , 0.35083827,\n",
       "       0.35416672, 0.3502341 , 0.33180442, 0.33312112, 0.3376574 ,\n",
       "       0.3501552 , 0.34999377, 0.35013303, 0.34669927, 0.34265432,\n",
       "       0.34691143, 0.33657458, 0.34003076, 0.3227723 , 0.32997224,\n",
       "       0.328765  , 0.33709005, 0.3304601 , 0.3318363 , 0.33691773,\n",
       "       0.34994596, 0.3450871 , 0.3431826 , 0.34922338, 0.34345704,\n",
       "       0.34209606, 0.34569654, 0.34198833, 0.34622887, 0.34752825,\n",
       "       0.3479217 , 0.35801086, 0.3607735 , 0.36327305, 0.3743436 ,\n",
       "       0.37320703, 0.37541357, 0.37971252, 0.3762144 , 0.37579924,\n",
       "       0.37220398, 0.37663668, 0.37718576, 0.38036272, 0.3846837 ,\n",
       "       0.38626313, 0.38109455, 0.38703927, 0.38600984, 0.38493595,\n",
       "       0.39264843, 0.39894035, 0.3978065 , 0.39501145, 0.3967326 ,\n",
       "       0.39576134, 0.4015147 , 0.39602172, 0.39014682, 0.39199445,\n",
       "       0.39421946, 0.39709613, 0.39395127, 0.3950481 , 0.38924375,\n",
       "       0.3862351 , 0.38004425, 0.38195926, 0.3863942 , 0.3845609 ,\n",
       "       0.38888502, 0.38646844, 0.3826163 , 0.3794662 , 0.37828413,\n",
       "       0.37023255, 0.35602558, 0.36252913, 0.36045474, 0.35456288,\n",
       "       0.3517408 , 0.35652187, 0.36348012, 0.367096  , 0.36913872,\n",
       "       0.37321344, 0.3715882 , 0.3628662 , 0.36640894, 0.36665496,\n",
       "       0.3592917 , 0.3620028 , 0.3670465 , 0.3763049 , 0.3760326 ,\n",
       "       0.37585464, 0.37846765, 0.3754192 , 0.37312654, 0.37380132,\n",
       "       0.3765488 , 0.37380818, 0.37054715, 0.36643463, 0.36504498,\n",
       "       0.36864564, 0.36027542, 0.3623499 , 0.36100224, 0.36497393,\n",
       "       0.36824247, 0.36521047, 0.36292195, 0.35931677, 0.35615405,\n",
       "       0.3544213 , 0.3474833 , 0.3469067 , 0.34101442, 0.34846607,\n",
       "       0.35325578, 0.35145056, 0.34852087, 0.3477952 , 0.35784495,\n",
       "       0.3598157 , 0.35882545, 0.35738352, 0.35696378, 0.36152783,\n",
       "       0.36647704, 0.36273366, 0.35902467, 0.36042297, 0.36206633,\n",
       "       0.35818535, 0.3510142 , 0.35141593, 0.35396925, 0.35206422,\n",
       "       0.36185482, 0.3630944 , 0.36645296, 0.36299583, 0.36527184,\n",
       "       0.3715999 , 0.36915284, 0.3654237 , 0.3654697 , 0.36313862,\n",
       "       0.35759583, 0.3585978 , 0.3605953 , 0.36204085, 0.36318815,\n",
       "       0.36584625, 0.36736193, 0.36825538, 0.3707551 , 0.36991715,\n",
       "       0.3663466 , 0.36452264, 0.36403856, 0.36798337, 0.3737894 ,\n",
       "       0.37386128, 0.37244162, 0.37270698, 0.37417075, 0.36947128,\n",
       "       0.37298587, 0.37166855, 0.37505278, 0.37551233, 0.37338296,\n",
       "       0.36594793, 0.36394116, 0.3657529 , 0.36599106, 0.3679633 ,\n",
       "       0.36001787, 0.3645582 , 0.36228353, 0.361615  , 0.3505811 ,\n",
       "       0.35306224, 0.35523888, 0.35289842, 0.3575224 , 0.3553532 ,\n",
       "       0.34803   , 0.34723964, 0.34375137, 0.3470572 , 0.349667  ,\n",
       "       0.34882826, 0.34174162, 0.34718058, 0.34854883, 0.35261866,\n",
       "       0.35213628, 0.3497741 , 0.3490867 , 0.35003698, 0.35542306,\n",
       "       0.35343912, 0.3478867 , 0.3458062 , 0.34580874, 0.34191105,\n",
       "       0.34554496, 0.34659058, 0.34600994, 0.3312718 , 0.32757136,\n",
       "       0.32679728, 0.31969413, 0.32319453, 0.3213001 , 0.30872983,\n",
       "       0.3112949 , 0.3076787 , 0.31689894, 0.31534803, 0.32323408,\n",
       "       0.3166835 , 0.30967653, 0.3060042 , 0.3055957 , 0.29076353,\n",
       "       0.29742718, 0.30440578, 0.3069843 , 0.3065926 , 0.31607324,\n",
       "       0.31315377, 0.305181  , 0.3128619 , 0.31179255, 0.31149486,\n",
       "       0.3152435 , 0.31349185, 0.32009143, 0.32407346, 0.32333758,\n",
       "       0.3224891 , 0.32192862, 0.3321526 , 0.33559343, 0.33460572,\n",
       "       0.3322277 , 0.327342  , 0.32880569, 0.325861  , 0.32590508,\n",
       "       0.32108846, 0.3224715 , 0.32457098, 0.3328617 , 0.32734865,\n",
       "       0.32847095, 0.32859257, 0.32352772, 0.32205042, 0.31472558,\n",
       "       0.31451157, 0.31707147, 0.31531107, 0.31691653, 0.31976223,\n",
       "       0.320873  , 0.32034424, 0.32223547, 0.3203906 , 0.32196245,\n",
       "       0.3189071 , 0.31916803, 0.31693694, 0.31714737, 0.31932986,\n",
       "       0.3233177 , 0.32707274, 0.32741088, 0.3261953 , 0.3266758 ,\n",
       "       0.3251101 , 0.32544538, 0.3257952 , 0.32726443, 0.32247356,\n",
       "       0.3209627 , 0.31919378, 0.31801194, 0.3158683 , 0.31712818,\n",
       "       0.31658685, 0.3109692 , 0.31493163, 0.31738   , 0.31978193,\n",
       "       0.32106972, 0.3246772 , 0.32746157, 0.32660523, 0.33486035,\n",
       "       0.33299363, 0.33249104, 0.3339515 , 0.33274052, 0.33363438,\n",
       "       0.32873124, 0.33020937, 0.3308138 , 0.3416261 , 0.3454566 ,\n",
       "       0.34434882, 0.344189  , 0.3383022 , 0.3369433 , 0.33592016,\n",
       "       0.33311716, 0.3335075 , 0.32539862, 0.32296008, 0.3209661 ,\n",
       "       0.32337108, 0.3260587 , 0.3216524 , 0.31321698, 0.3183893 ,\n",
       "       0.31907517, 0.3141806 , 0.3145511 , 0.31695485, 0.31628063,\n",
       "       0.32077646, 0.31621054, 0.31085318, 0.31545055, 0.3104287 ,\n",
       "       0.3155665 , 0.31816205, 0.3204363 , 0.32104182, 0.32047012,\n",
       "       0.31824118, 0.31827822, 0.32621482, 0.323548  , 0.32199645,\n",
       "       0.31939954, 0.31609198, 0.3162766 , 0.3172692 , 0.31313202,\n",
       "       0.3149763 , 0.31611454, 0.3095786 , 0.3101527 , 0.3117411 ,\n",
       "       0.30390766, 0.30510664, 0.29997146, 0.29175356, 0.30086693,\n",
       "       0.30298936, 0.30256686, 0.2939394 , 0.30515784, 0.3145147 ,\n",
       "       0.30750564, 0.30292737, 0.30112264, 0.30447364, 0.30659303,\n",
       "       0.30796823, 0.31258172, 0.3113338 , 0.31565017, 0.313874  ,\n",
       "       0.3094317 , 0.30838633, 0.31256226, 0.31287634, 0.31230602,\n",
       "       0.31267637, 0.31581956, 0.31681994, 0.31761646, 0.32310772,\n",
       "       0.32155532, 0.3254042 , 0.32489967, 0.32481718, 0.331385  ,\n",
       "       0.33449888, 0.33346543, 0.3418406 , 0.33929643, 0.34010112,\n",
       "       0.3374603 , 0.33001953, 0.32919642, 0.32379064, 0.3214847 ,\n",
       "       0.36486468, 0.37109232, 0.36827713, 0.3728711 , 0.3814638 ,\n",
       "       0.38567525, 0.37347135, 0.3728941 , 0.36160412, 0.3577255 ,\n",
       "       0.35224512, 0.34266254, 0.34572226, 0.34626424, 0.34470278,\n",
       "       0.3420987 , 0.34241706, 0.3408029 , 0.33798194, 0.3316509 ,\n",
       "       0.33410165, 0.33421418, 0.337568  , 0.3398688 , 0.33844104,\n",
       "       0.34237185, 0.342926  , 0.3358714 , 0.33707947, 0.34228268],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2003, 1, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 1, 1], padding=\"VALID\", strides=[1, 2, 1, 1]](max_pooling1d/ExpandDims)' with input shapes: [?,1,1,64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1653\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 1, 1], padding=\"VALID\", strides=[1, 2, 1, 1]](max_pooling1d/ExpandDims)' with input shapes: [?,1,1,64].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ac845a5f763f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    921\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/keras/layers/pooling.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mpad_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_last'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     outputs = self.pool_function(\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mpool2d\u001b[0;34m(x, pool_size, strides, padding, data_format, pool_mode)\u001b[0m\n\u001b[1;32m   5372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5373\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5374\u001b[0;31m     x = nn.max_pool(\n\u001b[0m\u001b[1;32m   5375\u001b[0m         x, pool_size, strides, padding=padding, data_format=tf_data_format)\n\u001b[1;32m   5376\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(value, ksize, strides, padding, data_format, name, input)\u001b[0m\n\u001b[1;32m   3921\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ksize cannot be zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m     return gen_nn_ops.max_pool(\n\u001b[0m\u001b[1;32m   3924\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   5232\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NHWC\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5233\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5234\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   5235\u001b[0m         \u001b[0;34m\"MaxPool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5236\u001b[0m                    data_format=data_format, name=name)\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    740\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         compute_device)\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3317\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3319\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3320\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3321\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1814\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1816\u001b[0;31m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[1;32m   1817\u001b[0m                                 control_input_ops, op_def)\n\u001b[1;32m   1818\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 1, 1], padding=\"VALID\", strides=[1, 2, 1, 1]](max_pooling1d/ExpandDims)' with input shapes: [?,1,1,64]."
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_t.shape[1], X_train_t.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dense(n_features))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary(1)\n",
    "\n",
    "#unsucessful tested layers:\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(GRU(64, dropout=0.1, recurrent_dropout=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
