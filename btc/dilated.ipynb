{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "def convertTimeStampToUTC(data):\n",
    "    \n",
    "    return datetime.fromtimestamp(data)\n",
    "  \n",
    "def getURL(apiKey, cryptoSymbol, frequency, timestamp, maxRecords):\n",
    "    \n",
    "    API_KEY = '&api_key=' + str(apiKey)\n",
    "    limit = '&limit=' + str(maxRecords)\n",
    "    header = 'fsym=' + str(cryptoSymbol)\n",
    "    currency = '&tsym=USD'\n",
    "\n",
    "    if (timestamp == \"\"):\n",
    "        url = 'https://min-api.cryptocompare.com/data/v2/histo' + frequency + '?' + header\n",
    "    \n",
    "    else:  \n",
    "        url = 'https://min-api.cryptocompare.com/data/v2/histo' + frequency + '?' + header + '&toTs=' + str(timestamp)\n",
    "\n",
    "    return url + currency + API_KEY + limit  \n",
    "\n",
    "\n",
    "def getCryptoData(apiKey, crypto, frequency, numOfRequests, maxRecords):\n",
    "    \n",
    "    numOfRequests = 5\n",
    "    data = pd.DataFrame()\n",
    "    nextTimeStamp = 0\n",
    "\n",
    "    for i in range(0, numOfRequests):\n",
    "        if (nextTimeStamp == 0):\n",
    "            url = getURL(apiKey, crypto, frequency, \"\", maxRecords)\n",
    "    \n",
    "        else:\n",
    "            url = getURL(apiKey, crypto, frequency, nextTimeStamp, maxRecords)\n",
    "\n",
    "        response = requests.get(url).json()\n",
    "        partialData = pd.DataFrame(response.get('Data').get('Data'))\n",
    "        nextTimeStamp = partialData['time'].min() - 1\n",
    "        data = pd.concat([data, partialData])\n",
    "\n",
    "    data = data.sort_values(by = ['time'],ignore_index = True)\n",
    "    data['timeUTC'] = data['time'].apply(convertTimeStampToUTC)\n",
    "    data = data.reset_index(drop = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from pandas import Series\n",
    "import math\n",
    "import numpy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data = pd.read_csv('bitcoin2000.csv')\n",
    "data1 = data.set_index(pd.DatetimeIndex(data['timeUTC']))[['close']]\n",
    "data2=data.set_index(pd.DatetimeIndex(data['timeUTC']))[['open']]\n",
    "data3=data.set_index(pd.DatetimeIndex(data['timeUTC']))[['close','open','low','high']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-12 19:00:00</th>\n",
       "      <td>7166.14</td>\n",
       "      <td>7170.18</td>\n",
       "      <td>7149.15</td>\n",
       "      <td>7219.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 20:00:00</th>\n",
       "      <td>7199.79</td>\n",
       "      <td>7166.14</td>\n",
       "      <td>7166.03</td>\n",
       "      <td>7208.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 21:00:00</th>\n",
       "      <td>7199.97</td>\n",
       "      <td>7199.79</td>\n",
       "      <td>7194.87</td>\n",
       "      <td>7222.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 22:00:00</th>\n",
       "      <td>7201.38</td>\n",
       "      <td>7199.97</td>\n",
       "      <td>7187.29</td>\n",
       "      <td>7211.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 23:00:00</th>\n",
       "      <td>7173.50</td>\n",
       "      <td>7201.38</td>\n",
       "      <td>7152.84</td>\n",
       "      <td>7229.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 11:00:00</th>\n",
       "      <td>33675.45</td>\n",
       "      <td>33450.01</td>\n",
       "      <td>33257.39</td>\n",
       "      <td>33847.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 12:00:00</th>\n",
       "      <td>33590.20</td>\n",
       "      <td>33675.45</td>\n",
       "      <td>33505.50</td>\n",
       "      <td>33779.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 13:00:00</th>\n",
       "      <td>33576.82</td>\n",
       "      <td>33590.20</td>\n",
       "      <td>33404.87</td>\n",
       "      <td>33945.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 14:00:00</th>\n",
       "      <td>33803.02</td>\n",
       "      <td>33576.82</td>\n",
       "      <td>33467.87</td>\n",
       "      <td>33907.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 15:00:00</th>\n",
       "      <td>33897.06</td>\n",
       "      <td>33803.02</td>\n",
       "      <td>33739.63</td>\n",
       "      <td>33950.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10005 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close      open       low      high\n",
       "timeUTC                                                    \n",
       "2019-12-12 19:00:00   7166.14   7170.18   7149.15   7219.93\n",
       "2019-12-12 20:00:00   7199.79   7166.14   7166.03   7208.36\n",
       "2019-12-12 21:00:00   7199.97   7199.79   7194.87   7222.43\n",
       "2019-12-12 22:00:00   7201.38   7199.97   7187.29   7211.80\n",
       "2019-12-12 23:00:00   7173.50   7201.38   7152.84   7229.22\n",
       "...                       ...       ...       ...       ...\n",
       "2021-02-01 11:00:00  33675.45  33450.01  33257.39  33847.46\n",
       "2021-02-01 12:00:00  33590.20  33675.45  33505.50  33779.76\n",
       "2021-02-01 13:00:00  33576.82  33590.20  33404.87  33945.41\n",
       "2021-02-01 14:00:00  33803.02  33576.82  33467.87  33907.89\n",
       "2021-02-01 15:00:00  33897.06  33803.02  33739.63  33950.67\n",
       "\n",
       "[10005 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-12 19:00:00</th>\n",
       "      <td>7166.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 20:00:00</th>\n",
       "      <td>7199.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 21:00:00</th>\n",
       "      <td>7199.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 22:00:00</th>\n",
       "      <td>7201.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 23:00:00</th>\n",
       "      <td>7173.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 11:00:00</th>\n",
       "      <td>33675.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 12:00:00</th>\n",
       "      <td>33590.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 13:00:00</th>\n",
       "      <td>33576.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 14:00:00</th>\n",
       "      <td>33803.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 15:00:00</th>\n",
       "      <td>33897.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10005 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close\n",
       "timeUTC                      \n",
       "2019-12-12 19:00:00   7166.14\n",
       "2019-12-12 20:00:00   7199.79\n",
       "2019-12-12 21:00:00   7199.97\n",
       "2019-12-12 22:00:00   7201.38\n",
       "2019-12-12 23:00:00   7173.50\n",
       "...                       ...\n",
       "2021-02-01 11:00:00  33675.45\n",
       "2021-02-01 12:00:00  33590.20\n",
       "2021-02-01 13:00:00  33576.82\n",
       "2021-02-01 14:00:00  33803.02\n",
       "2021-02-01 15:00:00  33897.06\n",
       "\n",
       "[10005 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-12 19:00:00</th>\n",
       "      <td>7166.14</td>\n",
       "      <td>7219.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 20:00:00</th>\n",
       "      <td>7199.79</td>\n",
       "      <td>7208.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 21:00:00</th>\n",
       "      <td>7199.97</td>\n",
       "      <td>7222.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 22:00:00</th>\n",
       "      <td>7201.38</td>\n",
       "      <td>7211.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 23:00:00</th>\n",
       "      <td>7173.50</td>\n",
       "      <td>7229.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 11:00:00</th>\n",
       "      <td>33675.45</td>\n",
       "      <td>33847.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 12:00:00</th>\n",
       "      <td>33590.20</td>\n",
       "      <td>33779.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 13:00:00</th>\n",
       "      <td>33576.82</td>\n",
       "      <td>33945.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 14:00:00</th>\n",
       "      <td>33803.02</td>\n",
       "      <td>33907.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 15:00:00</th>\n",
       "      <td>33897.06</td>\n",
       "      <td>33950.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10005 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        close      high\n",
       "timeUTC                                \n",
       "2019-12-12 19:00:00   7166.14   7219.93\n",
       "2019-12-12 20:00:00   7199.79   7208.36\n",
       "2019-12-12 21:00:00   7199.97   7222.43\n",
       "2019-12-12 22:00:00   7201.38   7211.80\n",
       "2019-12-12 23:00:00   7173.50   7229.22\n",
       "...                       ...       ...\n",
       "2021-02-01 11:00:00  33675.45  33847.46\n",
       "2021-02-01 12:00:00  33590.20  33779.76\n",
       "2021-02-01 13:00:00  33576.82  33945.41\n",
       "2021-02-01 14:00:00  33803.02  33907.89\n",
       "2021-02-01 15:00:00  33897.06  33950.67\n",
       "\n",
       "[10005 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3[['close','high']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "values = data3[['close','low','high','open']].values.reshape(-1,4)\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07853526, 0.08642124, 0.06953334, 0.07864372],\n",
       "       [0.07943861, 0.08687709, 0.06922349, 0.07853526],\n",
       "       [0.07944345, 0.08765591, 0.06960029, 0.07943861],\n",
       "       ...,\n",
       "       [0.787544  , 0.7954491 , 0.7852376 , 0.7879032 ],\n",
       "       [0.7936165 , 0.7971504 , 0.78423285, 0.787544  ],\n",
       "       [0.79614097, 0.80448914, 0.7853785 , 0.7936165 ]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8504 1501\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(scaled) * 0.85)\n",
    "val_size = len(scaled) - train_size\n",
    "\n",
    "test_size = len(scaled) - train_size\n",
    "train, test = scaled[0:train_size,:], scaled[train_size:len(scaled),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07853526, 0.07943861, 0.07944345, 0.07948129, 0.07873285,\n",
       "        0.07950117],\n",
       "       [0.07943861, 0.07944345, 0.07948129, 0.07873285, 0.07950117,\n",
       "        0.07977526],\n",
       "       [0.07944345, 0.07948129, 0.07873285, 0.07950117, 0.07977526,\n",
       "        0.07980587],\n",
       "       ...,\n",
       "       [0.2969556 , 0.2913795 , 0.28947163, 0.29217225, 0.2951843 ,\n",
       "        0.29748446],\n",
       "       [0.2913795 , 0.28947163, 0.29217225, 0.2951843 , 0.29748446,\n",
       "        0.29883933],\n",
       "       [0.28947163, 0.29217225, 0.2951843 , 0.29748446, 0.29883933,\n",
       "        0.29901117]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    print(len(dataY))\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "values = scaled.copy()\n",
    "n_train_hours = int(len(values) * 0.8)\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8498\n",
      "1495\n"
     ]
    }
   ],
   "source": [
    "look_back = 6\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "#valX, valY = create_dataset(val, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7980\n",
      "1977\n"
     ]
    }
   ],
   "source": [
    "look_back = 24\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "#valX, valY = create_dataset(val, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8498, 6)\n",
      "(8498,)\n",
      "(1495, 6)\n",
      "(1495,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "#print(valX.shape)\n",
    "#print(valY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#valX = np.reshape(valX, (valX.shape[0], 1, valX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0],testX.shape[1], 1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8498 samples, validate on 1495 samples\n",
      "Epoch 1/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 0.0069\n",
      "Epoch 00001: val_loss improved from inf to 0.46279, saving model to saved_weights/dilated2.hdf5\n",
      "8498/8498 [==============================] - 4s 470us/sample - loss: 0.0068 - val_loss: 0.4628\n",
      "Epoch 2/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 7.8246e-04\n",
      "Epoch 00002: val_loss did not improve from 0.46279\n",
      "8498/8498 [==============================] - 1s 110us/sample - loss: 7.7450e-04 - val_loss: 0.4988\n",
      "Epoch 3/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 2.9368e-04\n",
      "Epoch 00003: val_loss did not improve from 0.46279\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 2.9136e-04 - val_loss: 0.4908\n",
      "Epoch 4/100\n",
      "8448/8498 [============================>.] - ETA: 0s - loss: 2.0773e-04\n",
      "Epoch 00004: val_loss improved from 0.46279 to 0.45951, saving model to saved_weights/dilated2.hdf5\n",
      "8498/8498 [==============================] - 1s 119us/sample - loss: 2.0885e-04 - val_loss: 0.4595\n",
      "Epoch 5/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.9685e-04\n",
      "Epoch 00005: val_loss improved from 0.45951 to 0.39722, saving model to saved_weights/dilated2.hdf5\n",
      "8498/8498 [==============================] - 1s 120us/sample - loss: 1.9587e-04 - val_loss: 0.3972\n",
      "Epoch 6/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.2450e-04\n",
      "Epoch 00006: val_loss improved from 0.39722 to 0.38038, saving model to saved_weights/dilated2.hdf5\n",
      "8498/8498 [==============================] - 1s 121us/sample - loss: 1.2399e-04 - val_loss: 0.3804\n",
      "Epoch 7/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.3805e-04\n",
      "Epoch 00007: val_loss improved from 0.38038 to 0.31494, saving model to saved_weights/dilated2.hdf5\n",
      "8498/8498 [==============================] - 1s 120us/sample - loss: 1.4192e-04 - val_loss: 0.3149\n",
      "Epoch 8/100\n",
      "8448/8498 [============================>.] - ETA: 0s - loss: 1.9738e-04\n",
      "Epoch 00008: val_loss improved from 0.31494 to 0.28573, saving model to saved_weights/dilated2.hdf5\n",
      "8498/8498 [==============================] - 1s 119us/sample - loss: 1.9733e-04 - val_loss: 0.2857\n",
      "Epoch 9/100\n",
      "8192/8498 [===========================>..] - ETA: 0s - loss: 9.1631e-05\n",
      "Epoch 00009: val_loss improved from 0.28573 to 0.21282, saving model to saved_weights/dilated2.hdf5\n",
      "8498/8498 [==============================] - 1s 119us/sample - loss: 9.5581e-05 - val_loss: 0.2128\n",
      "Epoch 10/100\n",
      "8448/8498 [============================>.] - ETA: 0s - loss: 1.1830e-04\n",
      "Epoch 00010: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 1.1950e-04 - val_loss: 0.2760\n",
      "Epoch 11/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.1831e-04\n",
      "Epoch 00011: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 1.1852e-04 - val_loss: 0.3245\n",
      "Epoch 12/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 9.9171e-05\n",
      "Epoch 00012: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 1.0000e-04 - val_loss: 0.3399\n",
      "Epoch 13/100\n",
      "8192/8498 [===========================>..] - ETA: 0s - loss: 1.0669e-04\n",
      "Epoch 00013: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 132us/sample - loss: 1.0596e-04 - val_loss: 0.4206\n",
      "Epoch 14/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.8593e-04\n",
      "Epoch 00014: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 1.8950e-04 - val_loss: 0.4204\n",
      "Epoch 15/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.2486e-04\n",
      "Epoch 00015: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 1.2344e-04 - val_loss: 0.4068\n",
      "Epoch 16/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.1798e-04\n",
      "Epoch 00016: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 1.1646e-04 - val_loss: 0.4078\n",
      "Epoch 17/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 7.3282e-05\n",
      "Epoch 00017: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 7.3812e-05 - val_loss: 0.4298\n",
      "Epoch 18/100\n",
      "8064/8498 [===========================>..] - ETA: 0s - loss: 1.9362e-04\n",
      "Epoch 00018: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 111us/sample - loss: 1.9071e-04 - val_loss: 0.4585\n",
      "Epoch 19/100\n",
      "8448/8498 [============================>.] - ETA: 0s - loss: 1.0403e-04\n",
      "Epoch 00019: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 1.0570e-04 - val_loss: 0.4607\n",
      "Epoch 20/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.6347e-04\n",
      "Epoch 00020: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 1.6098e-04 - val_loss: 0.4228\n",
      "Epoch 21/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 9.8471e-05\n",
      "Epoch 00021: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 9.8269e-05 - val_loss: 0.4602\n",
      "Epoch 22/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 8.8226e-05\n",
      "Epoch 00022: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 8.9417e-05 - val_loss: 0.4473\n",
      "Epoch 23/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 8.5668e-05\n",
      "Epoch 00023: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 8.4924e-05 - val_loss: 0.4580\n",
      "Epoch 24/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.0728e-04\n",
      "Epoch 00024: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 1.0659e-04 - val_loss: 0.4973\n",
      "Epoch 25/100\n",
      "8448/8498 [============================>.] - ETA: 0s - loss: 7.7818e-05\n",
      "Epoch 00025: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 7.8756e-05 - val_loss: 0.5356\n",
      "Epoch 26/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 9.2135e-05\n",
      "Epoch 00026: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 9.3591e-05 - val_loss: 0.4340\n",
      "Epoch 27/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 8.0105e-05\n",
      "Epoch 00027: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 7.9705e-05 - val_loss: 0.4802\n",
      "Epoch 28/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 6.9133e-05\n",
      "Epoch 00028: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 6.9583e-05 - val_loss: 0.4803\n",
      "Epoch 29/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 1.1532e-04\n",
      "Epoch 00029: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 1.1725e-04 - val_loss: 0.3252\n",
      "Epoch 30/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 9.8139e-05\n",
      "Epoch 00030: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 9.7136e-05 - val_loss: 0.2967\n",
      "Epoch 31/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 8.8969e-05\n",
      "Epoch 00031: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 8.8192e-05 - val_loss: 0.2749\n",
      "Epoch 32/100\n",
      "8448/8498 [============================>.] - ETA: 0s - loss: 1.0624e-04\n",
      "Epoch 00032: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 1.0598e-04 - val_loss: 0.3041\n",
      "Epoch 33/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 8.3777e-05\n",
      "Epoch 00033: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 8.3229e-05 - val_loss: 0.2955\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8320/8498 [============================>.] - ETA: 0s - loss: 7.4536e-05\n",
      "Epoch 00034: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 111us/sample - loss: 7.3786e-05 - val_loss: 0.2847\n",
      "Epoch 35/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 8.0785e-05\n",
      "Epoch 00035: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 119us/sample - loss: 8.0142e-05 - val_loss: 0.3004\n",
      "Epoch 36/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 8.1794e-05\n",
      "Epoch 00036: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 8.2402e-05 - val_loss: 0.3167\n",
      "Epoch 37/100\n",
      "8448/8498 [============================>.] - ETA: 0s - loss: 1.0989e-04\n",
      "Epoch 00037: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 116us/sample - loss: 1.0973e-04 - val_loss: 0.2699\n",
      "Epoch 38/100\n",
      "8448/8498 [============================>.] - ETA: 0s - loss: 7.8966e-05\n",
      "Epoch 00038: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 112us/sample - loss: 7.9295e-05 - val_loss: 0.2864\n",
      "Epoch 39/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 5.9925e-05\n",
      "Epoch 00039: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 6.0102e-05 - val_loss: 0.3411\n",
      "Epoch 40/100\n",
      "8192/8498 [===========================>..] - ETA: 0s - loss: 8.3762e-05\n",
      "Epoch 00040: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 113us/sample - loss: 8.3616e-05 - val_loss: 0.3441\n",
      "Epoch 41/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 9.2074e-05\n",
      "Epoch 00041: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 110us/sample - loss: 9.1116e-05 - val_loss: 0.3529\n",
      "Epoch 42/100\n",
      "8448/8498 [============================>.] - ETA: 0s - loss: 5.5339e-05\n",
      "Epoch 00042: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 5.5206e-05 - val_loss: 0.3478\n",
      "Epoch 43/100\n",
      "8192/8498 [===========================>..] - ETA: 0s - loss: 6.4542e-05\n",
      "Epoch 00043: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 6.3393e-05 - val_loss: 0.3569\n",
      "Epoch 44/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 7.3543e-05\n",
      "Epoch 00044: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 7.2550e-05 - val_loss: 0.3189\n",
      "Epoch 45/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 6.3598e-05\n",
      "Epoch 00045: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 6.3187e-05 - val_loss: 0.3792\n",
      "Epoch 46/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 5.0410e-05\n",
      "Epoch 00046: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 108us/sample - loss: 5.1069e-05 - val_loss: 0.3498\n",
      "Epoch 47/100\n",
      "8320/8498 [============================>.] - ETA: 0s - loss: 6.8651e-05\n",
      "Epoch 00047: val_loss did not improve from 0.21282\n",
      "8498/8498 [==============================] - 1s 109us/sample - loss: 6.8450e-05 - val_loss: 0.3750\n",
      "Epoch 48/100\n",
      "6784/8498 [======================>.......] - ETA: 0s - loss: 1.6415e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-c242179f7901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mtraining_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_complete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/spoof/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/spoof/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoof/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoof/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoof/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoof/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spoof/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "import numpy as np\n",
    "\n",
    "batch_size =128\n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "dilations = 6\n",
    "seq_length=24\n",
    "\n",
    "class TCNBlock(k.Model):\n",
    "    def __init__(self, dilation, seq_length):\n",
    "        super(TCNBlock, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.convolution0 = k.layers.Conv1D(16, kernel_size=4, strides=1, padding='causal', dilation_rate=dilation)\n",
    "        self.BatchNorm0 = k.layers.BatchNormalization()\n",
    "        self.relu0 = k.layers.ReLU()\n",
    "        self.dropout0 = k.layers.Dropout(rate=0.5)\n",
    "\n",
    "        self.convolution1 = k.layers.Conv1D(16, kernel_size=5, strides=1, padding='causal', dilation_rate=dilation)\n",
    "        self.BatchNorm1 = k.layers.BatchNormalization()\n",
    "        self.relu1 = k.layers.ReLU()\n",
    "        self.dropout1 = k.layers.Dropout(rate=0.5)\n",
    "        self.residual = k.layers.Conv1D(1, kernel_size=1, padding='same')\n",
    "\n",
    "\n",
    "    def build_block(self, dilation, training=False):\n",
    "        inputs = k.Input(shape=(6, 1))\n",
    "        output_layer1 = self.convolution0(inputs)\n",
    "        output_layer2 = self.BatchNorm0(output_layer1)\n",
    "        output_layer3 = self.relu0(output_layer2)\n",
    "        output_layer4 = self.dropout0(output_layer3, training)\n",
    "        output_layer5 = self.convolution1(output_layer4)\n",
    "        output_layer6 = self.BatchNorm1(output_layer5)\n",
    "        output_layer7 = self.relu1(output_layer6)\n",
    "        \n",
    "        output = self.dropout1(output_layer7, training)\n",
    "        residual = self.residual(output)\n",
    "        outputs = k.layers.add([inputs, residual])\n",
    "\n",
    "\n",
    "        return k.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    mdl = k.models.Sequential()\n",
    "    for dilation in range(dilations):\n",
    "        dilation_actual = int(np.power(2, dilation))\n",
    "        block = TCNBlock(dilation_actual, seq_length).build_block(dilation_actual)\n",
    "        mdl.add(block)\n",
    "    mdl.add(MaxPooling1D(pool_size=2))\n",
    "    mdl.add(Flatten())\n",
    "    mdl.add(Dense(100))\n",
    "    mdl.add(Dense(1))\n",
    "\n",
    "    return mdl\n",
    "\n",
    "\n",
    "Model_complete = build_model()\n",
    "opt = k.optimizers.Adam(learning_rate=learning_rate)\n",
    "Model_complete.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "filepath=\"saved_weights/dilated2.hdf5\"\n",
    "mc = ModelCheckpoint(filepath, monitor='val_loss',mode='min',save_best_only='True', verbose=1)\n",
    "\n",
    "# Train Model\n",
    "training_process = Model_complete.fit(trainX, trainY, epochs=epochs, verbose=1, validation_data=(testX, testY),batch_size=128, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7998 samples, validate on 1995 samples\n",
      "Epoch 1/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.0419e-04\n",
      "Epoch 00001: val_loss improved from 0.00121 to 0.00031, saving model to saved_weights/dilated1.hdf5\n",
      "7998/7998 [==============================] - 4s 554us/sample - loss: 2.0255e-04 - val_loss: 3.0522e-04\n",
      "Epoch 2/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 5.9159e-05\n",
      "Epoch 00002: val_loss did not improve from 0.00031\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 5.8828e-05 - val_loss: 0.0034\n",
      "Epoch 3/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 6.7146e-05\n",
      "Epoch 00003: val_loss did not improve from 0.00031\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 6.8500e-05 - val_loss: 0.0029\n",
      "Epoch 4/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 7.5084e-05\n",
      "Epoch 00004: val_loss did not improve from 0.00031\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 7.4838e-05 - val_loss: 0.0021\n",
      "Epoch 5/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 5.2883e-05\n",
      "Epoch 00005: val_loss did not improve from 0.00031\n",
      "7998/7998 [==============================] - 3s 329us/sample - loss: 5.2492e-05 - val_loss: 0.0029\n",
      "Epoch 6/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 3.2475e-05\n",
      "Epoch 00006: val_loss improved from 0.00031 to 0.00024, saving model to saved_weights/dilated1.hdf5\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 3.2617e-05 - val_loss: 2.4449e-04\n",
      "Epoch 7/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.7538e-05\n",
      "Epoch 00007: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 3.7947e-05 - val_loss: 4.0880e-04\n",
      "Epoch 8/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 4.9127e-05\n",
      "Epoch 00008: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 4s 469us/sample - loss: 4.9278e-05 - val_loss: 0.0132\n",
      "Epoch 9/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.9275e-05\n",
      "Epoch 00009: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 321us/sample - loss: 6.8757e-05 - val_loss: 0.0013\n",
      "Epoch 10/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 4.0435e-05\n",
      "Epoch 00010: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 4s 539us/sample - loss: 4.0295e-05 - val_loss: 5.0372e-04\n",
      "Epoch 11/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 3.2688e-05\n",
      "Epoch 00011: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 3.2635e-05 - val_loss: 0.0021\n",
      "Epoch 12/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.1799e-05\n",
      "Epoch 00012: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 3.1800e-05 - val_loss: 0.0020\n",
      "Epoch 13/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.3040e-05\n",
      "Epoch 00013: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 3.4371e-05 - val_loss: 2.9427e-04\n",
      "Epoch 14/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 6.6351e-05\n",
      "Epoch 00014: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 6.5485e-05 - val_loss: 0.0034\n",
      "Epoch 15/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 3.0865e-05\n",
      "Epoch 00015: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 348us/sample - loss: 3.0970e-05 - val_loss: 0.0020\n",
      "Epoch 16/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.6476e-05\n",
      "Epoch 00016: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 4s 496us/sample - loss: 2.6396e-05 - val_loss: 6.6513e-04\n",
      "Epoch 17/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 2.7743e-05\n",
      "Epoch 00017: val_loss did not improve from 0.00024\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 2.7648e-05 - val_loss: 8.1497e-04\n",
      "Epoch 18/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 4.2476e-05\n",
      "Epoch 00018: val_loss improved from 0.00024 to 0.00018, saving model to saved_weights/dilated1.hdf5\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 4.2230e-05 - val_loss: 1.7624e-04\n",
      "Epoch 19/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.2006e-05\n",
      "Epoch 00019: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 2.1960e-05 - val_loss: 2.7766e-04\n",
      "Epoch 20/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 3.7363e-05\n",
      "Epoch 00020: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 336us/sample - loss: 3.7213e-05 - val_loss: 0.0071\n",
      "Epoch 21/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.5129e-05\n",
      "Epoch 00021: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 2.5340e-05 - val_loss: 0.0029\n",
      "Epoch 22/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.9314e-05\n",
      "Epoch 00022: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 394us/sample - loss: 2.9297e-05 - val_loss: 2.0551e-04\n",
      "Epoch 23/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 2.1473e-05\n",
      "Epoch 00023: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 460us/sample - loss: 2.1593e-05 - val_loss: 4.8408e-04\n",
      "Epoch 24/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.3970e-05\n",
      "Epoch 00024: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 1.4063e-05 - val_loss: 2.8729e-04\n",
      "Epoch 25/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 4.8082e-05\n",
      "Epoch 00025: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 310us/sample - loss: 4.7645e-05 - val_loss: 0.0101\n",
      "Epoch 26/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.7677e-05\n",
      "Epoch 00026: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 1.7521e-05 - val_loss: 0.0032\n",
      "Epoch 27/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.4997e-05\n",
      "Epoch 00027: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 1.4975e-05 - val_loss: 3.6179e-04\n",
      "Epoch 28/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 2.2850e-05\n",
      "Epoch 00028: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 347us/sample - loss: 2.3112e-05 - val_loss: 0.0017\n",
      "Epoch 29/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.9817e-05\n",
      "Epoch 00029: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 322us/sample - loss: 1.9731e-05 - val_loss: 0.0032\n",
      "Epoch 30/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.8653e-05\n",
      "Epoch 00030: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.8662e-05 - val_loss: 5.8230e-04\n",
      "Epoch 31/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.7440e-05\n",
      "Epoch 00031: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 1.7320e-05 - val_loss: 0.0012\n",
      "Epoch 32/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 2.0995e-05\n",
      "Epoch 00032: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 357us/sample - loss: 2.1191e-05 - val_loss: 0.0086\n",
      "Epoch 33/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 3.3114e-05\n",
      "Epoch 00033: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 337us/sample - loss: 3.3138e-05 - val_loss: 0.0143\n",
      "Epoch 34/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.3519e-05\n",
      "Epoch 00034: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.3618e-05 - val_loss: 3.5333e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.8858e-05\n",
      "Epoch 00035: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.8886e-05 - val_loss: 0.0024\n",
      "Epoch 36/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.1307e-05\n",
      "Epoch 00036: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 331us/sample - loss: 2.1383e-05 - val_loss: 0.0014\n",
      "Epoch 37/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.2576e-05\n",
      "Epoch 00037: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 346us/sample - loss: 3.2456e-05 - val_loss: 0.0026\n",
      "Epoch 38/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.3723e-05\n",
      "Epoch 00038: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 630us/sample - loss: 2.4206e-05 - val_loss: 0.0033\n",
      "Epoch 39/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.6728e-05\n",
      "Epoch 00039: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 630us/sample - loss: 1.6952e-05 - val_loss: 4.5279e-04\n",
      "Epoch 40/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.3824e-05\n",
      "Epoch 00040: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 380us/sample - loss: 1.3650e-05 - val_loss: 3.3939e-04\n",
      "Epoch 41/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.2839e-05\n",
      "Epoch 00041: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 469us/sample - loss: 2.2818e-05 - val_loss: 8.5795e-04\n",
      "Epoch 42/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 2.3744e-05\n",
      "Epoch 00042: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 419us/sample - loss: 2.3481e-05 - val_loss: 3.4646e-04\n",
      "Epoch 43/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4699e-05\n",
      "Epoch 00043: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 1.4691e-05 - val_loss: 0.0012\n",
      "Epoch 44/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.4417e-05\n",
      "Epoch 00044: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 355us/sample - loss: 1.5016e-05 - val_loss: 0.0032\n",
      "Epoch 45/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.0805e-05\n",
      "Epoch 00045: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 632us/sample - loss: 2.0661e-05 - val_loss: 9.0863e-04\n",
      "Epoch 46/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.1515e-05\n",
      "Epoch 00046: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 577us/sample - loss: 2.1448e-05 - val_loss: 0.0025\n",
      "Epoch 47/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.4498e-05\n",
      "Epoch 00047: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 397us/sample - loss: 1.4435e-05 - val_loss: 0.0045\n",
      "Epoch 48/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.0385e-05\n",
      "Epoch 00048: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 2.0502e-05 - val_loss: 0.0071\n",
      "Epoch 49/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.7734e-05\n",
      "Epoch 00049: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 441us/sample - loss: 1.7653e-05 - val_loss: 0.0027\n",
      "Epoch 50/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.5485e-05\n",
      "Epoch 00050: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.5429e-05 - val_loss: 0.0057\n",
      "Epoch 51/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.7438e-05\n",
      "Epoch 00051: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 3.7114e-05 - val_loss: 0.0017\n",
      "Epoch 52/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.6991e-05\n",
      "Epoch 00052: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 1.6935e-05 - val_loss: 0.0042\n",
      "Epoch 53/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.7492e-05\n",
      "Epoch 00053: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 372us/sample - loss: 1.7486e-05 - val_loss: 0.0020\n",
      "Epoch 54/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.3509e-05\n",
      "Epoch 00054: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 1.4153e-05 - val_loss: 0.0059\n",
      "Epoch 55/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.8564e-05\n",
      "Epoch 00055: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 425us/sample - loss: 1.8413e-05 - val_loss: 0.0017\n",
      "Epoch 56/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.8002e-05\n",
      "Epoch 00056: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 568us/sample - loss: 1.7955e-05 - val_loss: 0.0042\n",
      "Epoch 57/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.9921e-05\n",
      "Epoch 00057: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 538us/sample - loss: 1.9910e-05 - val_loss: 0.0023\n",
      "Epoch 58/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.2090e-05\n",
      "Epoch 00058: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 480us/sample - loss: 2.1976e-05 - val_loss: 0.0075\n",
      "Epoch 59/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 2.1007e-05\n",
      "Epoch 00059: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 325us/sample - loss: 2.0956e-05 - val_loss: 0.0014\n",
      "Epoch 60/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.5247e-05\n",
      "Epoch 00060: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 360us/sample - loss: 1.5003e-05 - val_loss: 0.0055\n",
      "Epoch 61/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.5226e-05\n",
      "Epoch 00061: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.5156e-05 - val_loss: 0.0015\n",
      "Epoch 62/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.0380e-05\n",
      "Epoch 00062: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 330us/sample - loss: 1.0610e-05 - val_loss: 0.0024\n",
      "Epoch 63/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.7259e-05\n",
      "Epoch 00063: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 2.7254e-05 - val_loss: 9.0897e-04\n",
      "Epoch 64/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.4869e-05\n",
      "Epoch 00064: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 399us/sample - loss: 1.4955e-05 - val_loss: 0.0117\n",
      "Epoch 65/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.5659e-05\n",
      "Epoch 00065: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 325us/sample - loss: 1.5685e-05 - val_loss: 0.0048\n",
      "Epoch 66/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0925e-05\n",
      "Epoch 00066: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 401us/sample - loss: 1.1205e-05 - val_loss: 0.0015\n",
      "Epoch 67/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.5771e-05\n",
      "Epoch 00067: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 1.5716e-05 - val_loss: 0.0083\n",
      "Epoch 68/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.1588e-05\n",
      "Epoch 00068: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 365us/sample - loss: 1.1806e-05 - val_loss: 0.0057\n",
      "Epoch 69/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0754e-05\n",
      "Epoch 00069: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.0798e-05 - val_loss: 0.0057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2388e-05\n",
      "Epoch 00070: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.2374e-05 - val_loss: 0.0020\n",
      "Epoch 71/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.7430e-05\n",
      "Epoch 00071: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 325us/sample - loss: 1.7133e-05 - val_loss: 0.0030\n",
      "Epoch 72/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.6602e-05\n",
      "Epoch 00072: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 331us/sample - loss: 1.6492e-05 - val_loss: 0.0014\n",
      "Epoch 73/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.7608e-05\n",
      "Epoch 00073: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.7542e-05 - val_loss: 7.8225e-04\n",
      "Epoch 74/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.3600e-05\n",
      "Epoch 00074: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 1.3574e-05 - val_loss: 0.0034\n",
      "Epoch 75/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.3307e-05\n",
      "Epoch 00075: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 1.3281e-05 - val_loss: 2.8590e-04\n",
      "Epoch 76/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.2562e-05\n",
      "Epoch 00076: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 369us/sample - loss: 1.2481e-05 - val_loss: 0.0012\n",
      "Epoch 77/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.2703e-05\n",
      "Epoch 00077: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 1.2583e-05 - val_loss: 0.0016\n",
      "Epoch 78/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.5095e-05\n",
      "Epoch 00078: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 459us/sample - loss: 1.5622e-05 - val_loss: 4.8788e-04\n",
      "Epoch 79/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.6666e-05\n",
      "Epoch 00079: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 321us/sample - loss: 1.6500e-05 - val_loss: 8.1704e-04\n",
      "Epoch 80/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.2377e-05\n",
      "Epoch 00080: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 350us/sample - loss: 1.2469e-05 - val_loss: 0.0018\n",
      "Epoch 81/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.1203e-06\n",
      "Epoch 00081: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 9.1725e-06 - val_loss: 0.0030\n",
      "Epoch 82/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.6298e-05\n",
      "Epoch 00082: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 345us/sample - loss: 1.6227e-05 - val_loss: 0.0027\n",
      "Epoch 83/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 2.3868e-05\n",
      "Epoch 00083: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 359us/sample - loss: 2.3768e-05 - val_loss: 5.8867e-04\n",
      "Epoch 84/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 9.7626e-06\n",
      "Epoch 00084: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 489us/sample - loss: 9.7445e-06 - val_loss: 0.0048\n",
      "Epoch 85/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.5860e-06\n",
      "Epoch 00085: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 8.6368e-06 - val_loss: 0.0045\n",
      "Epoch 86/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.3037e-05\n",
      "Epoch 00086: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 320us/sample - loss: 1.3000e-05 - val_loss: 3.3652e-04\n",
      "Epoch 87/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.6086e-05\n",
      "Epoch 00087: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.6447e-05 - val_loss: 6.1233e-04\n",
      "Epoch 88/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 1.1834e-05\n",
      "Epoch 00088: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 1.1714e-05 - val_loss: 0.0014\n",
      "Epoch 89/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2101e-05\n",
      "Epoch 00089: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 1.2059e-05 - val_loss: 0.0015\n",
      "Epoch 90/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.0254e-05\n",
      "Epoch 00090: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 324us/sample - loss: 1.0290e-05 - val_loss: 5.0951e-04\n",
      "Epoch 91/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.8960e-05\n",
      "Epoch 00091: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.9067e-05 - val_loss: 0.0026\n",
      "Epoch 92/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 3.1464e-05\n",
      "Epoch 00092: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 3.1145e-05 - val_loss: 0.0043\n",
      "Epoch 93/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.0891e-06\n",
      "Epoch 00093: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 320us/sample - loss: 9.0764e-06 - val_loss: 0.0024\n",
      "Epoch 94/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.0120e-05\n",
      "Epoch 00094: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 551us/sample - loss: 1.0251e-05 - val_loss: 0.0029\n",
      "Epoch 95/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.2290e-05\n",
      "Epoch 00095: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 395us/sample - loss: 1.2225e-05 - val_loss: 0.0041\n",
      "Epoch 96/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 8.5684e-06\n",
      "Epoch 00096: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 450us/sample - loss: 8.5380e-06 - val_loss: 0.0024\n",
      "Epoch 97/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4166e-05\n",
      "Epoch 00097: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.4072e-05 - val_loss: 0.0021\n",
      "Epoch 98/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0611e-05\n",
      "Epoch 00098: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 1.0676e-05 - val_loss: 0.0078\n",
      "Epoch 99/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 9.2852e-06\n",
      "Epoch 00099: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 9.3910e-06 - val_loss: 0.0050\n",
      "Epoch 100/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.3393e-05\n",
      "Epoch 00100: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 1.3344e-05 - val_loss: 6.3234e-04\n",
      "Epoch 101/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.1575e-05\n",
      "Epoch 00101: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 1.1538e-05 - val_loss: 0.0035\n",
      "Epoch 102/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.9172e-06\n",
      "Epoch 00102: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 9.8760e-06 - val_loss: 8.6416e-04\n",
      "Epoch 103/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2296e-05\n",
      "Epoch 00103: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 310us/sample - loss: 1.2240e-05 - val_loss: 0.0020\n",
      "Epoch 104/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2849e-05\n",
      "Epoch 00104: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 1.2860e-05 - val_loss: 0.0071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.4864e-05\n",
      "Epoch 00105: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 324us/sample - loss: 1.4759e-05 - val_loss: 0.0065\n",
      "Epoch 106/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 2.3480e-05\n",
      "Epoch 00106: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 321us/sample - loss: 2.3290e-05 - val_loss: 0.0018\n",
      "Epoch 107/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0578e-05\n",
      "Epoch 00107: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.0522e-05 - val_loss: 0.0031\n",
      "Epoch 108/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 8.7072e-06\n",
      "Epoch 00108: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 338us/sample - loss: 8.8121e-06 - val_loss: 0.0015\n",
      "Epoch 109/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 1.1903e-05\n",
      "Epoch 00109: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 418us/sample - loss: 1.2349e-05 - val_loss: 0.0026\n",
      "Epoch 110/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.1265e-05\n",
      "Epoch 00110: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 479us/sample - loss: 1.1226e-05 - val_loss: 0.0014\n",
      "Epoch 111/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0931e-05\n",
      "Epoch 00111: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.0841e-05 - val_loss: 0.0049\n",
      "Epoch 112/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.5013e-06\n",
      "Epoch 00112: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 398us/sample - loss: 8.4348e-06 - val_loss: 0.0016\n",
      "Epoch 113/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0813e-05\n",
      "Epoch 00113: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.0739e-05 - val_loss: 0.0026\n",
      "Epoch 114/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1698e-05\n",
      "Epoch 00114: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 408us/sample - loss: 1.1630e-05 - val_loss: 0.0012\n",
      "Epoch 115/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.1725e-06\n",
      "Epoch 00115: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 9.1447e-06 - val_loss: 4.2308e-04\n",
      "Epoch 116/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.6922e-06\n",
      "Epoch 00116: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 6.6868e-06 - val_loss: 8.9466e-04\n",
      "Epoch 117/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.7445e-06\n",
      "Epoch 00117: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 9.6712e-06 - val_loss: 0.0014\n",
      "Epoch 118/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1030e-05\n",
      "Epoch 00118: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.1026e-05 - val_loss: 0.0048\n",
      "Epoch 119/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4836e-05\n",
      "Epoch 00119: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 1.4732e-05 - val_loss: 0.0029\n",
      "Epoch 120/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1583e-05\n",
      "Epoch 00120: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.1561e-05 - val_loss: 0.0027\n",
      "Epoch 121/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 9.4714e-06\n",
      "Epoch 00121: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 361us/sample - loss: 9.4421e-06 - val_loss: 8.2732e-04\n",
      "Epoch 122/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.9284e-06\n",
      "Epoch 00122: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 8.9919e-06 - val_loss: 0.0029\n",
      "Epoch 123/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4720e-05\n",
      "Epoch 00123: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.4655e-05 - val_loss: 0.0073\n",
      "Epoch 124/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.1636e-05\n",
      "Epoch 00124: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 327us/sample - loss: 1.1631e-05 - val_loss: 0.0019\n",
      "Epoch 125/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.5117e-06\n",
      "Epoch 00125: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 5s 632us/sample - loss: 7.5053e-06 - val_loss: 8.2899e-04\n",
      "Epoch 126/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 9.4874e-06\n",
      "Epoch 00126: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 382us/sample - loss: 9.4444e-06 - val_loss: 0.0028\n",
      "Epoch 127/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.7841e-06\n",
      "Epoch 00127: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 311us/sample - loss: 9.8038e-06 - val_loss: 0.0059\n",
      "Epoch 128/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2778e-05\n",
      "Epoch 00128: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 1.2743e-05 - val_loss: 0.0078\n",
      "Epoch 129/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.5853e-06\n",
      "Epoch 00129: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 7.5640e-06 - val_loss: 0.0056\n",
      "Epoch 130/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.5563e-06\n",
      "Epoch 00130: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 8.5659e-06 - val_loss: 0.0020\n",
      "Epoch 131/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.8936e-06\n",
      "Epoch 00131: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 341us/sample - loss: 7.8645e-06 - val_loss: 0.0014\n",
      "Epoch 132/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.0730e-05\n",
      "Epoch 00132: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 347us/sample - loss: 1.0672e-05 - val_loss: 0.0023\n",
      "Epoch 133/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.2509e-06\n",
      "Epoch 00133: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 8.2830e-06 - val_loss: 0.0015\n",
      "Epoch 134/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.5211e-06\n",
      "Epoch 00134: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 7.5478e-06 - val_loss: 0.0015\n",
      "Epoch 135/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.0787e-05\n",
      "Epoch 00135: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.0722e-05 - val_loss: 0.0019\n",
      "Epoch 136/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.9811e-06\n",
      "Epoch 00136: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 336us/sample - loss: 8.9408e-06 - val_loss: 0.0027\n",
      "Epoch 137/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.9540e-06\n",
      "Epoch 00137: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 8.8829e-06 - val_loss: 0.0020\n",
      "Epoch 138/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.9233e-06\n",
      "Epoch 00138: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 322us/sample - loss: 7.8757e-06 - val_loss: 0.0034\n",
      "Epoch 139/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.3279e-06\n",
      "Epoch 00139: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 329us/sample - loss: 9.2417e-06 - val_loss: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 1.0014e-05\n",
      "Epoch 00140: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 331us/sample - loss: 1.0215e-05 - val_loss: 0.0037\n",
      "Epoch 141/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.1342e-05\n",
      "Epoch 00141: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 318us/sample - loss: 1.1281e-05 - val_loss: 0.0037\n",
      "Epoch 142/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.6402e-06\n",
      "Epoch 00142: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 8.6425e-06 - val_loss: 0.0106\n",
      "Epoch 143/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.0518e-05\n",
      "Epoch 00143: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 346us/sample - loss: 1.0507e-05 - val_loss: 0.0056\n",
      "Epoch 144/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 6.6539e-06\n",
      "Epoch 00144: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 6.6344e-06 - val_loss: 0.0042\n",
      "Epoch 145/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.4861e-05\n",
      "Epoch 00145: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 324us/sample - loss: 1.4801e-05 - val_loss: 0.0056\n",
      "Epoch 146/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 1.0062e-05\n",
      "Epoch 00146: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 402us/sample - loss: 1.0011e-05 - val_loss: 0.0083\n",
      "Epoch 147/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 7.7323e-06\n",
      "Epoch 00147: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 478us/sample - loss: 7.7335e-06 - val_loss: 0.0066\n",
      "Epoch 148/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 6.9360e-06\n",
      "Epoch 00148: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 402us/sample - loss: 6.9391e-06 - val_loss: 0.0028\n",
      "Epoch 149/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.2794e-06\n",
      "Epoch 00149: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 435us/sample - loss: 8.2446e-06 - val_loss: 0.0073\n",
      "Epoch 150/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 9.5598e-06\n",
      "Epoch 00150: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 322us/sample - loss: 9.5354e-06 - val_loss: 0.0056\n",
      "Epoch 151/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.4849e-06\n",
      "Epoch 00151: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 374us/sample - loss: 9.3870e-06 - val_loss: 0.0077\n",
      "Epoch 152/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.4844e-05\n",
      "Epoch 00152: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 1.4875e-05 - val_loss: 0.0094\n",
      "Epoch 153/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.1569e-06\n",
      "Epoch 00153: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 9.1055e-06 - val_loss: 0.0082\n",
      "Epoch 154/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.3227e-06\n",
      "Epoch 00154: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 7.3540e-06 - val_loss: 0.0068\n",
      "Epoch 155/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.2860e-06\n",
      "Epoch 00155: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 6.3072e-06 - val_loss: 0.0072\n",
      "Epoch 156/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.1871e-06\n",
      "Epoch 00156: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 8.2972e-06 - val_loss: 0.0204\n",
      "Epoch 157/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 8.7131e-06\n",
      "Epoch 00157: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 8.6754e-06 - val_loss: 0.0078\n",
      "Epoch 158/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1608e-05\n",
      "Epoch 00158: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 1.1512e-05 - val_loss: 0.0112\n",
      "Epoch 159/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.6222e-06\n",
      "Epoch 00159: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 359us/sample - loss: 7.5938e-06 - val_loss: 0.0048\n",
      "Epoch 160/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 9.6964e-06\n",
      "Epoch 00160: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 9.6575e-06 - val_loss: 0.0038\n",
      "Epoch 161/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 7.5615e-06\n",
      "Epoch 00161: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 342us/sample - loss: 7.5672e-06 - val_loss: 0.0046\n",
      "Epoch 162/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.2593e-05\n",
      "Epoch 00162: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 1.2831e-05 - val_loss: 0.0052\n",
      "Epoch 163/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1732e-05\n",
      "Epoch 00163: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 2s 312us/sample - loss: 1.1662e-05 - val_loss: 0.0032\n",
      "Epoch 164/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 1.1510e-05\n",
      "Epoch 00164: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.1443e-05 - val_loss: 0.0057\n",
      "Epoch 165/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.6996e-06\n",
      "Epoch 00165: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 347us/sample - loss: 8.6884e-06 - val_loss: 0.0086\n",
      "Epoch 166/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.9318e-06\n",
      "Epoch 00166: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 6.9917e-06 - val_loss: 0.0053\n",
      "Epoch 167/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 9.0159e-06\n",
      "Epoch 00167: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 361us/sample - loss: 9.0123e-06 - val_loss: 0.0060\n",
      "Epoch 168/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.4872e-06\n",
      "Epoch 00168: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 323us/sample - loss: 8.5520e-06 - val_loss: 0.0170\n",
      "Epoch 169/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 1.1107e-05\n",
      "Epoch 00169: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 341us/sample - loss: 1.1108e-05 - val_loss: 0.0024\n",
      "Epoch 170/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.0714e-06\n",
      "Epoch 00170: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 7.0386e-06 - val_loss: 0.0030\n",
      "Epoch 171/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.9295e-06\n",
      "Epoch 00171: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 6.9887e-06 - val_loss: 0.0041\n",
      "Epoch 172/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.6422e-06\n",
      "Epoch 00172: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 9.6825e-06 - val_loss: 0.0096\n",
      "Epoch 173/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.3818e-06\n",
      "Epoch 00173: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 7.4525e-06 - val_loss: 0.0064\n",
      "Epoch 174/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 7.0970e-06\n",
      "Epoch 00174: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 313us/sample - loss: 7.0667e-06 - val_loss: 0.0037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.3074e-06\n",
      "Epoch 00175: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 315us/sample - loss: 8.2836e-06 - val_loss: 0.0064\n",
      "Epoch 176/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 8.7822e-06\n",
      "Epoch 00176: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 325us/sample - loss: 8.7690e-06 - val_loss: 0.0059\n",
      "Epoch 177/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 8.0099e-06\n",
      "Epoch 00177: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 341us/sample - loss: 8.0008e-06 - val_loss: 0.0043\n",
      "Epoch 178/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 7.6857e-06\n",
      "Epoch 00178: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 324us/sample - loss: 7.6950e-06 - val_loss: 0.0035\n",
      "Epoch 179/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 8.0994e-06\n",
      "Epoch 00179: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 328us/sample - loss: 8.0947e-06 - val_loss: 0.0080\n",
      "Epoch 180/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.0835e-06\n",
      "Epoch 00180: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 370us/sample - loss: 9.0625e-06 - val_loss: 0.0073\n",
      "Epoch 181/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.9132e-06\n",
      "Epoch 00181: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 1.0065e-05 - val_loss: 0.0105\n",
      "Epoch 182/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.2797e-06\n",
      "Epoch 00182: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 9.2524e-06 - val_loss: 0.0041\n",
      "Epoch 183/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.8247e-06\n",
      "Epoch 00183: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 321us/sample - loss: 8.8176e-06 - val_loss: 0.0043\n",
      "Epoch 184/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 6.4441e-06\n",
      "Epoch 00184: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 319us/sample - loss: 6.4801e-06 - val_loss: 0.0037\n",
      "Epoch 185/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.2758e-06\n",
      "Epoch 00185: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 8.1814e-06 - val_loss: 0.0054\n",
      "Epoch 186/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 9.6831e-06\n",
      "Epoch 00186: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 9.7678e-06 - val_loss: 0.0043\n",
      "Epoch 187/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.1240e-06\n",
      "Epoch 00187: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 317us/sample - loss: 9.2574e-06 - val_loss: 0.0071\n",
      "Epoch 188/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 9.6066e-06\n",
      "Epoch 00188: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 334us/sample - loss: 9.5087e-06 - val_loss: 0.0054\n",
      "Epoch 189/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 6.7323e-06\n",
      "Epoch 00189: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 4s 487us/sample - loss: 6.7740e-06 - val_loss: 0.0021\n",
      "Epoch 190/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 8.4481e-06\n",
      "Epoch 00190: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 8.3769e-06 - val_loss: 0.0055\n",
      "Epoch 191/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.5691e-06\n",
      "Epoch 00191: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 316us/sample - loss: 6.5778e-06 - val_loss: 0.0017\n",
      "Epoch 192/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 8.1873e-06\n",
      "Epoch 00192: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 326us/sample - loss: 8.2554e-06 - val_loss: 0.0098\n",
      "Epoch 193/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 7.5533e-06\n",
      "Epoch 00193: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 355us/sample - loss: 7.5318e-06 - val_loss: 0.0033\n",
      "Epoch 194/200\n",
      "7968/7998 [============================>.] - ETA: 0s - loss: 8.5234e-06\n",
      "Epoch 00194: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 425us/sample - loss: 8.5105e-06 - val_loss: 0.0039\n",
      "Epoch 195/200\n",
      "7872/7998 [============================>.] - ETA: 0s - loss: 6.0899e-06\n",
      "Epoch 00195: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 322us/sample - loss: 6.1086e-06 - val_loss: 6.7817e-04\n",
      "Epoch 196/200\n",
      "7808/7998 [============================>.] - ETA: 0s - loss: 6.7642e-06\n",
      "Epoch 00196: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 320us/sample - loss: 6.8359e-06 - val_loss: 4.4758e-04\n",
      "Epoch 197/200\n",
      "7840/7998 [============================>.] - ETA: 0s - loss: 9.1157e-06\n",
      "Epoch 00197: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 337us/sample - loss: 9.0922e-06 - val_loss: 0.0044\n",
      "Epoch 198/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 7.1899e-06\n",
      "Epoch 00198: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 6s 764us/sample - loss: 7.1981e-06 - val_loss: 0.0022\n",
      "Epoch 199/200\n",
      "7936/7998 [============================>.] - ETA: 0s - loss: 7.0518e-06\n",
      "Epoch 00199: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 372us/sample - loss: 7.0191e-06 - val_loss: 0.0043\n",
      "Epoch 200/200\n",
      "7904/7998 [============================>.] - ETA: 0s - loss: 6.9664e-06\n",
      "Epoch 00200: val_loss did not improve from 0.00018\n",
      "7998/7998 [==============================] - 3s 314us/sample - loss: 7.0075e-06 - val_loss: 0.0027\n"
     ]
    }
   ],
   "source": [
    "training_process = model.fit(trainX, trainY, epochs=200, verbose=1, validation_data=(testX, testY),batch_size=32, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('dilated_rmse660_model.h5')\n",
    "\n",
    "#model.load_model('dilated_rmse660_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model_96 (Model)             (None, 6, 1)              6913      \n",
      "_________________________________________________________________\n",
      "model_97 (Model)             (None, 6, 1)              6913      \n",
      "_________________________________________________________________\n",
      "model_98 (Model)             (None, 6, 1)              6913      \n",
      "_________________________________________________________________\n",
      "model_99 (Model)             (None, 6, 1)              6913      \n",
      "_________________________________________________________________\n",
      "model_100 (Model)            (None, 6, 1)              6913      \n",
      "_________________________________________________________________\n",
      "model_101 (Model)            (None, 6, 1)              6913      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 3, 1)              0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 41,979\n",
      "Trainable params: 40,827\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model_complete.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model_complete.load_weights('saved_weights/dilated.hdf5')\n",
    "Model_complete.load_weights('saved_weights/dilated2.hdf5')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model_complete.save('dilated_rmse660_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7494, 6, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nbeats_kerasmodel import NBeatsNet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = NBeatsNet(backcast_length=look_back, forecast_length=look_back,\n",
    "                      stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK), nb_blocks_per_stack=2,\n",
    "                      thetas_dim=(4, 4), share_weights_in_stack=True, hidden_layer_units=64)\n",
    "\n",
    "    # Definition of the objective function and the optimizer.\n",
    "model.compile_model(loss='mae', learning_rate=1e-5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history=model.fit(trainX, trainY, validation_data=(testX, testY), epochs=200, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.save('n_beats_model.h5')\n",
    "\n",
    "    # Predict on the testing set.\n",
    "predictions = model.predict(testX)\n",
    "print(predictions.shape)\n",
    "\n",
    "    # Load the model.\n",
    "model2 = NBeatsNet.load('n_beats_model.h5')\n",
    "\n",
    "predictions2 = model2.predict(testX)\n",
    "np.testing.assert_almost_equal(predictions, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(training_process.history['loss'], label='train')\n",
    "pyplot.plot(training_process.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = Model_complete.predict(testX)\n",
    "pyplot.plot(yhat, label='predict')\n",
    "pyplot.plot(testY, label='true')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = model.predict(testX)\n",
    "pyplot.plot(yhat, label='predict')\n",
    "pyplot.plot(testY, label='true')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 660.549\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "yhat_inverse = scaler.inverse_transform(yhat.reshape(-1,1))\n",
    "testY_inverse = scaler.inverse_transform(testY.reshape(-1, 1))\n",
    "rmse = sqrt(mean_squared_error(testY_inverse, yhat_inverse))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 494.510\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "yhat_inverse = scaler.inverse_transform(yhat.reshape(-1,1))\n",
    "testY_inverse = scaler.inverse_transform(testY.reshape(-1, 1))\n",
    "rmse = sqrt(mean_squared_error(testY_inverse, yhat_inverse))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shifting 하는거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1e647b3640>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "split_date = pd.Timestamp('01-01-2021')\n",
    "# 2021/1/1 까지의 데이터를 트레이닝셋.\n",
    "# 그 이후 데이터를 테스트셋으로 한다.\n",
    "\n",
    "train = data1.loc[:split_date]\n",
    "test = data1.loc[split_date:]\n",
    "# Feature는 Unadjusted 한 개\n",
    "\n",
    "ax = train.plot()\n",
    "test.plot(ax=ax)\n",
    "plt.legend(['train', 'test'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0019861 ],\n",
       "       [0.00317089],\n",
       "       [0.00241308],\n",
       "       ...,\n",
       "       [0.96353706],\n",
       "       [0.95444165],\n",
       "       [0.97917115]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "train_sc = sc.fit_transform(train)\n",
    "test_sc = sc.transform(test)\n",
    "\n",
    "train_sc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-20 05:00:00</th>\n",
       "      <td>0.001986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 06:00:00</th>\n",
       "      <td>0.003171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 07:00:00</th>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 08:00:00</th>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 09:00:00</th>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Scaled\n",
       "timeUTC                      \n",
       "2020-10-20 05:00:00  0.001986\n",
       "2020-10-20 06:00:00  0.003171\n",
       "2020-10-20 07:00:00  0.002413\n",
       "2020-10-20 08:00:00  0.003235\n",
       "2020-10-20 09:00:00  0.001791"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sc_df = pd.DataFrame(train_sc, columns=['Scaled'], index=train.index)\n",
    "test_sc_df = pd.DataFrame(test_sc, columns=['Scaled'], index=test.index)\n",
    "train_sc_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaled</th>\n",
       "      <th>shift_1</th>\n",
       "      <th>shift_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-20 05:00:00</th>\n",
       "      <td>0.001986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 06:00:00</th>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 07:00:00</th>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.001986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 08:00:00</th>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.003171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 09:00:00</th>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 10:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 11:00:00</th>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Scaled   shift_1   shift_2\n",
       "timeUTC                                          \n",
       "2020-10-20 05:00:00  0.001986       NaN       NaN\n",
       "2020-10-20 06:00:00  0.003171  0.001986       NaN\n",
       "2020-10-20 07:00:00  0.002413  0.003171  0.001986\n",
       "2020-10-20 08:00:00  0.003235  0.002413  0.003171\n",
       "2020-10-20 09:00:00  0.001791  0.003235  0.002413\n",
       "2020-10-20 10:00:00  0.000000  0.001791  0.003235\n",
       "2020-10-20 11:00:00  0.001553  0.000000  0.001791"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s in range(1, 3):\n",
    "    train_sc_df['shift_{}'.format(s)] = train_sc_df['Scaled'].shift(s)\n",
    "    test_sc_df['shift_{}'.format(s)] = test_sc_df['Scaled'].shift(s)\n",
    "\n",
    "train_sc_df.head(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_sc_df.dropna().drop('Scaled', axis=1)\n",
    "y_train = train_sc_df.dropna()[['Scaled']]\n",
    "\n",
    "X_test = test_sc_df.dropna().drop('Scaled', axis=1)\n",
    "y_test = test_sc_df.dropna()[['Scaled']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shift_1</th>\n",
       "      <th>shift_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-20 07:00:00</th>\n",
       "      <td>0.003171</td>\n",
       "      <td>0.001986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 08:00:00</th>\n",
       "      <td>0.002413</td>\n",
       "      <td>0.003171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 09:00:00</th>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 10:00:00</th>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 11:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      shift_1   shift_2\n",
       "timeUTC                                \n",
       "2020-10-20 07:00:00  0.003171  0.001986\n",
       "2020-10-20 08:00:00  0.002413  0.003171\n",
       "2020-10-20 09:00:00  0.003235  0.002413\n",
       "2020-10-20 10:00:00  0.001791  0.003235\n",
       "2020-10-20 11:00:00  0.000000  0.001791"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scaled</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-20 07:00:00</th>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 08:00:00</th>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 09:00:00</th>\n",
       "      <td>0.001791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 10:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-20 11:00:00</th>\n",
       "      <td>0.001553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Scaled\n",
       "timeUTC                      \n",
       "2020-10-20 07:00:00  0.002413\n",
       "2020-10-20 08:00:00  0.003235\n",
       "2020-10-20 09:00:00  0.001791\n",
       "2020-10-20 10:00:00  0.000000\n",
       "2020-10-20 11:00:00  0.001553"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1746, 2)\n",
      "[[0.00317089 0.0019861 ]\n",
      " [0.00241308 0.00317089]\n",
      " [0.00323499 0.00241308]\n",
      " ...\n",
      " [0.97030869 0.98571841]\n",
      " [0.96353706 0.97030869]\n",
      " [0.95444165 0.96353706]]\n",
      "(1746, 1)\n",
      "[[0.00241308]\n",
      " [0.00323499]\n",
      " [0.0017915 ]\n",
      " ...\n",
      " [0.96353706]\n",
      " [0.95444165]\n",
      " [0.97917115]]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.values\n",
    "X_test= X_test.values\n",
    "\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "print(X_train.shape)\n",
    "print(X_train)\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = X_train.reshape(X_train.shape[0], 2, 1) \n",
    "X_test_t = X_test.reshape(X_test.shape[0], 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 DATA\n",
      "(1746, 2, 1)\n",
      "[[[0.00317089]\n",
      "  [0.0019861 ]]\n",
      "\n",
      " [[0.00241308]\n",
      "  [0.00317089]]\n",
      "\n",
      " [[0.00323499]\n",
      "  [0.00241308]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.97030869]\n",
      "  [0.98571841]]\n",
      "\n",
      " [[0.96353706]\n",
      "  [0.97030869]]\n",
      "\n",
      " [[0.95444165]\n",
      "  [0.96353706]]]\n",
      "[[0.00241308]\n",
      " [0.00323499]\n",
      " [0.0017915 ]\n",
      " ...\n",
      " [0.96353706]\n",
      " [0.95444165]\n",
      " [0.97917115]]\n"
     ]
    }
   ],
   "source": [
    "print(\"최종 DATA\")\n",
    "print(X_train_t.shape)\n",
    "print(X_train_t)\n",
    "print(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 40,901\n",
      "Trainable params: 40,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "K.clear_session() \n",
    "model = Sequential() # Sequeatial Model \n",
    "model.add(LSTM(100, input_shape=(X_train_t.shape[1], X_train_t.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam') \n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 0.0359\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 6.2887e-04\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 2.1661e-04\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 1.7869e-04\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 1.4293e-04\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 1.2872e-04\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 9.8637e-05\n",
      "Epoch 8/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 8.3698e-05\n",
      "Epoch 9/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.6040e-05\n",
      "Epoch 10/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.5892e-05\n",
      "Epoch 11/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.4798e-05\n",
      "Epoch 12/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.0063e-05\n",
      "Epoch 13/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.2674e-05\n",
      "Epoch 14/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 6.9875e-05\n",
      "Epoch 15/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.1360e-05\n",
      "Epoch 16/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.4119e-05\n",
      "Epoch 17/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.1897e-05\n",
      "Epoch 18/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.3760e-05\n",
      "Epoch 19/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.2926e-05\n",
      "Epoch 20/20\n",
      "110/110 [==============================] - 0s 1ms/step - loss: 7.8779e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc0e5820d0>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', patience=1, verbose=1)\n",
    "\n",
    "model.fit(X_train_t, y_train, epochs=20,\n",
    "          batch_size=16, verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABa8UlEQVR4nO2dd3wc9Zn/39+Z7erNcu/GxtgUY1qAg0DI0VvggLQjIfgguZB2SciVQH53uUu7XDqBBEIqqUASWkjoHYxpxmCwwb1JsiWttH3m+/tjZndni6RdaVdb9H2/XrZ2Z2ZnHmlnPvPM832+zyOklCgUCoWi9tEqbYBCoVAoSoMSdIVCoagTlKArFApFnaAEXaFQKOoEJegKhUJRJ7gqdeDOzk45f/78Sh1eoVAoapLnn3++V0rZlW9dxQR9/vz5rF27tlKHVygUippECLF1pHUq5KJQKBR1ghJ0hUKhqBOUoCsUCkWdULEYukKhUADE43F27NhBJBKptClVhc/nY/bs2bjd7oI/owRdoVBUlB07dtDU1MT8+fMRQlTanKpASklfXx87duxgwYIFBX9OhVwUCkVFiUQidHR0KDF3IISgo6Oj6KeWMQVdCHGLEGKfEGL9COs/K4R40f63XghhCCHai7JCoVBMaZSY5zKev0khHvqtwOkjrZRSfl1KebiU8nDgC8AjUsr9RVuiKA9Swou/gmiw0pYoFIoyM6agSykfBQoV6MuA2yZkkaK0vPlXuPNqeOSrlbZEoZgSPPzww5x99tkA/OlPf+IrX/nKiNv29/fzgx/8oGTHLlkMXQgRwPLk/1CqfSpKwO6XrJ9PfhfiKotAoRgvhmEU/Zlzzz2Xa6+9dsT1VSvowDnAE6OFW4QQa4QQa4UQa3t6ekp4aEVeBnfBE99Ov994d+VsUSiqmC1btrBs2TLe9773cfDBB3PRRRcRCoWYP38+n//851m1ahW/+93vuP/++znuuONYtWoVF198MUNDQwDcd999LFu2jFWrVnH77ben9nvrrbfyz//8zwDs3buXCy64gMMOO4zDDjuMJ598kmuvvZbNmzdz+OGH89nPfnbCv0cp0xYvZYxwi5TyJuAmgNWrV6ved+Vm80MQc8TOi/XQX/k9zD8BmqaX1i6FYgS+9OdX2bBrsKT7XD6zmevOOWTM7TZu3MjNN9/M8ccfz4c//OGU59zR0cG6devo7e3lwgsv5G9/+xsNDQ189atf5Zvf/Caf+9znuPLKK3nwwQdZvHgxl1xySd79X3PNNZx00knccccdGIbB0NAQX/nKV1i/fj0vvvhiSX7XknjoQogW4CTgj6XYn6JExIYz3xuxwj8bHYI/XAE/ObO0NikUVcqcOXM4/vjjAXj/+9/P448/DpAS6KeffpoNGzZw/PHHc/jhh/PTn/6UrVu38vrrr7NgwQKWLFmCEIL3v//9eff/4IMPcvXVVwOg6zotLS0l/x3G9NCFELcBJwOdQogdwHWAG0BK+UN7swuA+6WUw3l3oqgMsazMlmyBd/LqHdA6F2Ydmbnt/s3lsU2hyEMhnnS5yE4TTL5vaGgArMk+p512GrfdlhmIKJV3XQoKyXK5TEo5Q0rpllLOllLeLKX8oUPMkVLeKqW8tLymKoomNowUjnt2eJRkpXs+C4//n+OzQ+nXQz0Q3Ft6+xSKKmLbtm089dRTAPzqV7/ihBNOyFh/7LHH8sQTT7Bp0yYAhoeHeeONN1i2bBlbtmxh82bL+ckW/CSnnnoqN9xwA2ANsA4MDNDU1EQwWLqUYjVTtJ6JDRMRvtTbeKg//3aJGAz3WIOojs+m+MZiuOEdVk67QlGnLF26lO9///scfPDBHDhwIBUeSdLV1cWtt97KZZddxqGHHspxxx3H66+/js/n46abbuKss85i1apVTJs2Le/+v/3tb/PQQw+xcuVKjjzySDZs2EBHRwfHH388K1asqLpBUUW1ERtiGB9+LG/7gfU7OP2cPNsN7bF+jiToAKFeCO6G5pnlsVWhqDAul4tf/OIXGcu2bNmS8f6UU07hueeey/ns6aefzuuvv56z/PLLL+fyyy8HoLu7mz/+MXeY8Ve/+tX4jc5Ceej1THSIQdObejsUCuffbnC39TO4B4w47HweXvlt7nabHiiDkQqFolQoQa9nYsMMmV4ujF4PgJsEMl/YJGgLOhIZ3A0/OgXW3pK73Wt/LpupCkUlmT9/PuvX5y1XVVMoQa9jzNgwIbwsOOIU3jBn4SaBsfZnsOP5zA1Tgg79938t774SUmN/z85ymqtQKCaIEvQ6RiZixKSLZdObaG9uxE0C193XwI9PydzQETtv2/DzvPvaKruJD6rZvQpFNaMEvY4xjTgJdHxuDXQ3HhLOlenXwd0Micb0+0Bnzr62y2k0Gv1E4sXXs1AoFJODEvQ6RtqC7nXrSM2DV8TTK8MH0q8PbGWzmJt629e0NGdfW+U0GkSU7fv6ymmyQqGYAErQ6xhpJGwPXQfdTROh9Lrh3vTrA2+zMZGu1/KHna05+9oi5gDQs09NMFLUF6WueFhJlKDXM7aH7nfroHuYJ9Ji/MC6DdaL0H7EcA9vG+nJEFtkWtxfN+fwzfhFdLQ2A9AzqKo7KOqLkQQ9kUjk2bq6UYJez5hx4nYM3R/ZQ6NIV1vcsOlt+4U10eFRc2Vq3Rsi3ZT29NhX+Y5xIdNarRj7gUFHSQCFog5wlrA96qijOPHEEzn33HNZvnw5W7ZsYcWKFaltv/GNb3D99dcDsHnzZk4//XSOPPJITjzxxLwTiyYbNVO0njESJKQLn1vHP7Q9Y9VZw3fA6wfD9meJeDt5NZIW8YGGBRDN3FVjwA9APFZExUaFoljuvRb2vFLafU5fCWeM3DXIWcL24Ycf5qyzzmL9+vUsWLAgZ6aokzVr1vDDH/6QJUuW8Mwzz/DRj36UBx98sLS2F4kS9HrGtLNcXDpCZmanLAq/Ar++DKYdwi7/ElzBdKU53dcEUXjNnMNB3Y28sXeI5bM7YCOYiXj2URSKuuLoo49mwYIFo24zNDTEk08+ycUXX5xaFo1GR/nE5KAEvY4RZiIVchFmOh4YlH6ahF0GoH8bb7lOZEl3E9iJLw0+N6dFfsm2gTi3nruCFr+b+cEnADCKqamuUBTLKJ70ZJEslwtWfRfTNFPvIxErbGmaJq2trVVVOhdUDL2uETKBgY7XpSNIT/kfIH3CEguyJQjvXNqVWjSz1U9bWwdRPMxu87N8ZjPobgBkovJeiEJRSkYrYdvd3c2+ffvo6+sjGo1y1113AdDc3MyCBQv43e9+B1i10l966aVJs3kklIdexyQ9dLcrs3B/UPrBsWhYelk+s5knzvgL37jzaT64rIvjF3Xy19f2Mqc9YG2kWYJuqJCLos5wlrD1+/10d3en1rndbr74xS9y9NFHM2vWLJYtW5Za98tf/pKrr76a//qv/yIej3PppZdy2GGHVeJXSKEEvY7RpJWH7tYzH8R+abyL/9J+knofkj5mtPhYtfIY/mvOwRwy02qN9b5j5qU/lPLQlaAr6o/RSthec801XHPNNTnLFyxYwH333VdOs4pGhVzqFSnRpJEW9JX/kFq1vf0dXBP7WOp9CC/TW/wIIVJinoPusXabqIEY+t5XQYWGFFOQMQVdCHGLEGKfEGLE2pJCiJOFEC8KIV4VQjxSWhMVo5KIQiRPl3TD8qTj0oVH1+D8G1KrmhobiQt36n0YL12N3pxdZKBZD3Of3PdvsPvlidtdLoJ7re5Kd3+m0pYoFJNOIR76rcDpI60UQrQCPwDOlVIeAlw80raKMvCTM+Erc+Cl30DvpvRy0xJ0y0MXoKeja7rbw/c+cGzqfVzz43GNcSro6RsAd32qJKaXhciA9XPrE5W1Q1EUeev0T3HG8zcppEn0o8Ao3YV5L3C7lHKbvf2+oq1QjJ+da62fd6yB7x2ZXm6nKRpCQ9cyB0V1jx/dne41aroDYx9Hcwj6zrWw/61xm1xWVFplzeHz+ejr61Oi7kBKSV9fHz6fb+yNHZRiUPQgwC2EeBhoAr4tpfxZvg2FEGuANQBz587Nt4lioiRi4PKAYQm6FG6EyBR0l8cHzswXt3/s/To9dEh7wtWClPD0DdAyq9KWKIpk9uzZ7Nixg54eVW/fic/nY/bs2UV9phSC7gKOBE4F/MBTQoinpZRvZG8opbwJuAlg9erV6nZcBiJ92/F1L0qFXKSW+xXruht0xwJ3Q842uR/KEvRYKP92lWJwJ/zlC6m3CVOqFK4awe12jzkzU1EYpchy2QH8RUo5LKXsBR4FKpuMOYW585GnrBfGyIIe8LosLz6JpwBB1zIFfV9PlZXRzcpq2b6/ym44CsUkUApB/yNwghDCJYQIAMcAr5Vgv4pxEB2065ynPHR3zjYNXhe40rE5zVu8h/7zR0pcQGmixDLL+rqFgfzeUfDGXypkkEIx+Yz5VCqEuA04GegUQuwArgPcAFLKH0opXxNC3Ae8DJjAj6WUtd8+u0YxwnZ5WzuGTh4PvdGrZ2a9eBtztskhS9D9RpWV0Y2HM97OFr3Q2wt3fRo+/WqFjFIoJpcxBV1KeVkB23wd+HpJLFJMCCMZ205YRYQMPTe/vNHrBkeWi+4rPuTSRJU1uoiPEGIRau6cYuqgzvY6Q8SzBF3LFfQTl3SCvy31vjFQQNqiPVPUaJxJXHgwI9XtoacY2Ab92ybXFoWiQihBr2Xy5O2KhC1stsAZem4e65z2QEYIJeAtIB9E0+AffoZ+5d9I6D5cxggCWinsG9kdxvH8OnFy5rq3H5t8exSKCqAEvZYJ7s5ZpCdsDz0l6GPnmGcX7xqR5edByyziegCvjJAwzLE/M1nYv+/X45fgburMWLUrrE5zxdRAnem1TM/G1MvN5gwOyEZ0I2LNuLM9dZnHQ89mzGn/WRi6Hz8RhmPG2BtPFraHfvzyOZx55OKMVQ+//HYlLFIoJh0196KW2b859TKInwBRfDJCNGHiu+dzAEiPw0P/l03gaEUX1QJ4zZBVvKsITHeAAFFCsQQt/ty0yIpgC7ruacATaAasm9wibTfxSP7mBQpFvaE89FrGUWUxLH0YrgB+EWMoEoNhq6SOz+/IYGnsgqbpqbc7PryOU/VbOevQGUUdVroCBESU4Wg1eejWE4nH60+lYW6T0wCQ1TaAq1CUCeWh1zLRtOcZwovb10AgFmGwv49kFNkfGDklcdHsGTzwHxcUf1xPAD+9hGKJsbedLOIhwtKD3+dOZfjskp0k0CFeZSmWCkWZUB56LeMQ9IUzu3C1z2We2MuBvnSRI3+gqfTH9TQQoLo8dDMWIoSXgNuVKgOwU3YS0/xoStAVUwTlodcwZjSYuiMvmDmN/fo0Fm3/K9x5cmqbBn8BlRSLRPM0EBCRqvLQE9EQYbwEPDocdQV79u5GeP4B85WH8cXChGMGfo8+5n4UilpGeeg1TCI0yBazmzdnnAOnXo9/2vycbfYMRkp+XM1re+hVlOViRIaJSA8Brw6eBqZf+D989uwjMNxNtIhhIs/9DK5vyXiqUSjqDSXoNYwRGaSXFl5e/RVo7MLnzZwV+rX4JbznyOLqKReCy9eInyihaPV46GZsmDAeGrMmSSUCXXSKAfzPfd9asF+lMCrqFyXoNYyMBBmS/lTqoMgqoPXW9L/n0NmtJT+uy9eIVyQYDpfe+x8XAztw975GGC/Nvqw0ysZpdNFPTLfLGwyphlqK+kUJeg2jhXs5QCMtAVvEsgpoufzNZTmu22+lBa7cdAN898gxti4ziRj83yF4hncRkR4afZkeutbUzTTRT3PfSwD07Nycby8KRV2gBL1WMQ08ob3slh1przSrVK7HX4YMF0C366cfvf0W6NuUaqZREX73j6mXbSJIU5age1um4xNp+37x12cmzTRFjZGI1XzdHyXotcrQPjSZYJfsSM/WzK5Z7i+giuJ4yG5ZF9xTnuMUwsZ7Ui9XaltyYujexpaM9+1iEIUiL/f/O/z0bNhbu/XzlaDXKoM7AdhLB52Ndjs5LTMtr60ht3RuSfBk3ij27nyrPMcpknuNo2jKiqFnN+9oF0GiierJzlFUETvXWj9HKsVcAyhBr1UGdgAQb5yJK1mLJSuG3t0ydmGuceHOFPTv/PHx8hynCDaY8/h4/OM5Hnp2v9Q2guwfjk2iZYqawW4OE5W1Oz1HCXqtYnvorrY56WVZIZeOBg9lIUskh4cqVCvF0Rh6GC/3ffpUdE1kbuO4+fzNOIIOEaRvSAm6Ig/2jOL/vqvK+uUWwZiCLoS4RQixTwiRt0+oEOJkIcSAEOJF+98XS2+mIoeBnYTx0t4xLb0sa1D0HYs6ynPsLA+9Qa9QPnpkIPVSQ7KoK0/dGk865KI1ddMmgvQORXO3U0x5pO2hv7Ktt8KWjJ9Cni1uBb4H/GyUbR6TUp5dEosUBWH0vcV2s5PZ7Q4RyxL01kCZPHRXZijHr1eo0UW4P/VSQyKEyN3GEe93N3bSPjxIX1AJuiIPMctD94jqmTBXLGN66FLKR4H9k2CLogiMva/xhpzNnHZHrRaHoG889PPlO7gr80bRoFdokNHhoXe1jJDR4wgPBdqm4REGg4PqdFbkkmzf6KaOBb1AjhNCvCSEuFcIcchIGwkh1ggh1goh1vb09Iy0mWIsjDjuwa1slrPobnZ4y44Y+sARV5Xv+Hpm9oxLVirk0g/AG/oSZn9ohAdIR4ploLUbgHC/mi2qGJmpLujrgHlSysOA7wJ3jrShlPImKeVqKeXqrq6uEhx6ihIPIZAMygBtzrCKw0MvaychV6agC3OSQxjrfgbP/RhCfQDc1PkFaF+Qf1uvNblq69wLWLJgHgCxQeVMKEamlgV9wvk5UspBx+t7hBA/EEJ0Silrd2Sh2rGzO6K4aQ04hNsh6BnLS01WNo1mTvJM0T993Pr57v+yfjaO4hx4AvCpDcxr7IZ9GwAQ/VvKa5+ipvHUsKBP2EMXQkwX9miUEOJoe599E92vYhTsjjxR3LT6HR66Q2jL6qFnhVw0M241pp5k5NA+YrhoaGobfcOWWaC7YNpyopqf7v4XR7f3/1bA498qqa2K2sFF7U48KyRt8TbgKWCpEGKHEOIKIcRVQohkkPYiYL0Q4iXgO8ClshJX91Rgz3owEhC3BD2h+fC5HV+hw0P3ucvYzCHLQ/eQIJqY/EyX/ft20CNbmNvZOPbGALqLYMM8Oo199IdGeKqIh2FgO/ztutIZqqgp3DWc5TJmyEVKedkY67+HldaoKCc7n4cfnUJ44en4bY/U5fFlpuppkzTDzXHMoPTjIU4kbpT3JpKH6Na19MgW5ncUUbPG5cNHjGAkQVu+iVeDu1Iv5eBuRHNxDbQVtc+UDrkoJgm7AJb/rfvgpdsAcHuz2stNlqA7GMKPG2PyPPR4ugb7zPg27jGOYV7HyI2wc3D78Yk4w/na50WD8N1Vqbfim8tg74aJWKuoFcz0+VvLg6JK0GuFaO70epc3yzPVyxg3HwFTc+MRccKT1Y7OTlVMsoUZmbn4YyBsD304X7elA1tzFoX3bizWwvEjJTz4Zdj84OQdU2FhpDO13CQwzdqMGitBrxWiuWVf+6JZMyO1yRd03e3DQ5yeyZpOn9VxSPe34XUVHuoRHr8l6PluQPZFvVe2phZ9//5JLKW67Sl49Gvs/83HKjLIPKVJpJ/83BgMRipY438CKEGvBYxE3l6YIZkVA9Ymv6t9q9/F32tr2dU/SSVHbzwx423cXVwTD80TwCdi+fuh2umgj8nDU4vCB3YXbWJRxCOw+SG471/hmRsBMKND7Jysv6fCwhHK84gEB0YaNK9ylKDXAn/5V3j6+zmLB+NZAp4crJy5KmfbcuEb2IxLmAzvfmPSjukkVqSguzw+vMQZSgp6PAxvPWK9tr20Ad+s1PZdYoBYOccHHvkK/Px86/vdcCcADUR4ecfAqB9TlBiHh+4lVrMllpWg1wL2hZ7Nynl5JtRc/SR8MP/25SQYDE76MQF8Te1Fba97AniJEUqGXB78L/jZubDrBasFGdAfSM867RL95a3OGMkVbr+IsW9guHzHVOTiKMX8Lm0d/SEl6IoyIfX8VROvPePg3IXdh4CvJXd5ubjwxwAkYpMQIjAsr/pbiQuJe1oBuP6iY4rahdsXwEcs7aHbjUKGd7ya8tL6A/OQn3yF/a0r6KKfnjJWZzTzDHYDRMNK0CeVqOWQbJKzmSn6VAxdUT4MLVfQ72n/AL6OeRWwxuZjz8LH10GDVXM9EYuM8YESYFheU1h66bnsPrjoFma2FZGyCOgePx5hEI9bF6xstOrJ33rvoykvTXP7EK1z0Ztn0CUG2DcRQe99E65vgTf/mnd1ZHiQbWbuk1YsogR9Utn+NACDeise4oRjFSoJPUGUoNcAhmZNtX/GXJZaduCYz2ZM8Jl0upZCx6JUGYBEfBKyXOwslIRw0TV3Kax4T9G7EG4rxVHafSPjEaupgSs+lPLQdY9VwdLVMp1OMUEPffdL1s91WdUgb18Dr96BGR2ih9bU4j8Zx1l2KQ998pDSahANhN3tuIRJOFqbNfOVoNcApoSIdDP8nl8h7dTE9x49t8JW2diVF41J8dDjqWO69XGeui47Zz0p6MMHAPARw7Q99KSg+1q6aSfIvsHQ+G22qz0mhh012ONhePk38LvLSYSDDMt0CeSFq04FIBZVgj5pOOLnNHQCEIspQVeUCREd4B7zGHyNLYhPvgIffTp/d55KYMf3zcQkXADJY0xkApVuzaY1E9bNwbCF1kuceDRs794SfT3Qhi4kwYEJNMSwvf6NW7ZzIJk54SgvMBzsZxgf0WYrfLZimfUUZkZV2uKkEU/fsF3NVgguFp0EB6UM1G576ymEMKJEpMfqaN/cCdVUX8T20OWkhFwsQZR5xhQKxn7CMWxvXwtZtdG9Ik48GsKLo6SCvxWAKzdeCdFnILwfWot8MrLbmjURYmd/2KofYzf4BmiO9zLMPPZeeDtz42+n5hIYMeWhTxqx9MB0Q4vtodeooCsPvQbQEhEieAh4qvD+a3vocjI8dFvQRVaDjaKwvXszHgPTxDe0HbA89Fg0giEFHrd9w/BbRdCmx3fA366Hbx0KvZuKO56jT2Vf0kPvTefsNzFMSHoJdM6BJaelGnCvDD4Bv7/Ciu8qyovj5rl8jjVAPSljQmVACXoNoBmWoDd6q1DQk+I6iYKe3dO0KOwCZtKIQ3A3Lrvbkpc40UiIGG58yRunrzX1sVfffBOQbNr4cnHHsx/nvcTps/PZY6/dl7HJAA3p79YO95wfvh3W/543d6nuSmUnZn1HX3Jdg2afW3HloSvKgmmgywRR6abBO/lT+8fEZQ/oTUoM3RJ0bYS8/IKwPXRpxDJi2afqLzBj/U1E8KRrzPvTjTP291kNuG6856n8+40OwT2fhYij5k7/dvjrF61dEaVvyA4Z7VzHRnN2arOgaMTrso+ZHLS1+d+7Xyr+d1QUhx1y6dG6Uk+cCTUoqigL9qBaBA8NVRxy0cwiZtb1jLOCYcpDn0DIxY6hm0Y81ZPUSVD603XdHWMVM4Q1MNrFQP7CWWtvhmdvgqccrQFuX5N66RNxtu7aBcG9eKN9PGIellpneFvTg9zuTEGfUVyavWI82E9Rcc2fFnQVclGUBdvzNXUvmlYlmS1ObHF1yXhhJUdf+T18/2jYeN/Y22Zj56FrEwm5pDz0/II+RAC/xxZ0h4c+PSnooj8dC3cirYkovQf604uySv0uefNmOLAFgI2+w1PLI67m9Eb2RKckbZ7anOBSU9gx9JgeSAm6EVchF0U5sPOlcftG365SaC4kAo+IEzMKEJ+d6wAI7hxH44hkZspE/hbJlEcjbmWtZBHET7MvnRZpHvY+ABqFdYF3iYH8lSWFdRP447ptaXOzbnBNsX0Ee631rvZ0yGXLsOOJI+vp47I3PzXWb6SYKPY1Zuje1PlhxOu0losQ4hYhxD4hxPoxtjtKCJEQQlxUOvMUqSpwWbHVqkEIDM1j9RWNFyDodtjkWw9tKf5YyYk/7omnLcqE5aHHZOa4xJD0ZzTY1k7994z1nSMJOpZ46xjE7RtbwkjXXO9pX80s0UvvTqsMcmPXXIwVFwOwctlBI5rbFds54jpFiZDW9yQ0PXVDNes45HIrcPpoGwghdOCrwP0lsEnhxBZ0Ua0eOmBqHitLxBi7a5G0BT1ijmOAN1lrxTVxD12acYzh/QyQ2WB6iExBJ+tYXfSzeyD3cVzag6Ee4qnqjIajrZmvcy6zRC+h3q1EpZvuadPRL7yR4Af/xmcuzbq8PvYsnPb/Um8feiW3k5KihNhjIpqmT+5EuTIwpqBLKR8Fxpoq93HgD8C+MbZTFIsdy5tQmKHMmJrbyuMuoG54crBJMo7xgGHr9Ir7O4r/bBI7bfG6gS8SjwwxLH28vPAjqdVBOYagi4G85XSNsCXoi7TdDO18HUwDz5CVRbPz/N/jauqilSF8e1/kVTmPxd1NoOk0LTwqt4xB11KYc2zq7e8eHCGzRlEa7PEPXdPS8xQSdRpyGQshxCzgAuCGArZdI4RYK4RY29Oj8mvHJBGDH58CWHW8qxVT9+IRiYIaRUcj1g3KR/EXTLx/J2Hpoam1s+jPpnCUDTDCA4Tx8Pahn8Y83opVDxGgeRRBbxYh+gdya7/HQ/0AHKO9zpLfnszAtvW4jRCfjH2UxqUn4/E30ygizA2/xnPmUlbNbcvZRwaOG/jc3kdJFDI+oRgftqBrmpYqNienqqAD3wI+L6Uc84yTUt4kpVwtpVzd1ZWnOYMiE8egnbSnoVcjUvfgKdBDj8WSgl78I22kbzt7ZBszWydwc3P0XQ1uf5UIXhq9LrRkaV5XM7ozm0jLvUTig3tzlhnhzEYVf1n7OgD9ehvNPhe63yrS5RYG+xsW0xoYYxzAMWZyEFsJxyepCfcURtM1sB0nnwzV5E20FIK+Gvi1EGILcBHwAyHE+SXYr8JMX8TLVhxVQUNGR+pePCQKEnTNnszTLoJFt/kyB3axl3ZmtJQgywWYbuwmLD00eF1gpxh++F0jt++TZ/6v9XMoS9ATMWSWoLsNK7fZ7w9YOeaedKxedi5jbNIZMh4SStDLSSrkoqdSVVsYrsm/+YRnqkgpU/26hBC3AndJKe+c6H4VpCfSAKsWdVfQkNFJeuhjhlykpGHQqoXyEde93L1xF2etml/wcVzDe9gj53JY8wQEXcs85UN4mebRIdwPQEt7nifHd10PM1chfFa+uDbsGCrasRZ+fCrZnU3N4B4AAn77acKb3qJ5zvKx7XTko3uIF5ZBpBgf9qCormngbcYUOq1iiHDcoMmRwloLFJK2eBvwFLBUCLFDCHGFEOIqIcRV5TdvimPnXX8hcSXTJyJi5cblLSzkEtqf8lwBdvfmTuwZESnxhfeyR3bQ7JuAH5JVejeC1yp6ttTONJlxWO5nTvgULDwJGq2bqifSm54tuvXJ/IcZsgS9ocGe6unw0BfNKuDm7G+D6/rpb12BmwSRGvQWa4ZkDF3XQQhi7hbaGCIcq72/+ZhXhpTyskJ3JqW8fELWKDIx7Yk0/jZc423oMBnoHjxiiCHDgK1Pwawj8xfQimX2z4xFi2gcEepDl3H2yLaJeU1a5metkIsOR3wAVlyUiqHmpcHy3tvMfoaiCcsOx2zTu42jOf2IRegv30ZjzPLiPV77Ruy1BD0kvSyb0UxBCIF0efAQJaI89PLhzHIBEt5WWsJD6b6zNUQVq4QiGXLxeCdQu2QSEC4vXuJ4el6Fn5xO/91fzL+hPcX6Lc1q5hCPFNHEwRbOQb0Vj2sCp62eG3IJuF1WO7+xMol0N1F3K12in1670JbZn54Zul12ox99JQCBhBVTd3nswU17Jml84Sks6Cy8QIvQvbhFgvbnvg4v/7bgzymKwc5D163vyPS10cYQwYgSdEUpsbvc6xOpXTIJCLcPD1Y5WoCX1z6ef0Nb0N/2WDMjY8UIeqJE+fjZHjredO2WQswIdNElBthjTy5KOJ4y5rb7U8W1/IaVl55qljH3WHjXl2i55MaizBUua3xi1kvfhduvLOqzigJJeuj2U7DwNtMowkrQFSXG9tAnVIxqEvD5/HhFgr7+fgDiuPJXJLRDLlGPlUlgxIoRdOtv4fZMUNBzYuieojx+rambLtHP9v2WkCeiYV4wF/PckV/jzDVfTgl6g2n9rl6fba+mwwmfBF9LUeYKlxcP6Vjug6/npkwqJkhWyEXzNdJImGAkXkmrxoUS9GomKeju6g656KFeFordzN/xZwAS6OwLOvLM97wCD345JehxW9DjxQi6XWnRNWFB98DSM1Nvg7K4nHZv5wLmib1s3W89bRjxCDFcBJecb2Wm2B2HGkxr8lHKQx8nwuVhuZae+r9hZ3/mBntfhT9cmW6grSieZJaLHXLR/c00iIjy0BXj5LW74JcX5y43ayPkwpbHADgsZE1RNxGZ0+NvPQse/RoMW00iDJ8l6GZRHroV4piwoAsBl92GqVv7GaQ4QdemLaNTDDLYZ2WxyHiUqHSnB2ptD70ZS/B9vonN8NWyqi/GQ4OZG9z1aXjltwxuUuUBxo0zDx1w+ZtoIKIGRRXj5DfvgzfvT4UVUtgeumsi1QUrQCPhzBRGe4KUHLbKPchAOwDXHfhC4TtNdisqVU0bu6FEsR46HYsA8A/tAKxiYzFcBJJxeFvQm4R1s/J7J5bHnP10FotkNo9OziD+39sfm9BxpjTJ4lx2DN3la6aBCIPh2pv+rwS9Gki2VAsfyFz+6/cC4KrykAvnfjfjbZMIZ04ysifzGINWKp/WkK7F8uquzBmWI1KqkEsSOzQSpMiQiD1ByBfdB/deiyvaTww3/mSXo6wYvVufWFMSrzfz901EMwU97rP+lh77BqMYD5agu+yQC95GNCExIkOjfKY6UYJeDdgi8dKzD+YdTJQT6aE5GRzxAUzSmSIujAwPXdqCHuzZSlzq6A3paokvbesv7BiJ5NNKaW5uwk4nXDJnVnEftD3w0wd+B8/cQEN4l+2h55/SMa1pYjcgLZ6Zq59Ieuh7XmHrjz/Aures0E+7CBZUekGRBzvkIuzU0tQksKgSdMV4sE+gwx67io177Up+8XR8uTdc5ReqECQcxaSyBd0Ultjt2rKRAzThb0jPmty2t8Cqm0kPfYKDjEmE7aFfe06emaGj4bZyyA3HfTcm3SOmPr5j0QRK/QLYdWOGWqxUz2RmkFz3M+bt+BPHDv0NgAARBmswK6MqsAU9NXkvWaYhrgRdMQ4MVzqOu3sgYnnp9gAiwN6hKhd0wNDTQusmkdGOTtqDTbNFD32yKV3fBBg6UKCgJ7sVlSrkcorVicjXOa+4z9keustMD/pGnSEXB48vvz7d/Hm82ILeePT7ATDtvPcYmU8qjSLCQFgJ+riQElOKdM9eeyDaiNVeX9EqbCM/9Yi7m1IBC8+b9/Llnz1D87J3co29bOH01gpZVjimK0CyIq47q/Ji0kNvESFeMRfQ4PDQm2J7+c19D/NUr5dvXHbMiCUOzEQUDXB7SuOhs/w8uL7A+L0T27NvN/eT7NERw52Zy770TAgf4IR/KEE/0GTFzXZrMFbGLEGPDvVlSLo1iKcEfTxI08REkKqabNfAlzXYhk4JehUQ93fhAyLSzaEv/SfHu3v46usG2ONrnzz9kIraVwiGI+TiFgbRRHoyjCnS3ut+mpne0gZnfwvu+iSX9XyHubs2YyTeyfYDv0xPi3/iO/DqHbDmIQASsQgeqqAMgu2hd4v0AHYs+zK69FelO96lv4TX/gxt8wFojuyk57GfEOnbh7MiTANh5aGPEykNJAIt+TSVakOnPHTFODDtSSE+EccXt0IQy7R0jRBvtWe5YHvoNtkeuiHSp1lQBmj2uWHawQBMT2wH4GBtOzsPhNOC/tf/sH5+90hY8wjxaFLQK9ws252b5hgjKzVxomEWJ51L4MRPw/63APi8vBkegCHpw9nFr1FEeGuo9tLsqgEpJSYi3dgk2aWqBvuKqhh6NZCn3dViYXV7f2PaGSmvsJqRjgkwbjLb0ZkOQR/GZ03Csb32ZFKPjyg7+/NUX+zbRGLnCxjxCHGp4/dWOONH0zD0Ctxgs24kjSLCi+YiogvfjTHjCBoI05On16libKQ0kYj0fTg5ka8GPXQl6FWAzDNte5HYRUzqbDvpW6X1+MqFI6zixsgYFDUcD4IhvPjcmlXbBPDavUXbRZDdA/kvoBsf2Uz/rjeJ4GFuexX0VnVl3mDfs3AS6mZ7cis0rpXLcL//t+jdy2kSEXqCStDHgzQtQddFpocujNp74lGCXg3kOXF8Is4BmjhoeoG1syuM0Jx56FkhF0eKX0h6rcyPrM5B3aKf7l0P5N332jd3MGvXX3lAO56j5o/RXHkysBtdrDMXs23lNcw+//ryH9PbRLg7sz3e254lVmZG0wy6xAF6BoL85qlNbOsros68AimTg6LJGLr1BBYODdMfGkPUwweIvflQ1TTDUIJeBUgzwXYzt/XZTqYxq636wy1AyuMG8AiDqKPDjjDTF0VE2L+Pljt8c9lb1+atcHeM9jo6Bm+0nTjxNMASoB1yPmBVlTxw9GegfeGkHFefsTLjfdBjdz5qm48Lk3Nev5YL7juaq3/6xKTYUy9YWS5aTtriZ8XP+fv/HKMG/d2fwfPL8znvv28rs5WFUUgLuluEEPuEEOtHWH+eEOJlIcSLQoi1QogTSm9mfSOMGJvkTCJa5mP1Uw3vyuxAX81omXnYibhDmB0hpYiWLCebfzz+p09uAcB01C2/ymVVcZRdB5fA0Ikj7NIFGibtDZMX0/c0T8t4r/vtpzf7hnKavg6PMDg09tKk2TQe5Kt3snP3rkqbkcKKoeNIW7QEvUsM8EH9L+wLjhxLN0L9AKyIvZK/ZPQkU4iHfitw+ijrHwAOk1IeDnwY+PHEzaowpoHc+cLkfUFmnDguEJlfh6ujyEkvFURkCXTUMSlDc4SUwjLpoeefWdngtfaTrIbopKNrxkTNLA12PFufZEFPZV/YNDS1Wi/mHJOxvN1TxemLw72I3/0jPTeczR+er476M1JKK4ae5aEDTKOfTftGnjEaCswEYL62hwOhyv/dxxR0KeWjwP5R1g/JtPI1kKx0U8s8dzPiRydz05euQG56sOyHE0aCODoamTNC2zunl/3YpUJkCfTQUDqOK8z0id5v2J634wawX6YnGnld1n5i/twQ1MxpnTnLKoIt6BoyXWVxMpCZ58fcGXbIRXcRnX5karnz71112K0ED9c288irW8fYeHKQpoGJlg7nObKYFmu72HFg5DLP8bBVW2cG+9k5ynaTRUli6EKIC4QQrwN3Y3nptY190v0Tf0D84oKyH07YHrqWdS88bfXysh+7VCQF3ZDWRTEwnBZ0t5E+0c89eqn1wiHo+2R6oDMUtcQo7s4dDO5uqZLxBLvhs4Y5uTH9xsyQy4feuSL12u24kqVRxXW8Q2nfsHvg5Qoa4kBKJDiyXNKC3i32Zwwyb+0b5tfPWnNEdm5/m543nwVgutjPzv46EXQp5R1SymXA+cB/jrSdEGKNHWdf29NTYA2PCiCzUsSefXvEB5SJse0ZCO5FmHESuBBZHnprR3d5jlsObEEPYYUFgklBlxKvkX5kvfj0U+3tnYLemnodidihmjypnKvmtuYsqwh2MbUG9yTnFBzxQXjPzam3zm5IGc5ANXcvCqevJSOrFHClsLJctHQM3XGTnin288fnt2CaEqTk5e9cwpN33sjnfv8SPT+6kKWaFTaaUU+CnsQOzywUQuR9NpZS3iSlXC2lXN3VlftIXS3EQpk1Pn79TJkeDW95N8YNx6OZcWJSR9iP1L/Sz4O5x4G3NlIWIR1Dt+ZzQjBkn9zRILqjJ2aqp6ZD0HtJ99mMRu3PmXHWmgdlHqMKMlyA1ESvRZ2T/MSgabDyohFWpgW9ukMuaUGX0WDl7Aj3Q781S9nKQyed5QLW9H97MtdHQzewYfcgbHuKc8RjfMfzPT79ynkcrr2V2nyxtosf3vUECaOyhfQmLOhCiMXCvtKEEKsAL9A30f1Wkngo80SLZgl8SbCLLumhHjDjGCLtocuVl8CH76uNCUU2QrcEOlnX5LTEw9agcqQfgJ96L4N/Xpv+gJY+9SLSQ+S0rwAQtasJCjPOHtlG7xk/sjZadnaZf4MiSN6MZIUuXk9j7rJVH0y9lGYVh1wcTVxErIIe+s3vhm/ZIStpItHSeegA/9EDax4BYLX2Bi9s74f+dDmO6SKrGQ3wqPdTFZ+tW0ja4m3AU8BSIcQOIcQVQoirhBBX2Zu8B1gvhHgR+D5wiayG/J0JEA9n9m2MDZY+PCQdnoovESTqak556O89sTrS84pBs0MuA/YA58XaIximtDwhYJdngVWXJPWBtIcew4Vu16uJ2yEXYVjjCubB51pVES/95ST8FgWSfMqYvnL07crFNS/Cx9dlLjvyQ/B5+0mymgXdUedfS4Qq59H2bgTgm/euT9VyyckQ7joI8/D30SqG2N0fJjKSDvitlop+EeNvG/ZWNH1xzOJcUsrLxlj/VeCrJbOoCjCzBJ1g6XNmxTcWp17rQpJo6Aa77rlIFtivIZLOzX3GUSzXtvK4uYLzDYnL9tBjrqzwkUPQ47hSreXm7P0b8G6EmSAh9cyytNVCy2z48F9g+qGVOX5jl/XPiRCpwTxRzYOiRtqDbRRh9g/HmNZcohr348D9xNcZWKyjO9MWHWjtC+kSA/Ts7yf88s1kW/rWe+5l4ZzZ8C3r5r7zrv/hz4HrOPewmZNgfS5VeLVUnuzYXnN4F5t7yty9pNGRoujN80hd5SQH5WK4GPLNSHctsv+WRvZNyiHopnAhbDH6cPBG+oaiaNLy0N0j1EevOHOPBU8V1JVxYv9NNRmvikku+ZCJGGHpIYKXANER6/dMFh933Ul/KIaJyD9G02Rdl+09z9AWejtn9cJ5C6B1LuFFZwBwrfvXVt769mdJ3HzGpDfJqNKrpbKIcGZWy8HaVp7Y1DvC1uMgz8W2eNFiOOZq642rch7LeEleCwl0pNDRhWkV6Eo+YmeXnXUU80oIV0bubyRhIswEMVzV6aFXK7ag6xgkzOoUdCMRJYaLmB6ggUjFBD3YujT1eigcyyzO5cR2tOaEXsu/I3vWsMeXPr97BkLIP34c1/Yn+c5v7ymd0QWgrpY8eMN7MzIsLtEfZsf+Eg7ghHMHVN510slw+v/Af/TV1GBokmT8P4GO1Fy4SRA3TLCbHGvZ3qxjUDSBK+N3jiVMdDNOAh1XrZQ+qAaEwBAuq9pllTaMNmIRYrhIuJtoF4PsOFCZQmJxYTlNUekiYSSQMk8MHVIe+uJYpqB/O3EB/Ps+0K2Jcnoo7fDt2buLhN2D7NFXtzGZKEHPxkgQiPXxHCvgEy/Du79MkwhzoK+EA6PBPenXB58DZ3zdmn0oBOi12XNEc3joaDo6piUqtoeeI+gOolIHR8aDb8PvcJkRTM1dPamKNYIUrpwm3dWEEY8Sw81g2yGs1jezbmuZ5niMgbBrnXtFgkC83xoUzafofmvS20JzC6YUbO1+F2A1anFOQEr2fgUY6NlNzBb0dpE1HldmlKBnM7QXDZMhbxe0zUvdoaP9u5D3fI6XX35h4sewPfQ4brjgJjhmzcT3WWGEHUM30JCaGxdGhoeue3PreSfpNwMZgj7jwU+gYWYU6FIUhmk/HcUqnA89EkY8Sky6GOhaTRf72bvtjYrYIRyDs15jKLN8rhN7kuE00c9+mmjEOp+vPjOzfg4rL069bI7tZsDefYcYzLm57tkfZN9geUJNStCzsbMyTJ+VipSMkc0Lv4549kaaf38Jj7/ZSzRhjD/lyj7G/879XvUNrI2T5KWQ4aE7Yugu78i/55BoyLggkkS1+vjbTCZSuNAxicaLOzdlcA9/fvgJnt+aGw4sJWYiShwXw92rAZg3/HJFniY0I0JIWh62xwzbxbnybOiYNd4nm2mM7AagY1lWUdkTPwOfexupezjbvZaZQausQQdBDjhqqr91+5eY/p3ZbPj1v5f2F7JRgp6N7SmKZKZJg5Ue1pSw5krN1/ZyIBRj9b/fzlW/WJd3F2Nie+jCXwXNGkqFPdBrSA1SMXSJGRsmJL343COHkoZEA3gbOTDzpMxdemovfbPSWE9HCcLx4houJH5wPOc8fCZf+EMZSu/2bAQ7ldK0Qy5G51JMNOaJPeypwMCobkTpx7rGvTJit6DL46HrbgxhPSnul83Ez7kBjrkqtwa+EBBoRxz091wkHkot7hAD9DomGy18+ZsAzJ63qMS/kYUS9GxiVnqiSM7G87UC0GKkY30NG+/gFd9H2PH6c+M7RlLQA+3jNrP6kPb/AjR3KoaeiIYI48HvGflUGxb231rPDLHIGszHrzi6NSharKC7w9agntFT4hBIcA98/2ju/cYHOe/7T9A/OEQMFwGfl4S/g24O8NyWyY2jy72v0pA4QNRutuIngomWP8sFMOyWg30007DoWDjjqyMnLpz2/zLedohB+pLNux3ZbfPeMVIJh4mhBD0b20PXfLbI2AMfXjM9w+2gN28CYInYYc2GLBIzdICE1PAEaqdWy5jYJ+vV71yM0F24hBVDN6PDhPHid49cZnbYbuyhZQ0Ia746+vtMFpoblzAIxcY3uWix2DWuc3pE7OvpjPDd3LjvfcT7dxLHhd/twt0ynXNdT/Pga7tLd7wCMO6/HoAFWIW1AkTtBhf5RdoTtwY2Z89dOPYgvcNz73VNp4NBBsJ2bR27iuuXzctxN5WnlpUS9GzsE9CVJeg+Mz1oF0tY3k+nGMjbMm0sEqEBhvDT6KujQT87bfGg7mbQ7EwLw8SMhohID75RBP3TZ1vx1OwmGYOySsrl1hK62/LQx9njcqboZTAct+qW7Hpx4vYk0uGG6eIAB2k7iUo3AY+O2PMKASJMH5zcDktRnzUutqfzOAAaRNRuQTf65w6/9PqijhOYeTAdYpDBpEbY2W1DnvLV9VeCnoWMWiEXl89+3LcnvASkozSmXZ70OvfPib5+f9HHiEeHCeGl0VubKYqjIxC6G91OnTPjYSKMLuinrLLqvouskMv24UlsHlEnCN26mRYVcnGU250leukPx62p7DedNMqHCiSRW1I2hgu/R4eTPg+ANslVFxPRYQalnzeOSA9MypGyXJxk1aMfC0/rDNpFkMGw/bRkp0pqnpEzviaKEvQsEhHr5HIHkoJuiUyzSE+AmCfSOadDd/5L0ccwoiHC0kujr54E3X5MFwKhuThcewsjFkEmYtYFPIqgJ2upa65MQb/mzFX5tlaMgrAHRUPFeOiRdK70NNGf0enenGj4xfbQ3/KkZ2bGk4K+/DzrGPFJHhQd2sebcjaBZAs/ChT0QudEzDkWAN3fQhPhtIduC7rLW74nTyXoWcTDlqB7/XbIRQgSmodG0p6GLtIneYziwyZmbJgInlT/zLrAUUrWO2DViV784leQiVj6As7m0l/BFX9Lvc2Ooa9YVjsdm6oF4fLgIVFcyCWRFtRGwumYL3Dz47n1S4rCTlu9s/tjRHxW3DiGm0aPK1XiQsZzvfhyooX3s182E2hMD7qbI6UtApz9LThtxL49uVx+F/zbHoSvhUYRJhiy/r67+/oB8ChBLy83P/42v33OKnafCA8xLL00+NLNf03NS6PIf9JFxy5YmYOMhgjhpamuBD3toesRK2shvGsD0ogRky58+br7LDsL5hyVeqtlhVxqddZsJdECrTSL4eJCLg5BbxKhDEF/bvOefJ8oet8ubwCvtDz/GbPnW7My7UYhIk9YppyI+DBD+GjKEvQRBzxXfwiOv6bwA+hu63ezs7Sidn+FvX/8IgBenxL0shEaDhK/79/5nz88DkA8EiSEj2ZHOMTUMz10J1HG0fU9XschFwSa7a3HpAsMy0MfLYaexDn1+qUz/1gOI+seraGTNoYIRYvIcjHSIZYmwry2Ox3TfnbjNtbvnECDF1vQ3d4AImrt56hz7VYKtocuEpNckTARJiy9NDc2YtopiSMW55oIdpZWT88+GNrH4dpmAHSPEvSysf++r3CV6y4u0h8FwIgMMSx9tPjT3qLUPDSK/CddVI4jUyVhDRQ2eOpI0B0eejL8EsUNdqOKUWPoNppMe5WLDzthlC0VIyEC7XRoQ2wvpgO9HecOigaWadt5el26vMUq7U1+/tT4WzCaMcsOt6MaoZh2iPXC9tBdZoxoYnxZOePBlQhbT8g+N8ZCq8dtQTH0YrE99D09PfS+8UxqcWw8TmCBTHlBb978ZwAifqtmixkdsjx0h6DrHm/ez4I1Yl9s3q6WCBPGS1M9eejJGLrQ0oLu8NDzxtCz0GT6Ub+uxhcmk0AHjYR4c3cRU/htD71JWqm5n4t8J7XqFs83aGDkiog/f3or537vcUxTsn7nQE45jFjE2qfXF4Cj18Dso9KhNNtD94lYOhOk3EiJywxjugJomsDVMR/ALs5V4mPZPYFPEc/znT/8NbU4Oo5xt0KZ2leNlDSHLO+jxWcLTmyYYbzMcgi6Z2DLiLuI4iYSN4oSID0RISzrbFA0hQDb045IF8KME0fH5xpb0EUyfe68H5TTwPrGLicxfKCI2LcjVxzAIzLFNRocoUWwabLjz/+DYa7k0v/rI977NmedeS4fOTE9ucZ41JrqPr29FY78eubnhcDQvPiIMxCO09U0suNUMhIRNCRmMn5ve9EasvQeul0H6nPu32YsjsnyXfeF9BS9RQixTwixfoT17xNCvCyEeEUI8aQQ4rDSm1kmYukuRMLuTL+3t5dQVsjFydajvwiBDui2GszGcRHJNwAVC7H9+fvyztjTjTAxzVe93XjGw+oPWz9nr041wD7LfJjm8A7isjAPPRXLdasJReNmlpXqeVRiXeEph3blwQdaLgAgKDI7ZsWH+/N/7o17+YL7Nu72/is/H7yCO7zXsbUv05tviFg3llNXzsu7C9Plw0uMnmB0cvqLxmz73HYuuC3ofqJ5W9BNiBFaFM7pLl/Jj0IU5Vbg9FHWvw2cJKVcCfwncFMJ7Joc7AbGABgGQ9tf5jCxGYEkMIIAzTv9U/C5t+D9twMQk+68GQXmn65hzp8v4d9uuStrhYHXGMZw116buVFZfKrVzLl5ZspDTxLHhbeQzkNJD70GOzZVDTNXkdB8LBI7CRWa6ZKwbqQvtZ0B80/knWIdUenidWmJcDyUf1A01ptOafQK67tzOcJmKfEENPcI3rfLS5sI8tpPPsoHfvDAyDbe81m4u/g5H9ns77dCUWFse+yaTQGi+RtcTAQhGJrzzpzFHzl5WYkPlGbMq0xK+SgwYvUcKeWTUspkwO5pYHaJbCs/dhlbAKSBce+/AnCUa1P+FKZLfpnutNPUzXBgNi5hEMlTqtTcZU1nfn1rVoPp4V40TMK+8tRyqApkVhwVV2GNKpIeuqt8g0Z1jxAkXH78xBiKFBiXtj103eNLCZxXJDigtQIjC3pkMLct40C/IzwzbDWFuant0yNPynH5OV9/kg+77mP1nl+PbOOzN8FzP+LxNyfQClJKwr+1eg/M6bavP9tDD4hI6UMugO7LnRWqu8oXQy/1M/8VwL0jrRRCrBFCrBVCrO3pKWEHoPHi9NClyXDMEiK/zMoQ8NuPSAefnblcd+MhkTfkYtphBz+Z8clkZ5OEv44FvbE742280KGapIeuK0GfCKbuw0+UoWhhdYYeXG/NwfD4fPDOL6SWB3UrHq/FggznSYOMB3vZLxvpm3ZcatnwoMP3swVdNIx8rjsrjhoFyNE3/7pxzG1GJNLPrIHnAbjoOHvmqi3oDZRJ0Ce54XvJBF0I8U4sQf/8SNtIKW+SUq6WUq7u6qoCQXN66KaBa9iq+mb+wy8yt/vkK3Btnt6Admee0QS9KXtCki3oMkv06oor/prx1qTAC0UJekmQbj9+ESNYgIe+78Hvc8SGrwLg9vjB15JaFwnMBKxzePdAbhqkMbyfA7KJraf9GN5zs/WZYDq7Rg7ts/bbPPK5rttZJpBOMBiN4eEJ9CANWTebXlc3zLfTYu0nEp8ovsheIbh9NSjoQohDgR8D50kpRxgSr0Ii6UfJBjNIZ/htfup9L9ryczK38zZmnOhJhMuDi0TeC8c0bEHPSvlKn+TTJ2p99dI2j7ieHtgMZD+ljEQy5JI9Y1RRHC4fZ+tP8/371lki9tB/pwaqs5n26L/SJqzkgK0DiVSqHUBPo+XFtjDEHrunmpQynaYb3s8ADbS0tqaeyhKhdOpi2M608bWOLOiidW7qdQNRq9LjKOjDu0ZdPxryTcvR+Mu8f0l3IvK3ptYXcgMsFmFPIvqj/wL47Fvw0adLfgwnExZ0IcRc4HbgA1LKyjQIHC+OGhJzzZ1oSIZal47ygUw0lxVyGchzEkrTOqmbRKagDw9YMcDW9ip4QikjwlHMf6SyCTnY/VudoqIonob9GwB497ZvE/7jp+GRr8Km/AOOMVd6+vvFRy/M+NsfceQ7iDdMZ4m2M1Um+ht3PM7Sf/0TNz/+NlqknwOyifaAJzUrspEQvXZDh5At6A0dM0Y2tjWd/dJSQMmCmbGt4y4NLO6zggfd02emF7akh/yWzShDQxW7U9N5xx8GDR0w7eDSH8NBIWmLtwFPAUuFEDuEEFcIIa4SQtjzd/ki0AH8QAjxohBibRntLS0OQW+X1uNYoH1WwR/X3V5cGBnV6ZJImcdDj4VofNiq59DVVVwpzlpDkB4YPWNJgY+d5/8ALvwRdC4pk1VTi/naHoyI5X3/8sk3824TdaW/m0Pnd2fUz1l12GEkulZysNjGUDRBf3CYz758Npt8H+Tmux7FHeunn0Yrxdf20GeIPrb0WZOJYgN7CEo/na2tIxvZlhb0ZnIF3TQlH/rJs6n3B4utbNs/gbALcPyKxek3jq5YZUkjHrDGJ2iZU/p952HM0Sop5WVjrP8I8JGSWTSJyHg4Fd3tNPtAg7buwpN0dJcHt0hY9aOzSE6SaRBRTFNadUq2PJ5aP6utfDWRqwJHpotfFngB+tvg0H8ok0FTjxaGMewz/NE3enhfnm088QGeNZdx1PHvQmSni7q8iObptIvnGI4mePTBuznXXvUR1z344v1EXM3Wud3YjeHvYHliKxv3BDl2YQexwX30ymZmt40yr8DhoTeJsOV9m4Z1/uhu3ty6jembfk1ycuVM0cfWvmGWTh+/N+0fJQRUcpI11LvKl6ropB6nKhaMEQun/gDdwhrMmT4z/wSIfGi6G68wc0MuUuJNWAWOXCSIGSY+Tc+Iwy+eVmd56FkIHJNaRslyUJSPBDrJ6ETG95HEiOM1QjynH87Rf//l9PKVF0PbfABcvgA+ogzHDC5/4crUJiG8eM0wMW+rfQCBNuNQVoa38LM9dk+B/rfYLro4ZlRBn2udH8M9+IjRH44T/vkl+N/+K+s+vIX2+z/P/7jTiXPTRD+7ggWOyTixQ6D3aCdxZnZz9k+8BFqZpPC0/4SDz4HpK8qz/yzqaKpi8SSioVRxrSYRZq9sZemsItpD6R58msFAKA57XoEDdhGjeBiXXSrUjUE0madupoXfU8hEmxpG2B76I+ZhcNY3K2zN1CSCh5g9QKmTO1cimbYbd2eNWbznx3CK1c3H5WnARwwzuDdjk7malfuQSAo6IGYcymJ28Nae/RAP0x3exI7A8tHnILi88JmNDHUfjZc4H/rJc/jftgYvf/LEFpoOvJax+TTRT894BD1uhYF2ehbmrmubnxFLLyneRlh0Snn2nYf6VpUxSERDDJOewXaP+DvaGopImdPdNIiIFff74QnwbXuqryMd0k2CqJ3xkl0zo55JeoS3GydmZBIoJoEzrJopLgyiCet7COSrFposfTFaSzR3AI8waB54HYCbF34bgHM1K3zoa3I4QNMPxU2cxN7XkX2bcWEw2FJAkoGmIzx+jtM3cLl+X2pxKBKlIZZ5I+nWBsYn6HZrSemp7yfjKS3oRixMiHTccFDvKG4H845nlrmbztCm9D4icQinc3E9JIglbO/IFvQb500djzVaxkJEihE4Zg2D896NhwRR25fwESOeXSslbo1tiFEF3QqXeAatp09fxyxwp0vhejocg3127ZL58bfY8rad8FbgYKBmx++vd/8stawhsgefmU5cMJtn00E/f3pxO7v6i2yKYd+8NF8ZMlmqiCkt6GYsTESmPfKYq8iByukrAfBF07Pj7nxhJzia3rozBN3ykuL++s5wcRJHNXquBNYcCYOofe4FiOamBNq1VlyjTX5xW0Ib3G05Lc1dc+Dyu1OrVx9+RHrbjkUYLj8Ha1v58V2PAeDtLGxMSiM3FXFmdAsAb/lWgKcJbeFJ6Jg0xvq4+hfPF7TfJHv2WbNW50yv72tvSgu6tDvSJ4nqRT6OuaxwjXSEUvYORlKeD4BbJFIXVdJD171pD6feKXjav6KkaC43XmIcPvgQAH4Rzc3ftuPKrjz1RlLY3vga193EpM7y+bNSA6YAixYschxURwt08BHXvVygP05c6jR3FZYGrMvcST3TIlaHnwfnXQP/ugOWnAbAM75/Zu+Ot/j504U33tj9ppVNveSg+u5TqwQdp4depKAnZzQaaUEfCMcz8ttTHrppEttn1aHweqdONUEl6JVBc3mZq6XrJfmxBd1IpCcZ2R66JzBKGMJRytgjDBZNa7LTSy+Bv/+fnKJbImjN5FytvcFe2pjeWthTbz5Bnxm1qjnqrXbYpjE9u/oK1738952FT3lpevsv7JCdzF66uuDP1CJTWtBJRIg4WsgVHXLRLQ9dT6Q98mAokhL0KFYH9phhIp/6Hp4n/w8AdxmbxFYb5SzmrxgZLatiZYAooZgBT30PfnEhfS/8mQO7LcH0+EcLueR5mhQCLrwJjvto7rrZR6de7pSdTG8uzHkRZu5cjtPlY8Sljr/dnmnqmHB2pesernb9aeQdDvWw4Vvn8elbH8CMRZg38AzPeI/DU0ArxFpmSgu6iIcIOwZFE+P00H1GulGGN7g9VWVuWDRYWS5xk/j2dMyvP1bfJ5WTfznjkEqbMCXRs+qP+0WMcDwBQWs6/g9+fy9tj1jlojvb2nI+n8LZbOSoK0feLsmlv8Jot4R3l+xgWnOBXYiMXEEHcAuDjib7ptLQCf+c9spldtG32HB6P+tuZXn/wyzZ9BP6e3bglnEGGhdT70xpQdejgwRJeyBmPm9kNOwYulPQv7b7Q3DftQCE9UZcGMQMg2FvenZaXEwdr/W4gxdU2oQpiZ5V4MxPhHDMTKWQtorh1Lp3HZYnNztJm/39zTkGTv/K2Adu6EBbeBIAPbIVbwGtB4ERBR3InGnqa029DElfZj/f/54JP7Xmssbc1iS+LjHAA2tfBcDdUscVTm2mtKB7EkESnvSkCuEqsqehXebV7xB0J3F3Mx6RIBwK0/bSjanl/3Ry4QXAap6ORWNvoyg5Im/IJV1NcaZIF0XVvaOEGlvnwDUvWs1d9MIcEWFv52kqIg04GXJpmQsfySwkNqfd4Wg55jQERDRVNCzFticBCNpldtsIcu8zLwPQ3DGTemfqCvquF/CbQxnV5Y5aPEpVuHzYXlCDHM6/3mOFXFq2/y1jcX02h87ikAth2iGgTZ3wUlXhqCk/LBrtkIuBac/gfY9upRVGpHvs+vPtC6CxiPINtrf93hOLCLctP8/6ueZhmHVkxqpG5/XiePLwE+X+DZkTjwBiCRPxmhVfX6G9zS2ebwBw+rH5e3zWE1NX0G86GQBPQ7q+yqXHFhljswdFA3kEPSS9uDw+3CQIOwfwZ9ROD+0JcfFP4KNPVtqKqYtDpMPuFvxECUYSxKOZE3K2nvL9kdvDjRfb2/Z4injiPela+PwWq8TsWPZc8wKG7mWG6OPff/88UmbWqdm3ZyftfdaYVbfoTy33thZeSbVWmQKu4ujMazQhmd1VbKcce/s2ckMuYTy2oBskIo71//ToOC1VKIrAER6Je1rxR/oZjMRJRMM4ZXbpgsKL0RVMqvNUEY1KNM1KhyyE9oXQ2M05A0/jJU44fjYBR22kfT17chob7/3AI3RPgV61U9dDt2ludMQPtSL/HLoLE41lWm57uggevF4fXpEgMWzNJM0ZlVcoyoVIn8shvYmAiDIYTpCIpWu6vMUsmHF46Y/dbo+blKvgFaBHrW5j79aft5phO+aC9PVl9it+wDiC9vn1H26BKSzoB2adDMC+FQWkYo2CqblpF0MMyMwMmYTUaQz48QqD+JA1AHX7aU9M6FgKRcGE0uUogqKJBhFjMBLHcAj6/7mvTE3tLyknfAo+cAcsPHn8+/jAndbPI96ff73dPnKn7GAwksgofBcf6gdgW8CaFTpAQ3maV1QhU+O3zIcRZ615EJ4JTvKRmvVYuUtmjui7hIHb48Mr4hihAwzIAA0tRRb/UijGy3Bv6uVBC+YRIMJgOI4Rd1RddJdpgpvumnjJ2EXvhC/shHO+k3+93VtgozmHoWgi3Y8WSIT6AQj5rZj58rn1n66YZOoKeiJCVLoLz5MdAdOOow+ReXHMbNTA30qHPMDFibuRCOZ3Tp0aLooK44juBdpn4iVGJDSE6RD0WV3tFTCsCLyNI2dJrXmYhKcZHdMKuTg89OFB64l4oPNwAJa595Xb0qqhkJ6itwgh9gkh1o+wfpkQ4ikhRFQI8S+lN7E8iESUKO4JN5pIeuhDMlPQRSKa0amnVQwzv6PO284pqofT/jP9OmDVLA/17yMaSZep+MTpKyfbqtLRvpBox3J8Imblojs89AN91tOJWHa2tWDFeyphYUUoRM1uBU4fZf1+4BrgG6UwaLIQhiXo3okKup26mO2hc+zVOa3XfHVeR0JRRQTa4bD3pl8DA/v3MmNXek6Ef7SiXDWA7vHjJcb+UCzDQ+8Mv81+2cj8RUvhiwdg9YcqaOXkMqaaSSkfxRLtkdbvk1I+B4w8d7cK0VKCPkGRtWeXDkk/8Yt/Cad+Ef6jD07+QsozUigqwvk/gOv6IWCN3RwqNuN2Xqau2i4S5/U30iDibNg1mJHl8g79VTawiK4mX/GZazXOpOahCyHWAGsA5s6dO5mHzkEzokSlZ8IeOnanlSH8uA85Gzg7va5BDYIqKkhygo7tWJyivZi5vlyDopOEcPtpciV4bfcgJNJSNlv08rTn+NF7mdYpk3r7klLeJKVcLaVc3dVV2U7wmml76O6JCnraQ8/BUVagb/ZpEzuOQjFeOhYTal7IaXpWlx9Xjdfl13SmG7txDW7L8NABhhsq6zBWiqn1POJAN2PWoKiugdALn6WWhWbXxsiJoUOGoL918nfHtX+FYsJoGrHGdG/P3x5+K7z7y7UfjthkjQdcFr6NwaHM8htmaxlmwNYAU3bqv25GiQsPLl2Df9s97v1odqeVvILuSwv69PaW3PUKxWRhT8MPSS+x6avg2AsqbFAJaOyG4R52GS185rZn+ZFjZr/WXP+VFfMxpqALIW4DTgY6hRA7gOsAN4CU8odCiOnAWqAZMIUQnwSWSykHy2X0hDES6NLAEPYZUGzZXAdJQZ8xLU8IybHfjBKgCsUkkyxpG8JLk69O/Lj3/hb+bzkrxduc584sBKc3T53JRE7G/GallJeNsX4P5NTCqW7seFui2GJcedCl1Xj3qtMOn/C+FIpyodkeelh68ddL+mzLLOLuFv6OV3JWXfCOGs6xnwA1HkQbJ3bOajKHfEKYlofua2yd+L4UijIhkoKOh4CnTjx0wPDkbxvZ4C/BtV2DTFFBt6Y/S70Eo/zJTiveIvuRKhSTiOayY+h48Xvq57I3RxD0qUr93KqLwRZ0UYpKc4bdvWKkE+uT64uvs65QlBhhn4Nh6aPFXT+XvfRkzna966ifcfbhtRUBLiX1c6suhuQ04QkMhqZIpjuOlNPbOgeapuYAjaJ6SHroYTz4PXUSQweEN1PQDzQtzWlhN5Won1t1MdgeulYKD/29v4aN9yrRVlQ1uiPkEqgnQc+a7SqnQFei0ZjSHnpJBL11LhzzTxPfj0JRRnR7UDSCt66KxOmuTJ9UiKkpaUmm5m+f8tBru5aFQlEowpWeWFRPHrquZdZrWT2/ymu8l5kpJ+iDkTg/e2wjAC6vEnTFFEFLx9DrqR2bllW+4OAZzSNsOTWon2+2QL77wJs8uXEXAC5PjRcnUigKxe78M7OjtbJ2lJipHmLJZkoNikop8b54K9903wpAS3NtF/hXKArGngB39hH1VoVw6pXIHY0pJeh7+/r4l/iNqXPg75ZP3XxVxRTDbtEm6m1OxBSseT4aU+p5ZfCZX2W8754+Z4QtFYo6IzkBrt4EXZHBlPLQxfan02+6V5ZmYpFCUQskG0DY6Yv1g+2hLzsblp5RWVOqgLr00B/auI/1Owdyljcd2MDLHAQzV8GFN1bAMoWiQtghl7oTdOEQ9CPeX1lbqoC6E3RpGtzy05s5+7uPEUtY3YSiCYO/rN9NY2Q32wPLYc1D0H1IhS1VKCaRug25JGPosqJWVAs1L+jPPHQXNz74GgnDEu/Hfv8dfu75Chdoj/Pzp7dimpKV193PZ3/xGI0iwoqDl1fYYoWiAqQ89DoT9JM/D3OOgaVnVtqSqqC2BX3/WxzzyPtofvALnP3dx4nEDZ55aQMAK7Qt/OddG7hn/W7Olg/zsu9KAOYtWFJJixWKyjDnGOtnZ52d/+0L4Yr7wd9aaUuqAiFlZR5VVq9eLdeuXTuxnex+CW78OwAWRn7BodNc3Dl4aWr1e6LXMVv08G3PDwCQ3Ycg3n+HKqSlmHpICf1boW1+pS1RTBAhxPNSytX51hXSU/QW4Gxgn5RyRZ71Avg2cCYQAi6XUq6bmMkFEg2mXn5j0Us88XYQHE+Uf/B+KWNz8U+PpWbMKRRTCiGUmE8BCgm53AqcPsr6M4Al9r81wA0TN2tsvnbf61x50wOp9+cO/Y7/5/7JyB8QmhJzhUJR14wp6FLKR4H9o2xyHvAzafE00CqEmFEqA/MSD7Pqias4S7fyynd2HIdrYAsNIsrGZR+Dz2/J3P59v4dPvFRWkxQKhaLSlGJQdBaw3fF+h70sByHEGiHEWiHE2p6ennEf0NixjnfpL3C+/iQAiQt+DDOPAGDp0kOsLkLv+Li1cdcyWHKaVbdcoVAo6phJnSkqpbwJuAmsQdHx7qe3dy/OYc15M2fA8vNg1wvQbD8cnHo9BDqsGaEKhUIxBSiFoO8EnEVRZtvLysbObZszBB1Nh3d8AmathvknWMt0F5zwqXKaoVAoFFVFKUIufwI+KCyOBQaklLtLsN8Radz2EADymKvg8ruthZoGC05U1dcUCsWUpZC0xduAk4FOIcQO4DrADSCl/CFwD1bK4iastMUPlctYrIOycPBZ/uR6N+ee8dWyHkqhUChqiTEFXUp52RjrJfCxklk0FtEgLhmn36dK3yoUCoWT2pv6H+oFQAY6K2yIQqFQVBc1J+jPvvomAJ7maRW2RKFQKKqLmhP0hYEQAGcck1OFQKFQKKY0NSfonV0z4OBzaJ0+v9KmKBQKRVVRey3o5h5r/VMoFApFBjXnoSsUCoUiP0rQFQqFok5Qgq5QKBR1ghJ0hUKhqBOUoCsUCkWdoARdoVAo6gQl6AqFQlEnKEFXKBSKOkFYxRIrcGAheoCt4/x4J9BbQnPKQbXbqOybONVuo7Jv4lSjjfOklF35VlRM0CeCEGKtlHJ1pe0YjWq3Udk3cardRmXfxKkFG52okItCoVDUCUrQFQqFok6oVUG/qdIGFEC126jsmzjVbqOyb+LUgo0pajKGrlAoFIpcatVDVygUCkUWStAVCoWiTqg5QRdCnC6E2CiE2CSEuLZCNtwihNgnhFjvWNYuhPirEOJN+2ebvVwIIb5j2/uyEGLVJNg3RwjxkBBigxDiVSHEJ6rQRp8Q4lkhxEu2jV+yly8QQjxj2/IbIYTHXu6132+y188vt432cXUhxAtCiLuqzT4hxBYhxCtCiBeFEGvtZVXzHdvHbRVC/F4I8boQ4jUhxHHVYqMQYqn9t0v+GxRCfLJa7BsXUsqa+QfowGZgIeABXgKWV8COvwNWAesdy74GXGu/vhb4qv36TOBeQADHAs9Mgn0zgFX26ybgDWB5ldkogEb7tRt4xj72b4FL7eU/BK62X38U+KH9+lLgN5P0XX8a+BVwl/2+auwDtgCdWcuq5ju2j/tT4CP2aw/QWm022sfWgT3AvGq0r+Dfo9IGFPlHPw74i+P9F4AvVMiW+VmCvhGYYb+eAWy0X98IXJZvu0m09Y/AadVqIxAA1gHHYM3Kc2V/38BfgOPs1y57O1Fmu2YDDwCnAHfZF3I12ZdP0KvmOwZagLez/w7VZKPjWO8GnqhW+wr9V2shl1nAdsf7HfayaqBbSrnbfr0H6LZfV9Rm+9H/CCwPuKpstMMZLwL7gL9iPX31SykTeexI2WivHwA6ymzit4DPAab9vqPK7JPA/UKI54UQa+xl1fQdLwB6gJ/YYasfCyEaqszGJJcCt9mvq9G+gqg1Qa8JpHX7rng+qBCiEfgD8Ekp5aBzXTXYKKU0pJSHY3nCRwPLKmmPEyHE2cA+KeXzlbZlFE6QUq4CzgA+JoT4O+fKKviOXVihyRuklEcAw1ghjBRVYCP2OMi5wO+y11WDfcVQa4K+E5jjeD/bXlYN7BVCzACwf+6zl1fEZiGEG0vMfymlvL0abUwipewHHsIKYbQKIVx57EjZaK9vAfrKaNbxwLlCiC3Ar7HCLt+uIvuQUu60f+4D7sC6KVbTd7wD2CGlfMZ+/3ssga8mG8G6Ia6TUu6131ebfQVTa4L+HLDEzjTwYD0m/anCNiX5E/CP9ut/xIpbJ5d/0B4hPxYYcDzOlQUhhABuBl6TUn6zSm3sEkK02q/9WDH+17CE/aIRbEzafhHwoO09lQUp5ReklLOllPOxzrMHpZTvqxb7hBANQoim5GusGPB6qug7llLuAbYLIZbai04FNlSTjTaXkQ63JO2oJvsKp9JB/GL/YY00v4EVb/23CtlwG7AbiGN5IVdgxUsfAN4E/ga029sK4Pu2va8AqyfBvhOwHhNfBl60/51ZZTYeCrxg27ge+KK9fCHwLLAJ6xHYay/32e832esXTuL3fTLpLJeqsM+24yX736vJa6GavmP7uIcDa+3v+U6grZpsBBqwnqRaHMuqxr5i/6mp/wqFQlEn1FrIRaFQKBQjoARdoVAo6gQl6AqFQlEnKEFXKBSKOkEJukKhUNQJStAVCoWiTlCCrlAoFHXC/wf5WWD9Ha4zTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(X_test_t)\n",
    "pyplot.plot(y_pred, label='predict')\n",
    "pyplot.plot(y_test, label='true')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_X = test_X.reshape((test_X.shape[0], 4))\n",
    "\n",
    "print(test_X.shape)\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 0:3]), axis=1)\n",
    "print(inv_yhat.shape)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(756, 1)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (756,1) doesn't match the broadcast shape (756,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-03005c0f8d92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mscaler2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0myhat_inverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtestY_inverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrmse_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_inverse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    459\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (756,1) doesn't match the broadcast shape (756,5)"
     ]
    }
   ],
   "source": [
    "#history = model.fit(X_train_t, y_train, epochs=30, batch_size=10, validation_data=(X_test_t, y_test), verbose=0, shuffle=False)\n",
    "yhat = model.predict(X_test_t)\n",
    "#yhat_inverse = yhat.ravel()\n",
    "#testY_inverse = y_test.ravel()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler2.min_, scaler2.scale_ = scaler.min_[0], scaler.scale_[0]\n",
    "\n",
    "yhat_inverse = scaler.inverse_transform(yhat)\n",
    "testY_inverse = scaler.inverse_transform(testY)\n",
    "rmse_sent = sqrt(mean_squared_error(testY_inverse, yhat_inverse))\n",
    "print (\"Done\")\n",
    "print ('Test RMSE: %.3f' % rmse_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9667460117283561,\n",
       " 0.9846662102636626,\n",
       " 0.9864486591660434,\n",
       " 0.999857561727005,\n",
       " 0.9900084899966671,\n",
       " 0.9975278796177024,\n",
       " 0.9887738372983345,\n",
       " 0.9930807653157739,\n",
       " 1.0165487375352438,\n",
       " 1.003704521092124,\n",
       " 1.0095495572590596,\n",
       " 1.005676700026123,\n",
       " 1.003131953014512,\n",
       " 1.0024681794024124,\n",
       " 0.9987985641321285,\n",
       " 0.9935925297037284,\n",
       " 1.0050894940232227,\n",
       " 1.0062318151928151,\n",
       " 1.0114384126183422,\n",
       " 1.0072857458135531,\n",
       " 1.0202600145928855,\n",
       " 1.0123611649086142,\n",
       " 1.0044020745318116,\n",
       " 1.010961554052228,\n",
       " 0.9983318395142909,\n",
       " 0.9976320340860981,\n",
       " 0.9961626115860304,\n",
       " 1.007211993189987,\n",
       " 1.0039623737760444,\n",
       " 1.0093778431354887,\n",
       " 1.01222491960401,\n",
       " 1.004485398106528,\n",
       " 1.0148839550322934,\n",
       " 1.0125964977074755,\n",
       " 1.013957261762136,\n",
       " 1.0143750056299714,\n",
       " 1.0273661643230971,\n",
       " 1.0342217803320333,\n",
       " 1.0368695558177872,\n",
       " 1.0404378316053076,\n",
       " 1.0283992640501562,\n",
       " 1.0318194715934168,\n",
       " 1.0361725653752263,\n",
       " 1.0831321556934772,\n",
       " 1.131379883437075,\n",
       " 1.121758262545828,\n",
       " 1.146191774837181,\n",
       " 1.1884706070460216,\n",
       " 1.2108942195959034,\n",
       " 1.215630151423707,\n",
       " 1.2198295469899922,\n",
       " 1.0899303459955139,\n",
       " 1.1431211885094537,\n",
       " 1.1689070198985703,\n",
       " 1.1706917207894572,\n",
       " 1.186644244367777,\n",
       " 1.193386134957167,\n",
       " 1.2021840910523993,\n",
       " 1.1939778449370793,\n",
       " 1.2297664462720581,\n",
       " 1.2573122066784967,\n",
       " 1.2711174592164882,\n",
       " 1.299479565456298,\n",
       " 1.2933417707836021,\n",
       " 1.2839194508751226,\n",
       " 1.2661067847909706,\n",
       " 1.296755222361345,\n",
       " 1.2787269283777576,\n",
       " 1.2672727518398745,\n",
       " 1.266097776836947,\n",
       " 1.2489899831551257,\n",
       " 1.1963339879113257,\n",
       " 1.2102754857539209,\n",
       " 1.2032847504346336,\n",
       " 1.2026727725581687,\n",
       " 1.2567886193508868,\n",
       " 1.248544652428094,\n",
       " 1.2238820003062707,\n",
       " 1.219100465711223,\n",
       " 1.2082723419779664,\n",
       " 1.227758235521966,\n",
       " 1.2436696603100534,\n",
       " 1.2473674254366602,\n",
       " 1.2338121436228189,\n",
       " 1.206282147135921,\n",
       " 1.2354048624935818,\n",
       " 1.1655791438840497,\n",
       " 1.1625153135218396,\n",
       " 1.103522785619702,\n",
       " 1.0625529217298872,\n",
       " 1.0898031086449333,\n",
       " 1.1175701269220721,\n",
       " 1.1258366137299234,\n",
       " 1.1674499833352852,\n",
       " 1.1458078107969336,\n",
       " 1.120124444884833,\n",
       " 1.1024429571311467,\n",
       " 1.1129529874879518,\n",
       " 1.1410200832334954,\n",
       " 1.1181601479106051,\n",
       " 1.1048413248898776,\n",
       " 1.1228814418131208,\n",
       " 1.1609468035275148,\n",
       " 1.1832437417239423,\n",
       " 1.2041900498139855,\n",
       " 1.1788974039076505,\n",
       " 1.1147675272265412,\n",
       " 1.0972149658148145,\n",
       " 1.070926940988893,\n",
       " 1.1106762271085366,\n",
       " 1.0932503400502642,\n",
       " 1.1144359219190543,\n",
       " 1.135406438885536,\n",
       " 1.1228420320142685,\n",
       " 1.1484386963688937,\n",
       " 1.1385778016988999,\n",
       " 1.1311192157675225,\n",
       " 1.17992881464333,\n",
       " 1.166785646726059,\n",
       " 1.1895059587615866,\n",
       " 1.2049118121301108,\n",
       " 1.24211691423527,\n",
       " 1.2632478853827926,\n",
       " 1.2767232216046769,\n",
       " 1.2603107293740372,\n",
       " 1.2888220298523594,\n",
       " 1.27398198859593,\n",
       " 1.2700173628313802,\n",
       " 1.2642624062046788,\n",
       " 1.2683300604433714,\n",
       " 1.2832556772630235,\n",
       " 1.3528989848035815,\n",
       " 1.3322685181014835,\n",
       " 1.3400446344121857,\n",
       " 1.3130331582787598,\n",
       " 1.3072888985974616,\n",
       " 1.2702701485411616,\n",
       " 1.300761509913253,\n",
       " 1.3301296920180516,\n",
       " 1.3289851188599533,\n",
       " 1.307613184942304,\n",
       " 1.289507197355265,\n",
       " 1.3091451001234091,\n",
       " 1.3103701818705917,\n",
       " 1.3271784610811346,\n",
       " 1.3463614621710969,\n",
       " 1.322979628511976,\n",
       " 1.3898952149748225,\n",
       " 1.3803569176582924,\n",
       " 1.388562037779359,\n",
       " 1.432272571680794,\n",
       " 1.4535009413311952,\n",
       " 1.4382865069856683,\n",
       " 1.443428359741652,\n",
       " 1.4721789339987206,\n",
       " 1.4712736346193687,\n",
       " 1.4672482051651605,\n",
       " 1.4545543089548065,\n",
       " 1.4347396250889535,\n",
       " 1.4538849053714427,\n",
       " 1.470207881058975,\n",
       " 1.4448324745750494,\n",
       " 1.4901492392782827,\n",
       " 1.4989815381982288,\n",
       " 1.5106215037878443,\n",
       " 1.5251237467683962,\n",
       " 1.5530050534622069,\n",
       " 1.5621025240287172,\n",
       " 1.5967932809670935,\n",
       " 1.5363437165016705,\n",
       " 1.5581407132497995,\n",
       " 1.5613025051120135,\n",
       " 1.5943605703836483,\n",
       " 1.592717181771504,\n",
       " 1.5803154810697846,\n",
       " 1.544150797654329,\n",
       " 1.5361528604758,\n",
       " 1.4532402736616432,\n",
       " 1.518619440966373,\n",
       " 1.5375170025132192,\n",
       " 1.4956311422986492,\n",
       " 1.5279432363777214,\n",
       " 1.5511330880167185,\n",
       " 1.5335495617630364,\n",
       " 1.5738550890436254,\n",
       " 1.6522862187311396,\n",
       " 1.6833912469710755,\n",
       " 1.6841608640429495,\n",
       " 1.6854833442930106,\n",
       " 1.6935584120778642,\n",
       " 1.658433584354985,\n",
       " 1.6870642402241174,\n",
       " 1.6681222019042812,\n",
       " 1.6046403349157305,\n",
       " 1.6299059569599952,\n",
       " 1.5563785322439712,\n",
       " 1.6116316332321436,\n",
       " 1.6498732130471203,\n",
       " 1.645444677650365,\n",
       " 1.6268832253880174,\n",
       " 1.6210601461090142,\n",
       " 1.6087693558412073,\n",
       " 1.6238368479367282,\n",
       " 1.6233087566321056,\n",
       " 1.5715107690090346,\n",
       " 1.5529087809535818,\n",
       " 1.5898357624782684,\n",
       " 1.6210685910659106,\n",
       " 1.6289201489915595,\n",
       " 1.6327749903164492,\n",
       " 1.6434922036157924,\n",
       " 1.6590922909929469,\n",
       " 1.6718351679533026,\n",
       " 1.6595201688090584,\n",
       " 1.6450213038112653,\n",
       " 1.6145811751776815,\n",
       " 1.6378537873942691,\n",
       " 1.6496868609982611,\n",
       " 1.6500449271706912,\n",
       " 1.6491705926332947,\n",
       " 1.6570446704440016,\n",
       " 1.634292267572266,\n",
       " 1.62294843847117,\n",
       " 1.6472378234981484,\n",
       " 1.6789238647725941,\n",
       " 1.6697363146658497,\n",
       " 1.6352459847044938,\n",
       " 1.634987006026321,\n",
       " 1.639962774629998,\n",
       " 1.6664394034932846,\n",
       " 1.6668374424616936,\n",
       " 1.659552822642393,\n",
       " 1.6563876527974197,\n",
       " 1.6380941871672685,\n",
       " 1.5665929891093833,\n",
       " 1.58620499400971,\n",
       " 1.5995733607775664,\n",
       " 1.5788100267536231,\n",
       " 1.5936495050129262,\n",
       " 1.5694980542819303,\n",
       " 1.5324748002486195,\n",
       " 1.5010280327529206,\n",
       " 1.445651072396926,\n",
       " 1.480025424950231,\n",
       " 1.4998181519281526,\n",
       " 1.5224281165268931,\n",
       " 1.5084021691153287,\n",
       " 1.4500683478511522,\n",
       " 1.4542384675668614,\n",
       " 1.4032590777656666,\n",
       " 1.348245250556241,\n",
       " 1.3572526415825172,\n",
       " 1.3230494401556574,\n",
       " 1.2081681875095711,\n",
       " 1.2938023024330483,\n",
       " 1.3528460630736943,\n",
       " 1.3694972660859537,\n",
       " 1.3483584129786603,\n",
       " 1.280674898435318,\n",
       " 1.2830349823894496,\n",
       " 1.1951505679515013,\n",
       " 1.2298097970507955,\n",
       " 1.1345596912073361,\n",
       " 1.1417519794978968,\n",
       " 1.2052169565726536,\n",
       " 1.1929064614054208,\n",
       " 1.1732578616918738,\n",
       " 1.237044310125841,\n",
       " 1.2745162728689432,\n",
       " 1.2970294019619324,\n",
       " 1.3560523317088986,\n",
       " 1.3255097375982992,\n",
       " 1.2876954726023078,\n",
       " 1.2952311891400106,\n",
       " 1.327555106158738,\n",
       " 1.3452895156423121,\n",
       " 1.3771844288506747,\n",
       " 1.3861017403367168,\n",
       " 1.4096147523263043,\n",
       " 1.3725025447470114,\n",
       " 1.379677380126652,\n",
       " 1.3321356507796382,\n",
       " 1.3497152360534352,\n",
       " 1.3289457090611005,\n",
       " 1.231767901056633,\n",
       " 1.2387028996604004,\n",
       " 1.26260381667012,\n",
       " 1.3285290911875185,\n",
       " 1.3276767135380538,\n",
       " 1.3284119877852145,\n",
       " 1.3102874212930016,\n",
       " 1.2889481412086874,\n",
       " 1.3114066595804093,\n",
       " 1.2568972777962943,\n",
       " 1.2751141758172468,\n",
       " 1.184225608712493,\n",
       " 1.2221198193004423,\n",
       " 1.215764144739805,\n",
       " 1.2596137389314763,\n",
       " 1.2246887751884912,\n",
       " 1.2319356742003187,\n",
       " 1.2587056245664918,\n",
       " 1.3274244908253987,\n",
       " 1.3017810977092772,\n",
       " 1.2917344139875508,\n",
       " 1.3236096222964875,\n",
       " 1.2931818795996866,\n",
       " 1.2860042292344138,\n",
       " 1.3049963742985056,\n",
       " 1.285436165133813,\n",
       " 1.3078051669624275,\n",
       " 1.3146619089656166,\n",
       " 1.3167388053651372,\n",
       " 1.370030424364714,\n",
       " 1.384637947807914,\n",
       " 1.397860498320016,\n",
       " 1.456493834055471,\n",
       " 1.4504686388080672,\n",
       " 1.4621671560988352,\n",
       " 1.4849736066947115,\n",
       " 1.4664144064208693,\n",
       " 1.464212524659274,\n",
       " 1.44515225694288,\n",
       " 1.468654008989938,\n",
       " 1.4715663931251295,\n",
       " 1.4884247790799274,\n",
       " 1.5113708529631662,\n",
       " 1.519763451127345,\n",
       " 1.4923094592525197,\n",
       " 1.523888531072937,\n",
       " 1.5184173249979729,\n",
       " 1.5127107861241473,\n",
       " 1.5537217488041941,\n",
       " 1.587230211776999,\n",
       " 1.5811881266158019,\n",
       " 1.5663007936007496,\n",
       " 1.575466949816688,\n",
       " 1.5702941322187494,\n",
       " 1.6009543927287795,\n",
       " 1.5716807941412267,\n",
       " 1.5404119337374897,\n",
       " 1.550241300568402,\n",
       " 1.5620839451235438,\n",
       " 1.5774036599317198,\n",
       " 1.5606561844108344,\n",
       " 1.5664955906065057,\n",
       " 1.535609005251637,\n",
       " 1.5196142568888327,\n",
       " 1.4867340987091597,\n",
       " 1.4969007008188226,\n",
       " 1.5204598785727796,\n",
       " 1.5107183392935961,\n",
       " 1.5337015709871813,\n",
       " 1.5208545395584299,\n",
       " 1.5003895940115122,\n",
       " 1.483666327367065,\n",
       " 1.4773934133840179,\n",
       " 1.4347064082584917,\n",
       " 1.3595372839217026,\n",
       " 1.3939245854089157,\n",
       " 1.3829523344112853,\n",
       " 1.3518084593696233,\n",
       " 1.3369014214551451,\n",
       " 1.362160287533892,\n",
       " 1.3989560907281127,\n",
       " 1.4180946150450846,\n",
       " 1.4289120418329384,\n",
       " 1.4505024186356552,\n",
       " 1.4418891255979025,\n",
       " 1.395707597308423,\n",
       " 1.4144570906110094,\n",
       " 1.4157598659616437,\n",
       " 1.3768021538018065,\n",
       " 1.3911400016214315,\n",
       " 1.4178328213812792,\n",
       " 1.4668940799726156,\n",
       " 1.4654499923432391,\n",
       " 1.4645064091592874,\n",
       " 1.4783673984127983,\n",
       " 1.4621969949465377,\n",
       " 1.4500418869862086,\n",
       " 1.453618607730626,\n",
       " 1.4681878473692267,\n",
       " 1.453655202543846,\n",
       " 1.4363728797528217,\n",
       " 1.414593335915613,\n",
       " 1.4072372154612522,\n",
       " 1.4263008611604047,\n",
       " 1.3820036842531955,\n",
       " 1.3929764982479527,\n",
       " 1.3858478286326825,\n",
       " 1.406861133380775,\n",
       " 1.4241654130597317,\n",
       " 1.408113238990028,\n",
       " 1.3960026078026893,\n",
       " 1.3769344581265255,\n",
       " 1.3602162584562167,\n",
       " 1.3510602361885544,\n",
       " 1.3144248871753756,\n",
       " 1.3113818877068448,\n",
       " 1.2803005053462204,\n",
       " 1.3196117797014764,\n",
       " 1.3449032996135588,\n",
       " 1.3353689432769134,\n",
       " 1.3199011602244783,\n",
       " 1.3160710907731525,\n",
       " 1.3691532748416853,\n",
       " 1.3795726626611295,\n",
       " 1.3743367893850271,\n",
       " 1.366714371289849,\n",
       " 1.3644955996144592,\n",
       " 1.388627908443155,\n",
       " 1.414817971769072,\n",
       " 1.3950066658859774,\n",
       " 1.3753901570086384,\n",
       " 1.3827839982704724,\n",
       " 1.3914761109059297,\n",
       " 1.37095261365786,\n",
       " 1.3330645960383016,\n",
       " 1.335185969210813,\n",
       " 1.3486720023781,\n",
       " 1.3386095547368324,\n",
       " 1.3903574356156483,\n",
       " 1.3969152261446856,\n",
       " 1.4146901714213649,\n",
       " 1.396393890805581,\n",
       " 1.4084380883319971,\n",
       " 1.4419510552818133,\n",
       " 1.428986920450758,\n",
       " 1.409242048228586,\n",
       " 1.4094852629872174,\n",
       " 1.397149432949294,\n",
       " 1.3678364245628885,\n",
       " 1.3731331015286496,\n",
       " 1.3836954906182157,\n",
       " 1.3913415545927048,\n",
       " 1.3974112266130994,\n",
       " 1.4114782728148954,\n",
       " 1.419502670858368,\n",
       " 1.4242340987091602,\n",
       " 1.4374746651293089,\n",
       " 1.4330354327871508,\n",
       " 1.4141271742949022,\n",
       " 1.4044728995703206,\n",
       " 1.4019112626449153,\n",
       " 1.4227933890625417,\n",
       " 1.4535555520524621,\n",
       " 1.4539367011070774,\n",
       " 1.4464116815147774,\n",
       " 1.4478180483366812,\n",
       " 1.4555772747335896,\n",
       " 1.43067365984164,\n",
       " 1.4492964787907718,\n",
       " 1.4423147514255086,\n",
       " 1.4602540918631153,\n",
       " 1.4626907434264456,\n",
       " 1.4514009620494894,\n",
       " 1.4120164980677936,\n",
       " 1.4013955572770755,\n",
       " 1.4109845243349874,\n",
       " 1.4122450749011373,\n",
       " 1.4226869826056405,\n",
       " 1.3806417942042821,\n",
       " 1.404660940610559,\n",
       " 1.3926251880410399,\n",
       " 1.3890890030897283,\n",
       " 1.3307777017106104,\n",
       " 1.3438808968319025,\n",
       " 1.3553801131399021,\n",
       " 1.3430155702485296,\n",
       " 1.367448519542756,\n",
       " 1.3559842090565968,\n",
       " 1.3173102474484968,\n",
       " 1.3131390017385351,\n",
       " 1.2947346256744705,\n",
       " 1.3121762766522838,\n",
       " 1.3259516903425723,\n",
       " 1.3215237179429438,\n",
       " 1.2841350787745576,\n",
       " 1.3128271013304746,\n",
       " 1.3200486654716115,\n",
       " 1.3415377027915651,\n",
       " 1.3389901407943214,\n",
       " 1.326516939457541,\n",
       " 1.3228878599803626,\n",
       " 1.3279047273742712,\n",
       " 1.3563535351715563,\n",
       " 1.3458716546710745,\n",
       " 1.3165541423076577,\n",
       " 1.305575135344509,\n",
       " 1.3055880842784178,\n",
       " 1.2850285552142542,\n",
       " 1.304196355381802,\n",
       " 1.309713727221136,\n",
       " 1.3066498968589264,\n",
       " 1.228963049372596,\n",
       " 1.2094805338113552,\n",
       " 1.205406123607145,\n",
       " 1.168034374352553,\n",
       " 1.186447195373515,\n",
       " 1.1764810202408729,\n",
       " 1.11040035851657,\n",
       " 1.1238785097240864,\n",
       " 1.1048779197030978,\n",
       " 1.1533356453748658,\n",
       " 1.1451817579923071,\n",
       " 1.186655504310306,\n",
       " 1.1522028951564232,\n",
       " 1.115374438128868,\n",
       " 1.0960816525992452,\n",
       " 1.0939360705502956,\n",
       " 1.0160701899777504,\n",
       " 1.051043571473611,\n",
       " 1.0876862394494338,\n",
       " 1.101230261320746,\n",
       " 1.0991725068235252,\n",
       " 1.1489943745327125,\n",
       " 1.1336476358624665,\n",
       " 1.091757834668012,\n",
       " 1.1321134686928556,\n",
       " 1.1264936313765053,\n",
       " 1.1249290623620656,\n",
       " 1.1446322727968794,\n",
       " 1.1354244547935828,\n",
       " 1.170123656688856,\n",
       " 1.1910716537702792,\n",
       " 1.1871999225315952,\n",
       " 1.1827359183158728,\n",
       " 1.1797875023645878,\n",
       " 1.233601582697522,\n",
       " 1.2517272751839874,\n",
       " 1.246523492744093,\n",
       " 1.2339973696774251,\n",
       " 1.208272904975093,\n",
       " 1.2159780836478609,\n",
       " 1.2004782097592175,\n",
       " 1.20071016457532,\n",
       " 1.1753679749218562,\n",
       " 1.1826435867871328,\n",
       " 1.1936895904083307,\n",
       " 1.2373365056344752,\n",
       " 1.2083078107969336,\n",
       " 1.214215902642033,\n",
       " 1.2148560303748206,\n",
       " 1.1882003684253197,\n",
       " 1.1804281930945026,\n",
       " 1.141909618693306,\n",
       " 1.1407847504346336,\n",
       " 1.1542426337455973,\n",
       " 1.1449875239836778,\n",
       " 1.1534279769036058,\n",
       " 1.1683924405249835,\n",
       " 1.1742346617062864,\n",
       " 1.171453455901561,\n",
       " 1.1814016151261564,\n",
       " 1.1716972336573193,\n",
       " 1.1799654094565502,\n",
       " 1.1638952194787997,\n",
       " 1.1652672434759892,\n",
       " 1.1535355093547603,\n",
       " 1.1546417987082593,\n",
       " 1.166118495131201,\n",
       " 1.1870952050660732,\n",
       " 1.206855841207786,\n",
       " 1.208635475124535,\n",
       " 1.2022375757794133,\n",
       " 1.204766558871483,\n",
       " 1.196526532928576,\n",
       " 1.1982909659229097,\n",
       " 1.2001319665264427,\n",
       " 1.2078647320584075,\n",
       " 1.1826542837325356,\n",
       " 1.1747064532982625,\n",
       " 1.1654029257834666,\n",
       " 1.1591880005044453,\n",
       " 1.147916798032663,\n",
       " 1.1545410222226224,\n",
       " 1.1516945087512274,\n",
       " 1.1221669984596399,\n",
       " 1.1429928251646202,\n",
       " 1.1558651914640627,\n",
       " 1.1684960319962525,\n",
       " 1.1752694504247252,\n",
       " 1.1942486465549078,\n",
       " 1.208902335762478,\n",
       " 1.204394980768018,\n",
       " 1.2478651148964537,\n",
       " 1.2380312440885302,\n",
       " 1.2353840315999025,\n",
       " 1.2430768243358883,\n",
       " 1.2366980668930667,\n",
       " 1.2414064118616737,\n",
       " 1.2155862376478428,\n",
       " 1.2233685469269364,\n",
       " 1.2265511696828297,\n",
       " 1.2835259158837253,\n",
       " 1.303730193761091,\n",
       " 1.2978862835884084,\n",
       " 1.2970429138929673,\n",
       " 1.2660020673254486,\n",
       " 1.2588401808797167,\n",
       " 1.2534489203967105,\n",
       " 1.238682068766721,\n",
       " 1.2407381342725623,\n",
       " 1.1980449361786456,\n",
       " 1.185213668669435,\n",
       " 1.1747244692063092,\n",
       " 1.1873761406321783,\n",
       " 1.20151862844892,\n",
       " 1.1783344067811878,\n",
       " 1.1339798041670797,\n",
       " 1.1611720023780996,\n",
       " 1.164779124967346,\n",
       " 1.1390450893138642,\n",
       " 1.1409924963742983,\n",
       " 1.1536295298748795,\n",
       " 1.1500848999666706,\n",
       " 1.1737268382982173,\n",
       " 1.1497161368488373,\n",
       " 1.121557272571681,\n",
       " 1.1457205462423317,\n",
       " 1.1193266779566358,\n",
       " 1.146330272130291,\n",
       " 1.1599773224757461,\n",
       " 1.171937633430319,\n",
       " 1.1751225081747183,\n",
       " 1.172115540522281,\n",
       " 1.160393377352202,\n",
       " 1.160588174357958,\n",
       " 1.202340604253556,\n",
       " 1.188306774882221,\n",
       " 1.1801444425427654,\n",
       " 1.1664850062605279,\n",
       " 1.1490928990298435,\n",
       " 1.150063506075865,\n",
       " 1.1552824894381737,\n",
       " 1.1335333474457947,\n",
       " 1.1432275949663553,\n",
       " 1.149211691423527,\n",
       " 1.1148598587552807,\n",
       " 1.117876397358868,\n",
       " 1.1262233927558034,\n",
       " 1.085069991802762,\n",
       " 1.0913671146622468,\n",
       " 1.064400678298938,\n",
       " 1.0212655274607476,\n",
       " 1.0691022673020276,\n",
       " 1.0802473584174828,\n",
       " 1.0780285867420933,\n",
       " 1.0327365939124244,\n",
       " 1.0916362272886957,\n",
       " 1.140801077351301,\n",
       " 1.1039686793438608,\n",
       " 1.0799219460783873,\n",
       " 1.070445015448641,\n",
       " 1.0880426166304846,\n",
       " 1.099174758812031,\n",
       " 1.1063991379387996,\n",
       " 1.1306412312071559,\n",
       " 1.1240828776809924,\n",
       " 1.1467699728860583,\n",
       " 1.1374332285408015,\n",
       " 1.1140879896949007,\n",
       " 1.108595389729131,\n",
       " 1.1305387657301398,\n",
       " 1.132189473304928,\n",
       " 1.129192076603641,\n",
       " 1.1311383576698222,\n",
       " 1.1476606343401223,\n",
       " 1.1529201534955367,\n",
       " 1.157108289119292,\n",
       " 1.1859906047039535,\n",
       " 1.1778237683874861,\n",
       " 1.1980742120292218,\n",
       " 1.1954191175808235,\n",
       " 1.194985046796321,\n",
       " 1.22955926332952,\n",
       " 1.2459604956176302,\n",
       " 1.2405163134047363,\n",
       " 1.2846569771107887,\n",
       " 1.271243007575689,\n",
       " 1.2754851909235851,\n",
       " 1.261565086971796,\n",
       " 1.2223686640303386,\n",
       " 1.2180352751479555,\n",
       " 1.1895836523650383,\n",
       " 1.1774521902840207,\n",
       " 1.4062829353318982,\n",
       " 1.4392616180087017,\n",
       " 1.4243489501229587,\n",
       " 1.4486878788970659,\n",
       " 1.4942698152468625,\n",
       " 1.5166393800726037,\n",
       " 1.451869938655833,\n",
       " 1.4488100492735083,\n",
       " 1.3890315773828288,\n",
       " 1.3685221550629203,\n",
       " 1.3395649608604399,\n",
       " 1.2889914919874248,\n",
       " 1.305132056605983,\n",
       " 1.3079915190112872,\n",
       " 1.2997531820597588,\n",
       " 1.2860183041625755,\n",
       " 1.2876971615936874,\n",
       " 1.2791852080386978,\n",
       " 1.264314201940313,\n",
       " 1.2309594371830328,\n",
       " 1.2438678352985688,\n",
       " 1.244460671272734,\n",
       " 1.2621325880752705,\n",
       " 1.2742606721735292,\n",
       " 1.26673396358985,\n",
       " 1.2874584508120672,\n",
       " 1.2903809688955348,\n",
       " 1.253191630709917,\n",
       " 1.2595580022159565,\n",
       " 1.2869883482114703,\n",
       " 1.2787438182915514]"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_inverse.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41064307],\n",
       "       [0.41071   ],\n",
       "       [0.41054648],\n",
       "       [0.4120195 ],\n",
       "       [0.4077616 ],\n",
       "       [0.40563706],\n",
       "       [0.4101848 ],\n",
       "       [0.41610965],\n",
       "       [0.41439262],\n",
       "       [0.4079402 ],\n",
       "       [0.40789166],\n",
       "       [0.40882316],\n",
       "       [0.4137763 ],\n",
       "       [0.41385075],\n",
       "       [0.41248858],\n",
       "       [0.4118966 ],\n",
       "       [0.40847337],\n",
       "       [0.39506638],\n",
       "       [0.39874795],\n",
       "       [0.4012557 ],\n",
       "       [0.39736018],\n",
       "       [0.40014467],\n",
       "       [0.39561188],\n",
       "       [0.38865435],\n",
       "       [0.38273656],\n",
       "       [0.3722981 ],\n",
       "       [0.3787802 ],\n",
       "       [0.38250872],\n",
       "       [0.38676453],\n",
       "       [0.3841249 ],\n",
       "       [0.37313148],\n",
       "       [0.37391818],\n",
       "       [0.3642933 ],\n",
       "       [0.35388842],\n",
       "       [0.35559326],\n",
       "       [0.34911725],\n",
       "       [0.3273221 ],\n",
       "       [0.34357464],\n",
       "       [0.35475928],\n",
       "       [0.35790998],\n",
       "       [0.35390988],\n",
       "       [0.34108543],\n",
       "       [0.34153304],\n",
       "       [0.3248486 ],\n",
       "       [0.3314326 ],\n",
       "       [0.31332725],\n",
       "       [0.3146956 ],\n",
       "       [0.3267614 ],\n",
       "       [0.32442212],\n",
       "       [0.32068732],\n",
       "       [0.33280623],\n",
       "       [0.33991736],\n",
       "       [0.34418643],\n",
       "       [0.35536608],\n",
       "       [0.34958336],\n",
       "       [0.34241676],\n",
       "       [0.34384552],\n",
       "       [0.34997076],\n",
       "       [0.3533289 ],\n",
       "       [0.35936403],\n",
       "       [0.36105025],\n",
       "       [0.36549416],\n",
       "       [0.35847846],\n",
       "       [0.3598355 ],\n",
       "       [0.35083827],\n",
       "       [0.35416672],\n",
       "       [0.3502341 ],\n",
       "       [0.33180442],\n",
       "       [0.33312112],\n",
       "       [0.3376574 ],\n",
       "       [0.3501552 ],\n",
       "       [0.34999377],\n",
       "       [0.35013303],\n",
       "       [0.34669927],\n",
       "       [0.34265432],\n",
       "       [0.34691143],\n",
       "       [0.33657458],\n",
       "       [0.34003076],\n",
       "       [0.3227723 ],\n",
       "       [0.32997224],\n",
       "       [0.328765  ],\n",
       "       [0.33709005],\n",
       "       [0.3304601 ],\n",
       "       [0.3318363 ],\n",
       "       [0.33691773],\n",
       "       [0.34994596],\n",
       "       [0.3450871 ],\n",
       "       [0.3431826 ],\n",
       "       [0.34922338],\n",
       "       [0.34345704],\n",
       "       [0.34209606],\n",
       "       [0.34569654],\n",
       "       [0.34198833],\n",
       "       [0.34622887],\n",
       "       [0.34752825],\n",
       "       [0.3479217 ],\n",
       "       [0.35801086],\n",
       "       [0.3607735 ],\n",
       "       [0.36327305],\n",
       "       [0.3743436 ],\n",
       "       [0.37320703],\n",
       "       [0.37541357],\n",
       "       [0.37971252],\n",
       "       [0.3762144 ],\n",
       "       [0.37579924],\n",
       "       [0.37220398],\n",
       "       [0.37663668],\n",
       "       [0.37718576],\n",
       "       [0.38036272],\n",
       "       [0.3846837 ],\n",
       "       [0.38626313],\n",
       "       [0.38109455],\n",
       "       [0.38703927],\n",
       "       [0.38600984],\n",
       "       [0.38493595],\n",
       "       [0.39264843],\n",
       "       [0.39894035],\n",
       "       [0.3978065 ],\n",
       "       [0.39501145],\n",
       "       [0.3967326 ],\n",
       "       [0.39576134],\n",
       "       [0.4015147 ],\n",
       "       [0.39602172],\n",
       "       [0.39014682],\n",
       "       [0.39199445],\n",
       "       [0.39421946],\n",
       "       [0.39709613],\n",
       "       [0.39395127],\n",
       "       [0.3950481 ],\n",
       "       [0.38924375],\n",
       "       [0.3862351 ],\n",
       "       [0.38004425],\n",
       "       [0.38195926],\n",
       "       [0.3863942 ],\n",
       "       [0.3845609 ],\n",
       "       [0.38888502],\n",
       "       [0.38646844],\n",
       "       [0.3826163 ],\n",
       "       [0.3794662 ],\n",
       "       [0.37828413],\n",
       "       [0.37023255],\n",
       "       [0.35602558],\n",
       "       [0.36252913],\n",
       "       [0.36045474],\n",
       "       [0.35456288],\n",
       "       [0.3517408 ],\n",
       "       [0.35652187],\n",
       "       [0.36348012],\n",
       "       [0.367096  ],\n",
       "       [0.36913872],\n",
       "       [0.37321344],\n",
       "       [0.3715882 ],\n",
       "       [0.3628662 ],\n",
       "       [0.36640894],\n",
       "       [0.36665496],\n",
       "       [0.3592917 ],\n",
       "       [0.3620028 ],\n",
       "       [0.3670465 ],\n",
       "       [0.3763049 ],\n",
       "       [0.3760326 ],\n",
       "       [0.37585464],\n",
       "       [0.37846765],\n",
       "       [0.3754192 ],\n",
       "       [0.37312654],\n",
       "       [0.37380132],\n",
       "       [0.3765488 ],\n",
       "       [0.37380818],\n",
       "       [0.37054715],\n",
       "       [0.36643463],\n",
       "       [0.36504498],\n",
       "       [0.36864564],\n",
       "       [0.36027542],\n",
       "       [0.3623499 ],\n",
       "       [0.36100224],\n",
       "       [0.36497393],\n",
       "       [0.36824247],\n",
       "       [0.36521047],\n",
       "       [0.36292195],\n",
       "       [0.35931677],\n",
       "       [0.35615405],\n",
       "       [0.3544213 ],\n",
       "       [0.3474833 ],\n",
       "       [0.3469067 ],\n",
       "       [0.34101442],\n",
       "       [0.34846607],\n",
       "       [0.35325578],\n",
       "       [0.35145056],\n",
       "       [0.34852087],\n",
       "       [0.3477952 ],\n",
       "       [0.35784495],\n",
       "       [0.3598157 ],\n",
       "       [0.35882545],\n",
       "       [0.35738352],\n",
       "       [0.35696378],\n",
       "       [0.36152783],\n",
       "       [0.36647704],\n",
       "       [0.36273366],\n",
       "       [0.35902467],\n",
       "       [0.36042297],\n",
       "       [0.36206633],\n",
       "       [0.35818535],\n",
       "       [0.3510142 ],\n",
       "       [0.35141593],\n",
       "       [0.35396925],\n",
       "       [0.35206422],\n",
       "       [0.36185482],\n",
       "       [0.3630944 ],\n",
       "       [0.36645296],\n",
       "       [0.36299583],\n",
       "       [0.36527184],\n",
       "       [0.3715999 ],\n",
       "       [0.36915284],\n",
       "       [0.3654237 ],\n",
       "       [0.3654697 ],\n",
       "       [0.36313862],\n",
       "       [0.35759583],\n",
       "       [0.3585978 ],\n",
       "       [0.3605953 ],\n",
       "       [0.36204085],\n",
       "       [0.36318815],\n",
       "       [0.36584625],\n",
       "       [0.36736193],\n",
       "       [0.36825538],\n",
       "       [0.3707551 ],\n",
       "       [0.36991715],\n",
       "       [0.3663466 ],\n",
       "       [0.36452264],\n",
       "       [0.36403856],\n",
       "       [0.36798337],\n",
       "       [0.3737894 ],\n",
       "       [0.37386128],\n",
       "       [0.37244162],\n",
       "       [0.37270698],\n",
       "       [0.37417075],\n",
       "       [0.36947128],\n",
       "       [0.37298587],\n",
       "       [0.37166855],\n",
       "       [0.37505278],\n",
       "       [0.37551233],\n",
       "       [0.37338296],\n",
       "       [0.36594793],\n",
       "       [0.36394116],\n",
       "       [0.3657529 ],\n",
       "       [0.36599106],\n",
       "       [0.3679633 ],\n",
       "       [0.36001787],\n",
       "       [0.3645582 ],\n",
       "       [0.36228353],\n",
       "       [0.361615  ],\n",
       "       [0.3505811 ],\n",
       "       [0.35306224],\n",
       "       [0.35523888],\n",
       "       [0.35289842],\n",
       "       [0.3575224 ],\n",
       "       [0.3553532 ],\n",
       "       [0.34803   ],\n",
       "       [0.34723964],\n",
       "       [0.34375137],\n",
       "       [0.3470572 ],\n",
       "       [0.349667  ],\n",
       "       [0.34882826],\n",
       "       [0.34174162],\n",
       "       [0.34718058],\n",
       "       [0.34854883],\n",
       "       [0.35261866],\n",
       "       [0.35213628],\n",
       "       [0.3497741 ],\n",
       "       [0.3490867 ],\n",
       "       [0.35003698],\n",
       "       [0.35542306],\n",
       "       [0.35343912],\n",
       "       [0.3478867 ],\n",
       "       [0.3458062 ],\n",
       "       [0.34580874],\n",
       "       [0.34191105],\n",
       "       [0.34554496],\n",
       "       [0.34659058],\n",
       "       [0.34600994],\n",
       "       [0.3312718 ],\n",
       "       [0.32757136],\n",
       "       [0.32679728],\n",
       "       [0.31969413],\n",
       "       [0.32319453],\n",
       "       [0.3213001 ],\n",
       "       [0.30872983],\n",
       "       [0.3112949 ],\n",
       "       [0.3076787 ],\n",
       "       [0.31689894],\n",
       "       [0.31534803],\n",
       "       [0.32323408],\n",
       "       [0.3166835 ],\n",
       "       [0.30967653],\n",
       "       [0.3060042 ],\n",
       "       [0.3055957 ],\n",
       "       [0.29076353],\n",
       "       [0.29742718],\n",
       "       [0.30440578],\n",
       "       [0.3069843 ],\n",
       "       [0.3065926 ],\n",
       "       [0.31607324],\n",
       "       [0.31315377],\n",
       "       [0.305181  ],\n",
       "       [0.3128619 ],\n",
       "       [0.31179255],\n",
       "       [0.31149486],\n",
       "       [0.3152435 ],\n",
       "       [0.31349185],\n",
       "       [0.32009143],\n",
       "       [0.32407346],\n",
       "       [0.32333758],\n",
       "       [0.3224891 ],\n",
       "       [0.32192862],\n",
       "       [0.3321526 ],\n",
       "       [0.33559343],\n",
       "       [0.33460572],\n",
       "       [0.3322277 ],\n",
       "       [0.327342  ],\n",
       "       [0.32880569],\n",
       "       [0.325861  ],\n",
       "       [0.32590508],\n",
       "       [0.32108846],\n",
       "       [0.3224715 ],\n",
       "       [0.32457098],\n",
       "       [0.3328617 ],\n",
       "       [0.32734865],\n",
       "       [0.32847095],\n",
       "       [0.32859257],\n",
       "       [0.32352772],\n",
       "       [0.32205042],\n",
       "       [0.31472558],\n",
       "       [0.31451157],\n",
       "       [0.31707147],\n",
       "       [0.31531107],\n",
       "       [0.31691653],\n",
       "       [0.31976223],\n",
       "       [0.320873  ],\n",
       "       [0.32034424],\n",
       "       [0.32223547],\n",
       "       [0.3203906 ],\n",
       "       [0.32196245],\n",
       "       [0.3189071 ],\n",
       "       [0.31916803],\n",
       "       [0.31693694],\n",
       "       [0.31714737],\n",
       "       [0.31932986],\n",
       "       [0.3233177 ],\n",
       "       [0.32707274],\n",
       "       [0.32741088],\n",
       "       [0.3261953 ],\n",
       "       [0.3266758 ],\n",
       "       [0.3251101 ],\n",
       "       [0.32544538],\n",
       "       [0.3257952 ],\n",
       "       [0.32726443],\n",
       "       [0.32247356],\n",
       "       [0.3209627 ],\n",
       "       [0.31919378],\n",
       "       [0.31801194],\n",
       "       [0.3158683 ],\n",
       "       [0.31712818],\n",
       "       [0.31658685],\n",
       "       [0.3109692 ],\n",
       "       [0.31493163],\n",
       "       [0.31738   ],\n",
       "       [0.31978193],\n",
       "       [0.32106972],\n",
       "       [0.3246772 ],\n",
       "       [0.32746157],\n",
       "       [0.32660523],\n",
       "       [0.33486035],\n",
       "       [0.33299363],\n",
       "       [0.33249104],\n",
       "       [0.3339515 ],\n",
       "       [0.33274052],\n",
       "       [0.33363438],\n",
       "       [0.32873124],\n",
       "       [0.33020937],\n",
       "       [0.3308138 ],\n",
       "       [0.3416261 ],\n",
       "       [0.3454566 ],\n",
       "       [0.34434882],\n",
       "       [0.344189  ],\n",
       "       [0.3383022 ],\n",
       "       [0.3369433 ],\n",
       "       [0.33592016],\n",
       "       [0.33311716],\n",
       "       [0.3335075 ],\n",
       "       [0.32539862],\n",
       "       [0.32296008],\n",
       "       [0.3209661 ],\n",
       "       [0.32337108],\n",
       "       [0.3260587 ],\n",
       "       [0.3216524 ],\n",
       "       [0.31321698],\n",
       "       [0.3183893 ],\n",
       "       [0.31907517],\n",
       "       [0.3141806 ],\n",
       "       [0.3145511 ],\n",
       "       [0.31695485],\n",
       "       [0.31628063],\n",
       "       [0.32077646],\n",
       "       [0.31621054],\n",
       "       [0.31085318],\n",
       "       [0.31545055],\n",
       "       [0.3104287 ],\n",
       "       [0.3155665 ],\n",
       "       [0.31816205],\n",
       "       [0.3204363 ],\n",
       "       [0.32104182],\n",
       "       [0.32047012],\n",
       "       [0.31824118],\n",
       "       [0.31827822],\n",
       "       [0.32621482],\n",
       "       [0.323548  ],\n",
       "       [0.32199645],\n",
       "       [0.31939954],\n",
       "       [0.31609198],\n",
       "       [0.3162766 ],\n",
       "       [0.3172692 ],\n",
       "       [0.31313202],\n",
       "       [0.3149763 ],\n",
       "       [0.31611454],\n",
       "       [0.3095786 ],\n",
       "       [0.3101527 ],\n",
       "       [0.3117411 ],\n",
       "       [0.30390766],\n",
       "       [0.30510664],\n",
       "       [0.29997146],\n",
       "       [0.29175356],\n",
       "       [0.30086693],\n",
       "       [0.30298936],\n",
       "       [0.30256686],\n",
       "       [0.2939394 ],\n",
       "       [0.30515784],\n",
       "       [0.3145147 ],\n",
       "       [0.30750564],\n",
       "       [0.30292737],\n",
       "       [0.30112264],\n",
       "       [0.30447364],\n",
       "       [0.30659303],\n",
       "       [0.30796823],\n",
       "       [0.31258172],\n",
       "       [0.3113338 ],\n",
       "       [0.31565017],\n",
       "       [0.313874  ],\n",
       "       [0.3094317 ],\n",
       "       [0.30838633],\n",
       "       [0.31256226],\n",
       "       [0.31287634],\n",
       "       [0.31230602],\n",
       "       [0.31267637],\n",
       "       [0.31581956],\n",
       "       [0.31681994],\n",
       "       [0.31761646],\n",
       "       [0.32310772],\n",
       "       [0.32155532],\n",
       "       [0.3254042 ],\n",
       "       [0.32489967],\n",
       "       [0.32481718],\n",
       "       [0.331385  ],\n",
       "       [0.33449888],\n",
       "       [0.33346543],\n",
       "       [0.3418406 ],\n",
       "       [0.33929643],\n",
       "       [0.34010112],\n",
       "       [0.3374603 ],\n",
       "       [0.33001953],\n",
       "       [0.32919642],\n",
       "       [0.32379064],\n",
       "       [0.3214847 ],\n",
       "       [0.36486468],\n",
       "       [0.37109232],\n",
       "       [0.36827713],\n",
       "       [0.3728711 ],\n",
       "       [0.3814638 ],\n",
       "       [0.38567525],\n",
       "       [0.37347135],\n",
       "       [0.3728941 ],\n",
       "       [0.36160412],\n",
       "       [0.3577255 ],\n",
       "       [0.35224512],\n",
       "       [0.34266254],\n",
       "       [0.34572226],\n",
       "       [0.34626424],\n",
       "       [0.34470278],\n",
       "       [0.3420987 ],\n",
       "       [0.34241706],\n",
       "       [0.3408029 ],\n",
       "       [0.33798194],\n",
       "       [0.3316509 ],\n",
       "       [0.33410165],\n",
       "       [0.33421418],\n",
       "       [0.337568  ],\n",
       "       [0.3398688 ],\n",
       "       [0.33844104],\n",
       "       [0.34237185],\n",
       "       [0.342926  ],\n",
       "       [0.3358714 ],\n",
       "       [0.33707947],\n",
       "       [0.34228268]], dtype=float32)"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_inverse.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [718, 500]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-655-339fed75d7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY_inverse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_inverse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;36m0.825\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \"\"\"\n\u001b[0;32m--> 335\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    336\u001b[0m         y_true, y_pred, multioutput)\n\u001b[1;32m    337\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0margument\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [718, 500]"
     ]
    }
   ],
   "source": [
    "mean_squared_error(testY_inverse.tolist(), yhat_inverse.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96674601, 0.98466621, 0.98644866, 0.99985756, 0.99000849,\n",
       "       0.99752788, 0.98877384, 0.99308077, 1.01654874, 1.00370452,\n",
       "       1.00954956, 1.0056767 , 1.00313195, 1.00246818, 0.99879856,\n",
       "       0.99359253, 1.00508949, 1.00623182, 1.01143841, 1.00728575,\n",
       "       1.02026001, 1.01236116, 1.00440207, 1.01096155, 0.99833184,\n",
       "       0.99763203, 0.99616261, 1.00721199, 1.00396237, 1.00937784,\n",
       "       1.01222492, 1.0044854 , 1.01488396, 1.0125965 , 1.01395726,\n",
       "       1.01437501, 1.02736616, 1.03422178, 1.03686956, 1.04043783,\n",
       "       1.02839926, 1.03181947, 1.03617257, 1.08313216, 1.13137988,\n",
       "       1.12175826, 1.14619177, 1.18847061, 1.21089422, 1.21563015,\n",
       "       1.21982955, 1.08993035, 1.14312119, 1.16890702, 1.17069172,\n",
       "       1.18664424, 1.19338613, 1.20218409, 1.19397784, 1.22976645,\n",
       "       1.25731221, 1.27111746, 1.29947957, 1.29334177, 1.28391945,\n",
       "       1.26610678, 1.29675522, 1.27872693, 1.26727275, 1.26609778,\n",
       "       1.24898998, 1.19633399, 1.21027549, 1.20328475, 1.20267277,\n",
       "       1.25678862, 1.24854465, 1.223882  , 1.21910047, 1.20827234,\n",
       "       1.22775824, 1.24366966, 1.24736743, 1.23381214, 1.20628215,\n",
       "       1.23540486, 1.16557914, 1.16251531, 1.10352279, 1.06255292,\n",
       "       1.08980311, 1.11757013, 1.12583661, 1.16744998, 1.14580781,\n",
       "       1.12012444, 1.10244296, 1.11295299, 1.14102008, 1.11816015,\n",
       "       1.10484132, 1.12288144, 1.1609468 , 1.18324374, 1.20419005,\n",
       "       1.1788974 , 1.11476753, 1.09721497, 1.07092694, 1.11067623,\n",
       "       1.09325034, 1.11443592, 1.13540644, 1.12284203, 1.1484387 ,\n",
       "       1.1385778 , 1.13111922, 1.17992881, 1.16678565, 1.18950596,\n",
       "       1.20491181, 1.24211691, 1.26324789, 1.27672322, 1.26031073,\n",
       "       1.28882203, 1.27398199, 1.27001736, 1.26426241, 1.26833006,\n",
       "       1.28325568, 1.35289898, 1.33226852, 1.34004463, 1.31303316,\n",
       "       1.3072889 , 1.27027015, 1.30076151, 1.33012969, 1.32898512,\n",
       "       1.30761318, 1.2895072 , 1.3091451 , 1.31037018, 1.32717846,\n",
       "       1.34636146, 1.32297963, 1.38989521, 1.38035692, 1.38856204,\n",
       "       1.43227257, 1.45350094, 1.43828651, 1.44342836, 1.47217893,\n",
       "       1.47127363, 1.46724821, 1.45455431, 1.43473963, 1.45388491,\n",
       "       1.47020788, 1.44483247, 1.49014924, 1.49898154, 1.5106215 ,\n",
       "       1.52512375, 1.55300505, 1.56210252, 1.59679328, 1.53634372,\n",
       "       1.55814071, 1.56130251, 1.59436057, 1.59271718, 1.58031548,\n",
       "       1.5441508 , 1.53615286, 1.45324027, 1.51861944, 1.537517  ,\n",
       "       1.49563114, 1.52794324, 1.55113309, 1.53354956, 1.57385509,\n",
       "       1.65228622, 1.68339125, 1.68416086, 1.68548334, 1.69355841,\n",
       "       1.65843358, 1.68706424, 1.6681222 , 1.60464033, 1.62990596,\n",
       "       1.55637853, 1.61163163, 1.64987321, 1.64544468, 1.62688323,\n",
       "       1.62106015, 1.60876936, 1.62383685, 1.62330876, 1.57151077,\n",
       "       1.55290878, 1.58983576, 1.62106859, 1.62892015, 1.63277499,\n",
       "       1.6434922 , 1.65909229, 1.67183517, 1.65952017, 1.6450213 ,\n",
       "       1.61458118, 1.63785379, 1.64968686, 1.65004493, 1.64917059,\n",
       "       1.65704467, 1.63429227, 1.62294844, 1.64723782, 1.67892386,\n",
       "       1.66973631, 1.63524598, 1.63498701, 1.63996277, 1.6664394 ,\n",
       "       1.66683744, 1.65955282, 1.65638765, 1.63809419, 1.56659299,\n",
       "       1.58620499, 1.59957336, 1.57881003, 1.59364951, 1.56949805,\n",
       "       1.5324748 , 1.50102803, 1.44565107, 1.48002542, 1.49981815,\n",
       "       1.52242812, 1.50840217, 1.45006835, 1.45423847, 1.40325908,\n",
       "       1.34824525, 1.35725264, 1.32304944, 1.20816819, 1.2938023 ,\n",
       "       1.35284606, 1.36949727, 1.34835841, 1.2806749 , 1.28303498,\n",
       "       1.19515057, 1.2298098 , 1.13455969, 1.14175198, 1.20521696,\n",
       "       1.19290646, 1.17325786, 1.23704431, 1.27451627, 1.2970294 ,\n",
       "       1.35605233, 1.32550974, 1.28769547, 1.29523119, 1.32755511,\n",
       "       1.34528952, 1.37718443, 1.38610174, 1.40961475, 1.37250254,\n",
       "       1.37967738, 1.33213565, 1.34971524, 1.32894571, 1.2317679 ,\n",
       "       1.2387029 , 1.26260382, 1.32852909, 1.32767671, 1.32841199,\n",
       "       1.31028742, 1.28894814, 1.31140666, 1.25689728, 1.27511418,\n",
       "       1.18422561, 1.22211982, 1.21576414, 1.25961374, 1.22468878,\n",
       "       1.23193567, 1.25870562, 1.32742449, 1.3017811 , 1.29173441,\n",
       "       1.32360962, 1.29318188, 1.28600423, 1.30499637, 1.28543617,\n",
       "       1.30780517, 1.31466191, 1.31673881, 1.37003042, 1.38463795,\n",
       "       1.3978605 , 1.45649383, 1.45046864, 1.46216716, 1.48497361,\n",
       "       1.46641441, 1.46421252, 1.44515226, 1.46865401, 1.47156639,\n",
       "       1.48842478, 1.51137085, 1.51976345, 1.49230946, 1.52388853,\n",
       "       1.51841732, 1.51271079, 1.55372175, 1.58723021, 1.58118813,\n",
       "       1.56630079, 1.57546695, 1.57029413, 1.60095439, 1.57168079,\n",
       "       1.54041193, 1.5502413 , 1.56208395, 1.57740366, 1.56065618,\n",
       "       1.56649559, 1.53560901, 1.51961426, 1.4867341 , 1.4969007 ,\n",
       "       1.52045988, 1.51071834, 1.53370157, 1.52085454, 1.50038959,\n",
       "       1.48366633, 1.47739341, 1.43470641, 1.35953728, 1.39392459,\n",
       "       1.38295233, 1.35180846, 1.33690142, 1.36216029, 1.39895609,\n",
       "       1.41809462, 1.42891204, 1.45050242, 1.44188913, 1.3957076 ,\n",
       "       1.41445709, 1.41575987, 1.37680215, 1.39114   , 1.41783282,\n",
       "       1.46689408, 1.46544999, 1.46450641, 1.4783674 , 1.46219699,\n",
       "       1.45004189, 1.45361861, 1.46818785, 1.4536552 , 1.43637288,\n",
       "       1.41459334, 1.40723722, 1.42630086, 1.38200368, 1.3929765 ,\n",
       "       1.38584783, 1.40686113, 1.42416541, 1.40811324, 1.39600261,\n",
       "       1.37693446, 1.36021626, 1.35106024, 1.31442489, 1.31138189,\n",
       "       1.28030051, 1.31961178, 1.3449033 , 1.33536894, 1.31990116,\n",
       "       1.31607109, 1.36915327, 1.37957266, 1.37433679, 1.36671437,\n",
       "       1.3644956 , 1.38862791, 1.41481797, 1.39500667, 1.37539016,\n",
       "       1.382784  , 1.39147611, 1.37095261, 1.3330646 , 1.33518597,\n",
       "       1.348672  , 1.33860955, 1.39035744, 1.39691523, 1.41469017,\n",
       "       1.39639389, 1.40843809, 1.44195106, 1.42898692, 1.40924205,\n",
       "       1.40948526, 1.39714943, 1.36783642, 1.3731331 , 1.38369549,\n",
       "       1.39134155, 1.39741123, 1.41147827, 1.41950267, 1.4242341 ,\n",
       "       1.43747467, 1.43303543, 1.41412717, 1.4044729 , 1.40191126,\n",
       "       1.42279339, 1.45355555, 1.4539367 , 1.44641168, 1.44781805,\n",
       "       1.45557727, 1.43067366, 1.44929648, 1.44231475, 1.46025409,\n",
       "       1.46269074, 1.45140096, 1.4120165 , 1.40139556, 1.41098452,\n",
       "       1.41224507, 1.42268698, 1.38064179, 1.40466094, 1.39262519,\n",
       "       1.389089  , 1.3307777 , 1.3438809 , 1.35538011, 1.34301557,\n",
       "       1.36744852, 1.35598421, 1.31731025, 1.313139  , 1.29473463,\n",
       "       1.31217628, 1.32595169, 1.32152372, 1.28413508, 1.3128271 ,\n",
       "       1.32004867, 1.3415377 , 1.33899014, 1.32651694, 1.32288786,\n",
       "       1.32790473, 1.35635354, 1.34587165, 1.31655414, 1.30557514,\n",
       "       1.30558808, 1.28502856, 1.30419636, 1.30971373, 1.3066499 ,\n",
       "       1.22896305, 1.20948053, 1.20540612, 1.16803437, 1.1864472 ,\n",
       "       1.17648102, 1.11040036, 1.12387851, 1.10487792, 1.15333565,\n",
       "       1.14518176, 1.1866555 , 1.1522029 , 1.11537444, 1.09608165,\n",
       "       1.09393607, 1.01607019, 1.05104357, 1.08768624, 1.10123026,\n",
       "       1.09917251, 1.14899437, 1.13364764, 1.09175783, 1.13211347,\n",
       "       1.12649363, 1.12492906, 1.14463227, 1.13542445, 1.17012366,\n",
       "       1.19107165, 1.18719992, 1.18273592, 1.1797875 , 1.23360158,\n",
       "       1.25172728, 1.24652349, 1.23399737, 1.2082729 , 1.21597808,\n",
       "       1.20047821, 1.20071016, 1.17536797, 1.18264359, 1.19368959,\n",
       "       1.23733651, 1.20830781, 1.2142159 , 1.21485603, 1.18820037,\n",
       "       1.18042819, 1.14190962, 1.14078475, 1.15424263, 1.14498752,\n",
       "       1.15342798, 1.16839244, 1.17423466, 1.17145346, 1.18140162,\n",
       "       1.17169723, 1.17996541, 1.16389522, 1.16526724, 1.15353551,\n",
       "       1.1546418 , 1.1661185 , 1.18709521, 1.20685584, 1.20863548,\n",
       "       1.20223758, 1.20476656, 1.19652653, 1.19829097, 1.20013197,\n",
       "       1.20786473, 1.18265428, 1.17470645, 1.16540293, 1.159188  ,\n",
       "       1.1479168 , 1.15454102, 1.15169451, 1.122167  , 1.14299283,\n",
       "       1.15586519, 1.16849603, 1.17526945, 1.19424865, 1.20890234,\n",
       "       1.20439498, 1.24786511, 1.23803124, 1.23538403, 1.24307682,\n",
       "       1.23669807, 1.24140641, 1.21558624, 1.22336855, 1.22655117,\n",
       "       1.28352592, 1.30373019, 1.29788628, 1.29704291, 1.26600207,\n",
       "       1.25884018, 1.25344892, 1.23868207, 1.24073813, 1.19804494,\n",
       "       1.18521367, 1.17472447, 1.18737614, 1.20151863, 1.17833441,\n",
       "       1.1339798 , 1.161172  , 1.16477912, 1.13904509, 1.1409925 ,\n",
       "       1.15362953, 1.1500849 , 1.17372684, 1.14971614, 1.12155727,\n",
       "       1.14572055, 1.11932668, 1.14633027, 1.15997732, 1.17193763,\n",
       "       1.17512251, 1.17211554, 1.16039338, 1.16058817, 1.2023406 ,\n",
       "       1.18830677, 1.18014444, 1.16648501, 1.1490929 , 1.15006351,\n",
       "       1.15528249, 1.13353335, 1.14322759, 1.14921169, 1.11485986,\n",
       "       1.1178764 , 1.12622339, 1.08506999, 1.09136711, 1.06440068,\n",
       "       1.02126553, 1.06910227, 1.08024736, 1.07802859, 1.03273659,\n",
       "       1.09163623, 1.14080108, 1.10396868, 1.07992195, 1.07044502,\n",
       "       1.08804262, 1.09917476, 1.10639914, 1.13064123, 1.12408288,\n",
       "       1.14676997, 1.13743323, 1.11408799, 1.10859539, 1.13053877,\n",
       "       1.13218947, 1.12919208, 1.13113836, 1.14766063, 1.15292015,\n",
       "       1.15710829, 1.1859906 , 1.17782377, 1.19807421, 1.19541912,\n",
       "       1.19498505, 1.22955926, 1.2459605 , 1.24051631, 1.28465698,\n",
       "       1.27124301, 1.27548519, 1.26156509, 1.22236866, 1.21803528,\n",
       "       1.18958365, 1.17745219, 1.40628294, 1.43926162, 1.42434895,\n",
       "       1.44868788, 1.49426982, 1.51663938, 1.45186994, 1.44881005,\n",
       "       1.38903158, 1.36852216, 1.33956496, 1.28899149, 1.30513206,\n",
       "       1.30799152, 1.29975318, 1.2860183 , 1.28769716, 1.27918521,\n",
       "       1.2643142 , 1.23095944, 1.24386784, 1.24446067, 1.26213259,\n",
       "       1.27426067, 1.26673396, 1.28745845, 1.29038097, 1.25319163,\n",
       "       1.259558  , 1.28698835, 1.27874382])"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41064307, 0.41071   , 0.41054648, 0.4120195 , 0.4077616 ,\n",
       "       0.40563706, 0.4101848 , 0.41610965, 0.41439262, 0.4079402 ,\n",
       "       0.40789166, 0.40882316, 0.4137763 , 0.41385075, 0.41248858,\n",
       "       0.4118966 , 0.40847337, 0.39506638, 0.39874795, 0.4012557 ,\n",
       "       0.39736018, 0.40014467, 0.39561188, 0.38865435, 0.38273656,\n",
       "       0.3722981 , 0.3787802 , 0.38250872, 0.38676453, 0.3841249 ,\n",
       "       0.37313148, 0.37391818, 0.3642933 , 0.35388842, 0.35559326,\n",
       "       0.34911725, 0.3273221 , 0.34357464, 0.35475928, 0.35790998,\n",
       "       0.35390988, 0.34108543, 0.34153304, 0.3248486 , 0.3314326 ,\n",
       "       0.31332725, 0.3146956 , 0.3267614 , 0.32442212, 0.32068732,\n",
       "       0.33280623, 0.33991736, 0.34418643, 0.35536608, 0.34958336,\n",
       "       0.34241676, 0.34384552, 0.34997076, 0.3533289 , 0.35936403,\n",
       "       0.36105025, 0.36549416, 0.35847846, 0.3598355 , 0.35083827,\n",
       "       0.35416672, 0.3502341 , 0.33180442, 0.33312112, 0.3376574 ,\n",
       "       0.3501552 , 0.34999377, 0.35013303, 0.34669927, 0.34265432,\n",
       "       0.34691143, 0.33657458, 0.34003076, 0.3227723 , 0.32997224,\n",
       "       0.328765  , 0.33709005, 0.3304601 , 0.3318363 , 0.33691773,\n",
       "       0.34994596, 0.3450871 , 0.3431826 , 0.34922338, 0.34345704,\n",
       "       0.34209606, 0.34569654, 0.34198833, 0.34622887, 0.34752825,\n",
       "       0.3479217 , 0.35801086, 0.3607735 , 0.36327305, 0.3743436 ,\n",
       "       0.37320703, 0.37541357, 0.37971252, 0.3762144 , 0.37579924,\n",
       "       0.37220398, 0.37663668, 0.37718576, 0.38036272, 0.3846837 ,\n",
       "       0.38626313, 0.38109455, 0.38703927, 0.38600984, 0.38493595,\n",
       "       0.39264843, 0.39894035, 0.3978065 , 0.39501145, 0.3967326 ,\n",
       "       0.39576134, 0.4015147 , 0.39602172, 0.39014682, 0.39199445,\n",
       "       0.39421946, 0.39709613, 0.39395127, 0.3950481 , 0.38924375,\n",
       "       0.3862351 , 0.38004425, 0.38195926, 0.3863942 , 0.3845609 ,\n",
       "       0.38888502, 0.38646844, 0.3826163 , 0.3794662 , 0.37828413,\n",
       "       0.37023255, 0.35602558, 0.36252913, 0.36045474, 0.35456288,\n",
       "       0.3517408 , 0.35652187, 0.36348012, 0.367096  , 0.36913872,\n",
       "       0.37321344, 0.3715882 , 0.3628662 , 0.36640894, 0.36665496,\n",
       "       0.3592917 , 0.3620028 , 0.3670465 , 0.3763049 , 0.3760326 ,\n",
       "       0.37585464, 0.37846765, 0.3754192 , 0.37312654, 0.37380132,\n",
       "       0.3765488 , 0.37380818, 0.37054715, 0.36643463, 0.36504498,\n",
       "       0.36864564, 0.36027542, 0.3623499 , 0.36100224, 0.36497393,\n",
       "       0.36824247, 0.36521047, 0.36292195, 0.35931677, 0.35615405,\n",
       "       0.3544213 , 0.3474833 , 0.3469067 , 0.34101442, 0.34846607,\n",
       "       0.35325578, 0.35145056, 0.34852087, 0.3477952 , 0.35784495,\n",
       "       0.3598157 , 0.35882545, 0.35738352, 0.35696378, 0.36152783,\n",
       "       0.36647704, 0.36273366, 0.35902467, 0.36042297, 0.36206633,\n",
       "       0.35818535, 0.3510142 , 0.35141593, 0.35396925, 0.35206422,\n",
       "       0.36185482, 0.3630944 , 0.36645296, 0.36299583, 0.36527184,\n",
       "       0.3715999 , 0.36915284, 0.3654237 , 0.3654697 , 0.36313862,\n",
       "       0.35759583, 0.3585978 , 0.3605953 , 0.36204085, 0.36318815,\n",
       "       0.36584625, 0.36736193, 0.36825538, 0.3707551 , 0.36991715,\n",
       "       0.3663466 , 0.36452264, 0.36403856, 0.36798337, 0.3737894 ,\n",
       "       0.37386128, 0.37244162, 0.37270698, 0.37417075, 0.36947128,\n",
       "       0.37298587, 0.37166855, 0.37505278, 0.37551233, 0.37338296,\n",
       "       0.36594793, 0.36394116, 0.3657529 , 0.36599106, 0.3679633 ,\n",
       "       0.36001787, 0.3645582 , 0.36228353, 0.361615  , 0.3505811 ,\n",
       "       0.35306224, 0.35523888, 0.35289842, 0.3575224 , 0.3553532 ,\n",
       "       0.34803   , 0.34723964, 0.34375137, 0.3470572 , 0.349667  ,\n",
       "       0.34882826, 0.34174162, 0.34718058, 0.34854883, 0.35261866,\n",
       "       0.35213628, 0.3497741 , 0.3490867 , 0.35003698, 0.35542306,\n",
       "       0.35343912, 0.3478867 , 0.3458062 , 0.34580874, 0.34191105,\n",
       "       0.34554496, 0.34659058, 0.34600994, 0.3312718 , 0.32757136,\n",
       "       0.32679728, 0.31969413, 0.32319453, 0.3213001 , 0.30872983,\n",
       "       0.3112949 , 0.3076787 , 0.31689894, 0.31534803, 0.32323408,\n",
       "       0.3166835 , 0.30967653, 0.3060042 , 0.3055957 , 0.29076353,\n",
       "       0.29742718, 0.30440578, 0.3069843 , 0.3065926 , 0.31607324,\n",
       "       0.31315377, 0.305181  , 0.3128619 , 0.31179255, 0.31149486,\n",
       "       0.3152435 , 0.31349185, 0.32009143, 0.32407346, 0.32333758,\n",
       "       0.3224891 , 0.32192862, 0.3321526 , 0.33559343, 0.33460572,\n",
       "       0.3322277 , 0.327342  , 0.32880569, 0.325861  , 0.32590508,\n",
       "       0.32108846, 0.3224715 , 0.32457098, 0.3328617 , 0.32734865,\n",
       "       0.32847095, 0.32859257, 0.32352772, 0.32205042, 0.31472558,\n",
       "       0.31451157, 0.31707147, 0.31531107, 0.31691653, 0.31976223,\n",
       "       0.320873  , 0.32034424, 0.32223547, 0.3203906 , 0.32196245,\n",
       "       0.3189071 , 0.31916803, 0.31693694, 0.31714737, 0.31932986,\n",
       "       0.3233177 , 0.32707274, 0.32741088, 0.3261953 , 0.3266758 ,\n",
       "       0.3251101 , 0.32544538, 0.3257952 , 0.32726443, 0.32247356,\n",
       "       0.3209627 , 0.31919378, 0.31801194, 0.3158683 , 0.31712818,\n",
       "       0.31658685, 0.3109692 , 0.31493163, 0.31738   , 0.31978193,\n",
       "       0.32106972, 0.3246772 , 0.32746157, 0.32660523, 0.33486035,\n",
       "       0.33299363, 0.33249104, 0.3339515 , 0.33274052, 0.33363438,\n",
       "       0.32873124, 0.33020937, 0.3308138 , 0.3416261 , 0.3454566 ,\n",
       "       0.34434882, 0.344189  , 0.3383022 , 0.3369433 , 0.33592016,\n",
       "       0.33311716, 0.3335075 , 0.32539862, 0.32296008, 0.3209661 ,\n",
       "       0.32337108, 0.3260587 , 0.3216524 , 0.31321698, 0.3183893 ,\n",
       "       0.31907517, 0.3141806 , 0.3145511 , 0.31695485, 0.31628063,\n",
       "       0.32077646, 0.31621054, 0.31085318, 0.31545055, 0.3104287 ,\n",
       "       0.3155665 , 0.31816205, 0.3204363 , 0.32104182, 0.32047012,\n",
       "       0.31824118, 0.31827822, 0.32621482, 0.323548  , 0.32199645,\n",
       "       0.31939954, 0.31609198, 0.3162766 , 0.3172692 , 0.31313202,\n",
       "       0.3149763 , 0.31611454, 0.3095786 , 0.3101527 , 0.3117411 ,\n",
       "       0.30390766, 0.30510664, 0.29997146, 0.29175356, 0.30086693,\n",
       "       0.30298936, 0.30256686, 0.2939394 , 0.30515784, 0.3145147 ,\n",
       "       0.30750564, 0.30292737, 0.30112264, 0.30447364, 0.30659303,\n",
       "       0.30796823, 0.31258172, 0.3113338 , 0.31565017, 0.313874  ,\n",
       "       0.3094317 , 0.30838633, 0.31256226, 0.31287634, 0.31230602,\n",
       "       0.31267637, 0.31581956, 0.31681994, 0.31761646, 0.32310772,\n",
       "       0.32155532, 0.3254042 , 0.32489967, 0.32481718, 0.331385  ,\n",
       "       0.33449888, 0.33346543, 0.3418406 , 0.33929643, 0.34010112,\n",
       "       0.3374603 , 0.33001953, 0.32919642, 0.32379064, 0.3214847 ,\n",
       "       0.36486468, 0.37109232, 0.36827713, 0.3728711 , 0.3814638 ,\n",
       "       0.38567525, 0.37347135, 0.3728941 , 0.36160412, 0.3577255 ,\n",
       "       0.35224512, 0.34266254, 0.34572226, 0.34626424, 0.34470278,\n",
       "       0.3420987 , 0.34241706, 0.3408029 , 0.33798194, 0.3316509 ,\n",
       "       0.33410165, 0.33421418, 0.337568  , 0.3398688 , 0.33844104,\n",
       "       0.34237185, 0.342926  , 0.3358714 , 0.33707947, 0.34228268],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2003, 1, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 1, 1], padding=\"VALID\", strides=[1, 2, 1, 1]](max_pooling1d/ExpandDims)' with input shapes: [?,1,1,64].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1653\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 1, 1], padding=\"VALID\", strides=[1, 2, 1, 1]](max_pooling1d/ExpandDims)' with input shapes: [?,1,1,64].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ac845a5f763f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    921\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/keras/layers/pooling.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mpad_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_last'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     outputs = self.pool_function(\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mpool2d\u001b[0;34m(x, pool_size, strides, padding, data_format, pool_mode)\u001b[0m\n\u001b[1;32m   5372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5373\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'max'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5374\u001b[0;31m     x = nn.max_pool(\n\u001b[0m\u001b[1;32m   5375\u001b[0m         x, pool_size, strides, padding=padding, data_format=tf_data_format)\n\u001b[1;32m   5376\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mpool_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(value, ksize, strides, padding, data_format, name, input)\u001b[0m\n\u001b[1;32m   3921\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ksize cannot be zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m     return gen_nn_ops.max_pool(\n\u001b[0m\u001b[1;32m   3924\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool\u001b[0;34m(input, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[1;32m   5232\u001b[0m     \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"NHWC\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5233\u001b[0m   \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5234\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   5235\u001b[0m         \u001b[0;34m\"MaxPool\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5236\u001b[0m                    data_format=data_format, name=name)\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    740\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         compute_device)\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3317\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3318\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3319\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3320\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3321\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1814\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1816\u001b[0;31m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[1;32m   1817\u001b[0m                                 control_input_ops, op_def)\n\u001b[1;32m   1818\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/asv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1655\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Negative dimension size caused by subtracting 2 from 1 for '{{node max_pooling1d/MaxPool}} = MaxPool[T=DT_FLOAT, data_format=\"NHWC\", ksize=[1, 2, 1, 1], padding=\"VALID\", strides=[1, 2, 1, 1]](max_pooling1d/ExpandDims)' with input shapes: [?,1,1,64]."
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_t.shape[1], X_train_t.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(48, activation='relu'))\n",
    "model.add(Dense(n_features))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary(1)\n",
    "\n",
    "#unsucessful tested layers:\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(GRU(64, dropout=0.1, recurrent_dropout=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
