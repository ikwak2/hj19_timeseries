{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "45c250f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b75fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from pandas import Series\n",
    "import math\n",
    "import numpy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "#data = pd.read_csv('bitcoin.csv')\n",
    "data = pd.read_csv('bitcoin2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19c5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_importance(seq, model):\n",
    "\n",
    "    seq = tf.Variable(seq[np.newaxis,:,:], dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(seq)\n",
    "\n",
    "    grads = tape.gradient(predictions, seq)\n",
    "    grads = tf.reduce_mean(grads, axis=1).numpy()[0]\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def gradient_weight(seq, model):\n",
    "\n",
    "    seq = tf.Variable(seq[np.newaxis,:,:], dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(seq)\n",
    "\n",
    "    grad = tape.gradient(predictions, seq)\n",
    "    #gr=grads\n",
    "    #grads = tf.reduce_mean(grads, axis=1).numpy()[0]\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def activation_grad(seq, model):\n",
    "    \n",
    "    seq = seq[np.newaxis,:,:]\n",
    "    grad_model = Model([model.inputs], \n",
    "                       [model.get_layer('extractor').output, \n",
    "                        model.output])\n",
    "\n",
    "    # Obtain the predicted value and the intermediate filters\n",
    "    with tf.GradientTape() as tape:\n",
    "        seq_outputs, predictions = grad_model(seq)\n",
    "\n",
    "    # Extract filters and gradients\n",
    "    output = seq_outputs[0]\n",
    "    grads = tape.gradient(predictions, seq_outputs)[0]\n",
    "\n",
    "    # Average gradients spatially\n",
    "    weights = tf.reduce_mean(grads, axis=0)\n",
    "    \n",
    "    # Get a ponderated map of filters according to grad importance\n",
    "    cam = np.ones(output.shape[0], dtype=np.float32)\n",
    "    for index, w in enumerate(weights):\n",
    "        cam += w * output[:, index]\n",
    "\n",
    "    time = int(seq.shape[1]/output.shape[0])\n",
    "    cam = zoom(cam.numpy(), time, order=1)\n",
    "    heatmap = (cam - cam.min())/(cam.max() - cam.min())\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca45a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index(['timeUTC'],inplace=True)\n",
    "#data = data['item_cnt_day'].resample('D').sum()\n",
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15776573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1ff74c89d0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data['volumeto'][40:200],'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a75ca88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['high', 'low', 'open', 'volumefrom', 'volumeto', 'close'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns[1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ca174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "values = data[data.columns[1:7]].values.reshape(-1,6)\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "#scaled = (values-(np.min(values)))/(np.max(values)-(np.min(values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0654119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volumefrom</th>\n",
       "      <th>volumeto</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timeUTC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-12-12 19:00:00</th>\n",
       "      <td>7219.93</td>\n",
       "      <td>7149.15</td>\n",
       "      <td>7170.18</td>\n",
       "      <td>1536.79</td>\n",
       "      <td>11041994.81</td>\n",
       "      <td>7166.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 20:00:00</th>\n",
       "      <td>7208.36</td>\n",
       "      <td>7166.03</td>\n",
       "      <td>7166.14</td>\n",
       "      <td>785.11</td>\n",
       "      <td>5654607.43</td>\n",
       "      <td>7199.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 21:00:00</th>\n",
       "      <td>7222.43</td>\n",
       "      <td>7194.87</td>\n",
       "      <td>7199.79</td>\n",
       "      <td>672.46</td>\n",
       "      <td>4855213.22</td>\n",
       "      <td>7199.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 22:00:00</th>\n",
       "      <td>7211.80</td>\n",
       "      <td>7187.29</td>\n",
       "      <td>7199.97</td>\n",
       "      <td>807.61</td>\n",
       "      <td>5821087.81</td>\n",
       "      <td>7201.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12 23:00:00</th>\n",
       "      <td>7229.22</td>\n",
       "      <td>7152.84</td>\n",
       "      <td>7201.38</td>\n",
       "      <td>1606.06</td>\n",
       "      <td>11537799.57</td>\n",
       "      <td>7173.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 11:00:00</th>\n",
       "      <td>33847.46</td>\n",
       "      <td>33257.39</td>\n",
       "      <td>33450.01</td>\n",
       "      <td>2066.49</td>\n",
       "      <td>69445570.35</td>\n",
       "      <td>33675.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 12:00:00</th>\n",
       "      <td>33779.76</td>\n",
       "      <td>33505.50</td>\n",
       "      <td>33675.45</td>\n",
       "      <td>1173.82</td>\n",
       "      <td>39473368.04</td>\n",
       "      <td>33590.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 13:00:00</th>\n",
       "      <td>33945.41</td>\n",
       "      <td>33404.87</td>\n",
       "      <td>33590.20</td>\n",
       "      <td>1430.06</td>\n",
       "      <td>48159838.72</td>\n",
       "      <td>33576.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 14:00:00</th>\n",
       "      <td>33907.89</td>\n",
       "      <td>33467.87</td>\n",
       "      <td>33576.82</td>\n",
       "      <td>1198.20</td>\n",
       "      <td>40319458.23</td>\n",
       "      <td>33803.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 15:00:00</th>\n",
       "      <td>33950.67</td>\n",
       "      <td>33739.63</td>\n",
       "      <td>33803.02</td>\n",
       "      <td>569.39</td>\n",
       "      <td>19288055.21</td>\n",
       "      <td>33897.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10005 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         high       low      open  volumefrom     volumeto  \\\n",
       "timeUTC                                                                      \n",
       "2019-12-12 19:00:00   7219.93   7149.15   7170.18     1536.79  11041994.81   \n",
       "2019-12-12 20:00:00   7208.36   7166.03   7166.14      785.11   5654607.43   \n",
       "2019-12-12 21:00:00   7222.43   7194.87   7199.79      672.46   4855213.22   \n",
       "2019-12-12 22:00:00   7211.80   7187.29   7199.97      807.61   5821087.81   \n",
       "2019-12-12 23:00:00   7229.22   7152.84   7201.38     1606.06  11537799.57   \n",
       "...                       ...       ...       ...         ...          ...   \n",
       "2021-02-01 11:00:00  33847.46  33257.39  33450.01     2066.49  69445570.35   \n",
       "2021-02-01 12:00:00  33779.76  33505.50  33675.45     1173.82  39473368.04   \n",
       "2021-02-01 13:00:00  33945.41  33404.87  33590.20     1430.06  48159838.72   \n",
       "2021-02-01 14:00:00  33907.89  33467.87  33576.82     1198.20  40319458.23   \n",
       "2021-02-01 15:00:00  33950.67  33739.63  33803.02      569.39  19288055.21   \n",
       "\n",
       "                        close  \n",
       "timeUTC                        \n",
       "2019-12-12 19:00:00   7166.14  \n",
       "2019-12-12 20:00:00   7199.79  \n",
       "2019-12-12 21:00:00   7199.97  \n",
       "2019-12-12 22:00:00   7201.38  \n",
       "2019-12-12 23:00:00   7173.50  \n",
       "...                       ...  \n",
       "2021-02-01 11:00:00  33675.45  \n",
       "2021-02-01 12:00:00  33590.20  \n",
       "2021-02-01 13:00:00  33576.82  \n",
       "2021-02-01 14:00:00  33803.02  \n",
       "2021-02-01 15:00:00  33897.06  \n",
       "\n",
       "[10005 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = data[data.columns[1:7]]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c267dbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.21993000e+03, 7.14915000e+03, 7.17018000e+03, 1.53679000e+03,\n",
       "        1.10419948e+07, 7.16614000e+03],\n",
       "       [7.20836000e+03, 7.16603000e+03, 7.16614000e+03, 7.85110000e+02,\n",
       "        5.65460743e+06, 7.19979000e+03],\n",
       "       [7.22243000e+03, 7.19487000e+03, 7.19979000e+03, 6.72460000e+02,\n",
       "        4.85521322e+06, 7.19997000e+03],\n",
       "       ...,\n",
       "       [3.39454100e+04, 3.34048700e+04, 3.35902000e+04, 1.43006000e+03,\n",
       "        4.81598387e+07, 3.35768200e+04],\n",
       "       [3.39078900e+04, 3.34678700e+04, 3.35768200e+04, 1.19820000e+03,\n",
       "        4.03194582e+07, 3.38030200e+04],\n",
       "       [3.39506700e+04, 3.37396300e+04, 3.38030200e+04, 5.69390000e+02,\n",
       "        1.92880552e+07, 3.38970600e+04]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.values.reshape(-1,6)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f0e7a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['high', 'low', 'open', 'volumefrom', 'volumeto', 'close'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns[1:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95794b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8004 1000 1001 1001\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(scaled) * 0.80)\n",
    "val_size = int(len(scaled) * 0.10)\n",
    "\n",
    "test_size = len(scaled) - train_size-val_size\n",
    "train,val, test,test1 = scaled[0:train_size,:],scaled[train_size:train_size+val_size,:], scaled[train_size+val_size:,:],values[train_size+val_size:,:]\n",
    "print(len(train),len(val), len(test),len(test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3170220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_mul(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, -1])\n",
    "    print(len(dataY))\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db51fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7980\n",
      "976\n",
      "977\n",
      "977\n"
     ]
    }
   ],
   "source": [
    "look_back = 24\n",
    "trainX, trainY = create_dataset_mul(train, look_back)\n",
    "valX, valY = create_dataset_mul(val, look_back)\n",
    "testX, testY = create_dataset_mul(test, look_back)\n",
    "test1X, test1Y = create_dataset_mul(test1, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bcff97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7980, 24, 6)\n",
      "(7980,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8275b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e0248fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_min, X_train_max = trainX.min(axis=0), trainX.max(axis=0)\n",
    "y_train_min, y_train_max = trainY.min(axis=0), trainY.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ab7995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = (trainX - X_train_min)/(X_train_max - X_train_min+ 1e-9 )\n",
    "valX= (valX - X_train_min)/(X_train_max - X_train_min+ 1e-9 )\n",
    "testX = (testX - X_train_min)/(X_train_max - X_train_min+ 1e-9 )\n",
    "trainY = (trainY - y_train_min)/(y_train_max - y_train_min + 1e-9)\n",
    "valY = (valY - y_train_min)/(y_train_max - y_train_min+ 1e-9 )\n",
    "testY = (testY - y_train_min)/(y_train_max - y_train_min+ 1e-9 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7405669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7980, 24, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911fe7e",
   "metadata": {},
   "source": [
    "## MTEX(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "451effb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41198bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape(-1,24,6,1)\n",
    "testX = testX.reshape(-1,24,6,1)\n",
    "valX = valX.reshape(-1,24,6,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "999edccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_25 (Reshape)         (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 5ms/step - loss: 0.0145 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00160, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.0519e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00160 to 0.00131, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7541e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00131 to 0.00109, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4976e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00109\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2578e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00109 to 0.00100, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0616e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00100\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.8122e-05 - val_loss: 8.8883e-04\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00100 to 0.00089, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.5896e-05 - val_loss: 8.3860e-04\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00089 to 0.00084, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.8114e-05 - val_loss: 8.9041e-04\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00084\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.2068e-05 - val_loss: 7.6004e-04\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00084 to 0.00076, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.9137e-05 - val_loss: 7.5275e-04\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00076 to 0.00075, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.0227e-05 - val_loss: 7.3639e-04\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00075 to 0.00074, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.7956e-05 - val_loss: 6.9183e-04\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00074 to 0.00069, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8577e-05 - val_loss: 6.9259e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00069\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2385e-05 - val_loss: 6.0180e-04\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00069 to 0.00060, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4332e-05 - val_loss: 9.6277e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00060\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5804e-05 - val_loss: 6.2812e-04\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00060\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.0085e-05 - val_loss: 5.8693e-04\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00060 to 0.00059, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3317e-05 - val_loss: 5.9001e-04\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00059\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7447e-05 - val_loss: 6.3348e-04\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00059\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1474e-05 - val_loss: 5.8345e-04\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00059 to 0.00058, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8458e-05 - val_loss: 6.0352e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00058\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8443e-05 - val_loss: 5.7152e-04\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00058 to 0.00057, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5557e-05 - val_loss: 6.0222e-04\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00057\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4493e-05 - val_loss: 7.8478e-04\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00057\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9621e-05 - val_loss: 5.2275e-04\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00057 to 0.00052, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9394e-05 - val_loss: 5.1409e-04\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00052 to 0.00051, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6072e-05 - val_loss: 5.8964e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00051\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.3500e-05 - val_loss: 5.5430e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00051\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7663e-05 - val_loss: 6.8755e-04\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00051\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9441e-05 - val_loss: 5.7545e-04\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00051\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.4911e-05 - val_loss: 4.7797e-04\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00051 to 0.00048, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7231e-05 - val_loss: 4.8153e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00048\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3966e-05 - val_loss: 4.2993e-04\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00048 to 0.00043, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.0234e-05 - val_loss: 5.5857e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00043\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6091e-05 - val_loss: 5.5416e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00043\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1093e-05 - val_loss: 8.3230e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00043\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6264e-05 - val_loss: 5.0402e-04\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00043\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7879e-05 - val_loss: 5.5455e-04\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00043\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8739e-05 - val_loss: 4.3079e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00043\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0617e-05 - val_loss: 5.0393e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00043\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2837e-05 - val_loss: 5.0009e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00043\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.0451e-05 - val_loss: 4.2365e-04\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00043 to 0.00042, saving model to saved_weights/bitcoin_MTEX_onestep(1).hdf5\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7946e-05 - val_loss: 6.5852e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00042\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6424e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00042\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2284e-05 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00042\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8494e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00042\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6628e-05 - val_loss: 6.2734e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00042\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8695e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00042\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7605e-05 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00042\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.6406e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00042\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7594e-05 - val_loss: 4.6841e-04\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00042\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8889e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00042\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6398e-05 - val_loss: 7.6805e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00042\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9108e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00042\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4490e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00042\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.3910e-05 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00042\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1076e-05 - val_loss: 8.5505e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00042\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4262e-05 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00042\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5579e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00042\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1789e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00042\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2339e-05 - val_loss: 8.3914e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00042\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9943e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00042\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6142e-05 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00042\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9252e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00042\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6456e-05 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00042\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6900e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00042\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2646e-05 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00042\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7004e-05 - val_loss: 9.7049e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00042\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8144e-05 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00042\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2787e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00042\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1351e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00042\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2610e-05 - val_loss: 0.0029\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00042\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7779e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00042\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3966e-05 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00042\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2223e-05 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00042\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4186e-05 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00042\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6149e-05 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00042\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9310e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00042\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6107e-05 - val_loss: 0.0024\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00042\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7570e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00042\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.8029e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00042\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5005e-05 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00042\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2120e-05 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00042\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.7745e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00042\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4074e-05 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00042\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1763e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00042\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3829e-05 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00042\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6514e-05 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00042\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1243e-05 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00042\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3269e-05 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00042\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0677e-05 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00042\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2069e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00042\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3273e-05 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00042\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6891e-05 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00042\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9432e-05 - val_loss: 0.0041\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00042\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4841e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00042\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1752e-05 - val_loss: 0.0060\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00042\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2191e-05 - val_loss: 0.0040\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00042\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7029e-05 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00042\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1975e-05 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00042\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8587e-05 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00042\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5042e-05 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00042\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0252e-05 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00042\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9967e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00042\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3520e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00042\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2778e-05 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00042\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5169e-05 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00042\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5350e-05 - val_loss: 0.0031\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00042\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3924e-05 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00042\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4457e-05 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00042\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4341e-05 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00042\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2167e-05 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00042\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3911e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00042\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3176e-05 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00042\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8397e-05 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00042\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5698e-05 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00042\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2120e-05 - val_loss: 0.0031\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00042\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9840e-05 - val_loss: 0.0029\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00042\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7285e-05 - val_loss: 0.0031\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00042\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0844e-05 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00042\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8173e-05 - val_loss: 0.0041\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00042\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9847e-05 - val_loss: 0.0040\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00042\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6166e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00042\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0440e-05 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00042\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8881e-05 - val_loss: 0.0037\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00042\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5240e-05 - val_loss: 0.0041\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00042\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3871e-05 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00042\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8629e-05 - val_loss: 0.0061\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00042\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3575e-05 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00042\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9248e-05 - val_loss: 0.0046\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00042\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3795e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00042\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8717e-05 - val_loss: 0.0048\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00042\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0206e-05 - val_loss: 0.0038\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00042\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2947e-05 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00042\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1963e-05 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00042\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3189e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00042\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0043e-05 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00042\n",
      "Epoch 139/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3227e-05 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00042\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1263e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00042\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0236e-05 - val_loss: 0.0041\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00042\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1616e-05 - val_loss: 0.0052\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00042\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2131e-05 - val_loss: 0.0044\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00042\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3707e-05 - val_loss: 0.0044\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00042\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2780e-05 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00042\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5394e-05 - val_loss: 0.0051\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00042\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1879e-05 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00042\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3598e-05 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00042\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3887e-05 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00042\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8153e-05 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00042\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7585e-05 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00042\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5762e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00042\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2103e-05 - val_loss: 0.0028\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00042\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9887e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00042\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4313e-05 - val_loss: 0.0038\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00042\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8303e-05 - val_loss: 0.0038\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00042\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.0297e-05 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00042\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1930e-05 - val_loss: 0.0059\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00042\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2391e-05 - val_loss: 0.0042\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00042\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0008e-05 - val_loss: 0.0038\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00042\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9751e-05 - val_loss: 0.0033\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00042\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0457e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00042\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3557e-05 - val_loss: 0.0064\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00042\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7794e-05 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00042\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5877e-05 - val_loss: 0.0048\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00042\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2328e-05 - val_loss: 0.0031\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00042\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8027e-05 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00042\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8033e-05 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00042\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1327e-05 - val_loss: 0.0047\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00042\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9040e-05 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00042\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2414e-05 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00042\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1912e-05 - val_loss: 0.0046\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00042\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.6500e-05 - val_loss: 0.0038\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00042\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.6210e-05 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00042\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8698e-05 - val_loss: 0.0029\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00042\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8705e-05 - val_loss: 0.0031\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00042\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3387e-05 - val_loss: 0.0042\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00042\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8714e-05 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00042\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1405e-05 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00042\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8537e-05 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00042\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2088e-05 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00042\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4888e-05 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00042\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1062e-05 - val_loss: 0.0039\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00042\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1131e-05 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00042\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9472e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00042\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0846e-05 - val_loss: 0.0029\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00042\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0303e-05 - val_loss: 0.0040\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00042\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0185e-05 - val_loss: 0.0036\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00042\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3807e-05 - val_loss: 0.0034\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00042\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8139e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00042\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7107e-05 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00042\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2683e-05 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00042\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9242e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00042\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8984e-05 - val_loss: 0.0027\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00042\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.6448e-05 - val_loss: 0.0035\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00042\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.7866e-05 - val_loss: 0.0043\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00042\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9380e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00042\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8853e-05 - val_loss: 0.0041\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00042\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4590e-05 - val_loss: 0.0037\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00042\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0875e-05 - val_loss: 0.0032\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00042\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(1).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8639f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(1).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3367b5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.02143772\n",
      "Test RMSE : 0.0752163956550646\n",
      "Test MAE : 0.053553373\n",
      "Test MSE : 0.005657506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd2e8b",
   "metadata": {},
   "source": [
    "## MTEX(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56b804fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0205 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00107, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.1660e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00107\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.8940e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00107\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.7054e-04 - val_loss: 8.3020e-04\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00107 to 0.00083, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.5133e-04 - val_loss: 7.7393e-04\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00083 to 0.00077, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 1.3834e-04 - val_loss: 6.7370e-04\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00077 to 0.00067, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.1803e-04 - val_loss: 5.7089e-04\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00067 to 0.00057, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.0807e-04 - val_loss: 7.0308e-04\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00057\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.9470e-05 - val_loss: 4.8802e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00057 to 0.00049, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.8801e-05 - val_loss: 3.9118e-04\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00049 to 0.00039, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 6.5986e-05 - val_loss: 3.7268e-04\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00039 to 0.00037, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.9317e-05 - val_loss: 3.0827e-04\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00037 to 0.00031, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.4131e-05 - val_loss: 7.6675e-04\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00031\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.6851e-05 - val_loss: 3.9153e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00031\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6084e-05 - val_loss: 3.1019e-04\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00031\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.5493e-05 - val_loss: 3.7369e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00031\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4487e-05 - val_loss: 2.6091e-04\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00031 to 0.00026, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1712e-05 - val_loss: 3.5052e-04\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00026\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3958e-05 - val_loss: 6.6159e-04\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00026\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4762e-05 - val_loss: 2.6973e-04\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00026\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.5505e-05 - val_loss: 2.4396e-04\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00026 to 0.00024, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4240e-05 - val_loss: 2.7051e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00024\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6743e-05 - val_loss: 4.1529e-04\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00024\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0534e-05 - val_loss: 4.1364e-04\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00024\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4529e-05 - val_loss: 2.4320e-04\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00024 to 0.00024, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8742e-05 - val_loss: 2.2357e-04\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00024 to 0.00022, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6005e-05 - val_loss: 5.4432e-04\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00022\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.0598e-05 - val_loss: 7.1177e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00022\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3324e-05 - val_loss: 2.7762e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00022\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.7711e-05 - val_loss: 4.5754e-04\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00022\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2752e-05 - val_loss: 3.1530e-04\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00022\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7513e-05 - val_loss: 2.6934e-04\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00022\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.9972e-05 - val_loss: 2.1663e-04\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00022 to 0.00022, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.8626e-05 - val_loss: 2.2668e-04\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00022\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1939e-05 - val_loss: 3.4564e-04\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00022\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4969e-05 - val_loss: 2.0493e-04\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00022 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8717e-05 - val_loss: 2.1182e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00020\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4242e-05 - val_loss: 2.1658e-04\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00020\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8051e-05 - val_loss: 2.1873e-04\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00020\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1354e-05 - val_loss: 2.0074e-04\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00020 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3258e-05 - val_loss: 2.4364e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00020\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0057e-05 - val_loss: 3.8221e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00020\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3103e-05 - val_loss: 5.7987e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00020\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0649e-05 - val_loss: 5.6427e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00020\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.0916e-04 - val_loss: 2.6502e-04\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00020\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4170e-05 - val_loss: 1.9940e-04\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00020 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1738e-05 - val_loss: 3.9722e-04\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00020\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7937e-05 - val_loss: 2.0627e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00020\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8642e-05 - val_loss: 2.8519e-04\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00020\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1430e-05 - val_loss: 2.4328e-04\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00020\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7654e-05 - val_loss: 1.8261e-04\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00020 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7326e-05 - val_loss: 2.0262e-04\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00018\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3555e-05 - val_loss: 2.2846e-04\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00018\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0591e-05 - val_loss: 1.7891e-04\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.00018 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2631e-05 - val_loss: 3.6994e-04\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00018\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5183e-05 - val_loss: 2.1252e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00018\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6407e-05 - val_loss: 2.2039e-04\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00018\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3923e-05 - val_loss: 2.4339e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00018\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7132e-05 - val_loss: 3.7247e-04\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00018\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1699e-05 - val_loss: 2.2148e-04\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00018\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7770e-05 - val_loss: 1.7912e-04\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00018\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3163e-05 - val_loss: 1.9692e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00018\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6568e-05 - val_loss: 1.7391e-04\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00018 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 4.0193e-05 - val_loss: 3.9396e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00017\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9469e-05 - val_loss: 2.2749e-04\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00017\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8405e-05 - val_loss: 1.7981e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00017\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1259e-05 - val_loss: 1.8473e-04\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00017\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5122e-05 - val_loss: 1.7654e-04\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00017\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6622e-05 - val_loss: 3.9282e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00017\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7130e-05 - val_loss: 1.8823e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00017\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8174e-05 - val_loss: 1.7204e-04\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.00017 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5417e-05 - val_loss: 1.7283e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00017\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8609e-05 - val_loss: 1.9777e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00017\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3540e-05 - val_loss: 2.1941e-04\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00017\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0483e-05 - val_loss: 3.3874e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00017\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7558e-05 - val_loss: 2.2413e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00017\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6486e-05 - val_loss: 3.8633e-04\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00017\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1889e-05 - val_loss: 6.0759e-04\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00017\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3018e-05 - val_loss: 3.0664e-04\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00017\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1301e-05 - val_loss: 1.6017e-04\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.00017 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2655e-05 - val_loss: 2.6057e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00016\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5901e-05 - val_loss: 1.6326e-04\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00016\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0267e-05 - val_loss: 5.5731e-04\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00016\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6159e-05 - val_loss: 3.4343e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00016\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1787e-05 - val_loss: 2.0849e-04\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00016\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4240e-05 - val_loss: 4.1879e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00016\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0743e-05 - val_loss: 1.8788e-04\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00016\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2213e-05 - val_loss: 6.3050e-04\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00016\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3438e-05 - val_loss: 3.2066e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00016\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4094e-05 - val_loss: 2.2121e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00016\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3138e-05 - val_loss: 3.3711e-04\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00016\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5085e-05 - val_loss: 1.7797e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00016\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0682e-05 - val_loss: 1.5686e-04\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.00016 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5972e-05 - val_loss: 4.3075e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00016\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1127e-05 - val_loss: 4.0238e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00016\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1044e-05 - val_loss: 1.5953e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00016\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1991e-05 - val_loss: 1.6041e-04\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00016\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9554e-05 - val_loss: 1.6689e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00016\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2920e-05 - val_loss: 9.5824e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00016\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2928e-05 - val_loss: 1.6580e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00016\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1289e-05 - val_loss: 2.5269e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00016\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5911e-05 - val_loss: 1.6885e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00016\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5288e-05 - val_loss: 1.7103e-04\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00016\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0980e-05 - val_loss: 2.1817e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00016\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6986e-05 - val_loss: 1.7351e-04\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00016\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6436e-05 - val_loss: 1.8532e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00016\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0926e-05 - val_loss: 2.7469e-04\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00016\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6967e-05 - val_loss: 2.0874e-04\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00016\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1088e-05 - val_loss: 3.0894e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00016\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2086e-05 - val_loss: 2.7208e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00016\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1973e-05 - val_loss: 4.2194e-04\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00016\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0151e-05 - val_loss: 3.5925e-04\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00016\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8298e-05 - val_loss: 1.6134e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00016\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2927e-05 - val_loss: 2.3317e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00016\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4775e-05 - val_loss: 1.7608e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00016\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2171e-05 - val_loss: 1.7386e-04\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00016\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.7549e-05 - val_loss: 2.3235e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00016\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6249e-05 - val_loss: 2.8506e-04\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00016\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9069e-05 - val_loss: 2.1882e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00016\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1721e-05 - val_loss: 1.8651e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00016\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0120e-05 - val_loss: 1.6058e-04\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00016\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5645e-05 - val_loss: 2.0271e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00016\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3054e-05 - val_loss: 3.1279e-04\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00016\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4572e-05 - val_loss: 1.5656e-04\n",
      "\n",
      "Epoch 00124: val_loss improved from 0.00016 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1209e-05 - val_loss: 1.6173e-04\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00016\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7670e-05 - val_loss: 2.0053e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00016\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8555e-05 - val_loss: 1.6773e-04\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00016\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0009e-05 - val_loss: 2.3508e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00016\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4231e-05 - val_loss: 1.5929e-04\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00016\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4595e-05 - val_loss: 1.8766e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00016\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0435e-05 - val_loss: 5.7404e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00016\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9172e-05 - val_loss: 1.9520e-04\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00016\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9146e-05 - val_loss: 1.6259e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00016\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6270e-05 - val_loss: 1.6053e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00016\n",
      "Epoch 135/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1299e-05 - val_loss: 3.9612e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00016\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9514e-05 - val_loss: 4.2144e-04\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00016\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5180e-05 - val_loss: 1.9450e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00016\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7531e-05 - val_loss: 4.2833e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00016\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8613e-05 - val_loss: 3.7125e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00016\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5365e-05 - val_loss: 2.6928e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00016\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6160e-05 - val_loss: 1.7181e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00016\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8194e-05 - val_loss: 2.8313e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00016\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8246e-05 - val_loss: 1.8111e-04\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00016\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0743e-05 - val_loss: 2.4420e-04\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00016\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0937e-05 - val_loss: 1.9685e-04\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00016\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6208e-05 - val_loss: 1.7332e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00016\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1207e-05 - val_loss: 1.8148e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00016\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1472e-05 - val_loss: 1.9266e-04\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00016\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1447e-05 - val_loss: 1.6228e-04\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00016\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6373e-05 - val_loss: 1.8694e-04\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00016\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0892e-05 - val_loss: 1.7275e-04\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00016\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8083e-05 - val_loss: 2.8588e-04\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00016\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 3.3364e-05 - val_loss: 1.6456e-04\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00016\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8124e-05 - val_loss: 1.5876e-04\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00016\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4605e-05 - val_loss: 1.7151e-04\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00016\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6816e-05 - val_loss: 2.1177e-04\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00016\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8623e-05 - val_loss: 2.1344e-04\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00016\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9991e-05 - val_loss: 1.6090e-04\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00016\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3285e-05 - val_loss: 1.6583e-04\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00016\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0504e-05 - val_loss: 1.5305e-04\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.00016 to 0.00015, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0486e-05 - val_loss: 2.1226e-04\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00015\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1787e-05 - val_loss: 1.7109e-04\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00015\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5200e-05 - val_loss: 4.2682e-04\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00015\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3362e-05 - val_loss: 2.7537e-04\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00015\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1170e-05 - val_loss: 1.5399e-04\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00015\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0932e-05 - val_loss: 3.3053e-04\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00015\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0506e-05 - val_loss: 1.5240e-04\n",
      "\n",
      "Epoch 00167: val_loss improved from 0.00015 to 0.00015, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4320e-05 - val_loss: 1.9632e-04\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00015\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1762e-05 - val_loss: 1.6156e-04\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00015\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8895e-05 - val_loss: 3.8082e-04\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00015\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0437e-05 - val_loss: 1.5572e-04\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00015\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9143e-05 - val_loss: 2.0823e-04\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00015\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1394e-05 - val_loss: 1.5330e-04\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00015\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9408e-05 - val_loss: 1.8261e-04\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00015\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5356e-05 - val_loss: 1.5819e-04\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00015\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2007e-05 - val_loss: 1.7946e-04\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00015\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9589e-05 - val_loss: 2.0561e-04\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00015\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0007e-05 - val_loss: 1.5604e-04\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00015\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0351e-05 - val_loss: 2.1944e-04\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00015\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1779e-05 - val_loss: 1.6703e-04\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00015\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0153e-05 - val_loss: 1.6089e-04\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00015\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1065e-05 - val_loss: 1.9766e-04\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00015\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7595e-05 - val_loss: 1.5393e-04\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00015\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0775e-05 - val_loss: 1.5863e-04\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00015\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0029e-05 - val_loss: 1.6799e-04\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00015\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1293e-05 - val_loss: 1.6039e-04\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00015\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5778e-05 - val_loss: 2.6706e-04\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00015\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5683e-05 - val_loss: 1.6413e-04\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00015\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8050e-05 - val_loss: 1.5467e-04\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00015\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8689e-05 - val_loss: 2.2874e-04\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00015\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1546e-05 - val_loss: 1.5165e-04\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.00015 to 0.00015, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9230e-05 - val_loss: 2.2462e-04\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00015\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0197e-05 - val_loss: 2.7058e-04\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00015\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 3.3889e-05 - val_loss: 3.2679e-04\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00015\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1138e-05 - val_loss: 4.6514e-04\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00015\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2014e-05 - val_loss: 1.7072e-04\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00015\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9238e-05 - val_loss: 1.5485e-04\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00015\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7100e-05 - val_loss: 2.0672e-04\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00015\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0047e-05 - val_loss: 1.5076e-04\n",
      "\n",
      "Epoch 00199: val_loss improved from 0.00015 to 0.00015, saving model to saved_weights/bitcoin_MTEX_onestep(2).hdf5\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8276e-05 - val_loss: 1.5267e-04\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00015\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(2).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7c90d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(2).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43b1f645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.011665503\n",
      "Test RMSE : 0.041703603467075995\n",
      "Test MAE : 0.029089898\n",
      "Test MSE : 0.0017391905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b243e372",
   "metadata": {},
   "source": [
    "## MTEX(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1dfd1886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0306 - val_loss: 0.0111\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01108, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.0412e-04 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01108 to 0.00202, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.4290e-04 - val_loss: 7.4548e-04\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00202 to 0.00075, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.1490e-04 - val_loss: 5.1942e-04\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00075 to 0.00052, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.0737e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00052\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.9147e-05 - val_loss: 4.4549e-04\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00052 to 0.00045, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.3459e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00045\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.1010e-04 - val_loss: 4.7429e-04\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00045\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.1388e-05 - val_loss: 3.2076e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00045 to 0.00032, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7670e-05 - val_loss: 6.5268e-04\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00032\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7441e-05 - val_loss: 8.3685e-04\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00032\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.1540e-05 - val_loss: 5.0138e-04\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00032\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3385e-05 - val_loss: 3.0915e-04\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00032 to 0.00031, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6293e-05 - val_loss: 5.0736e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00031\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.8292e-05 - val_loss: 2.6447e-04\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00031 to 0.00026, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.9154e-05 - val_loss: 4.7920e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00026\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6983e-05 - val_loss: 2.9385e-04\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00026\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0698e-05 - val_loss: 5.1169e-04\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00026\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7725e-05 - val_loss: 6.1220e-04\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00026\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7926e-05 - val_loss: 2.6622e-04\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00026\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1805e-05 - val_loss: 4.6022e-04\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00026\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4722e-05 - val_loss: 5.3270e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00026\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3063e-05 - val_loss: 8.2766e-04\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00026\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7974e-05 - val_loss: 3.9474e-04\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00026\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4586e-05 - val_loss: 3.2755e-04\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00026\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3520e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00026\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1106e-05 - val_loss: 2.4324e-04\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00026 to 0.00024, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1031e-05 - val_loss: 5.3802e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00024\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2554e-05 - val_loss: 4.2477e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00024\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0051e-05 - val_loss: 2.2756e-04\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00024 to 0.00023, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6040e-05 - val_loss: 3.8380e-04\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00023\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5008e-05 - val_loss: 5.4754e-04\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00023\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4022e-05 - val_loss: 4.2358e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00023\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8809e-05 - val_loss: 3.8093e-04\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00023\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9793e-05 - val_loss: 2.3208e-04\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00023\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4234e-05 - val_loss: 2.2807e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00023\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0508e-05 - val_loss: 6.3955e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00023\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2360e-05 - val_loss: 2.1965e-04\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00023 to 0.00022, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7407e-05 - val_loss: 2.4171e-04\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00022\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6610e-05 - val_loss: 2.6613e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00022\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5743e-05 - val_loss: 2.7303e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00022\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3046e-05 - val_loss: 3.6744e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00022\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0837e-05 - val_loss: 2.3785e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00022\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6389e-05 - val_loss: 4.3698e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00022\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1398e-05 - val_loss: 1.9757e-04\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00022 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7606e-05 - val_loss: 3.5501e-04\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00020\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7721e-05 - val_loss: 1.8940e-04\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.00020 to 0.00019, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6940e-05 - val_loss: 2.7665e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00019\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7610e-05 - val_loss: 2.0593e-04\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00019\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0747e-05 - val_loss: 3.1728e-04\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00019\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7371e-05 - val_loss: 5.7227e-04\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00019\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8870e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00019\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8973e-05 - val_loss: 1.8487e-04\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00019 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3432e-05 - val_loss: 2.8483e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00018\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8674e-05 - val_loss: 3.4676e-04\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00018\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0429e-05 - val_loss: 3.9490e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00018\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1751e-05 - val_loss: 5.4548e-04\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00018\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7919e-05 - val_loss: 2.0053e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00018\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6267e-05 - val_loss: 2.4728e-04\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00018\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8072e-05 - val_loss: 2.2266e-04\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00018\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5417e-05 - val_loss: 3.9867e-04\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00018\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5320e-05 - val_loss: 1.9046e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00018\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4230e-05 - val_loss: 3.4048e-04\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00018\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2857e-05 - val_loss: 2.7899e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00018\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8759e-05 - val_loss: 2.2865e-04\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00018\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3636e-05 - val_loss: 3.7967e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00018\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5848e-05 - val_loss: 1.7884e-04\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.00018 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3987e-05 - val_loss: 1.7609e-04\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00018 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7119e-05 - val_loss: 2.8918e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00018\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5188e-05 - val_loss: 2.0010e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00018\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4191e-05 - val_loss: 2.0222e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00018\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6239e-05 - val_loss: 1.9931e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00018\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2389e-05 - val_loss: 2.3758e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00018\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2263e-05 - val_loss: 2.0781e-04\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00018\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2282e-05 - val_loss: 2.8402e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00018\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6903e-05 - val_loss: 1.7694e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00018\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0619e-05 - val_loss: 1.6662e-04\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.00018 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1532e-05 - val_loss: 1.7936e-04\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00017\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1225e-05 - val_loss: 1.7772e-04\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00017\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8926e-05 - val_loss: 3.1387e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00017\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0291e-05 - val_loss: 2.5032e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00017\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8773e-05 - val_loss: 1.6553e-04\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.00017 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3826e-05 - val_loss: 2.2293e-04\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00017\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2985e-05 - val_loss: 2.7493e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00017\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5890e-05 - val_loss: 1.6042e-04\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00017 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5220e-05 - val_loss: 1.6509e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00016\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3239e-05 - val_loss: 1.6766e-04\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00016\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9320e-05 - val_loss: 1.5674e-04\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00016 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0440e-05 - val_loss: 1.6767e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00016\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5100e-05 - val_loss: 1.9158e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00016\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1844e-05 - val_loss: 1.8156e-04\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00016\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2000e-05 - val_loss: 2.2945e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00016\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8526e-05 - val_loss: 2.8245e-04\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00016\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1825e-05 - val_loss: 1.9082e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00016\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2290e-05 - val_loss: 1.6871e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00016\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4466e-05 - val_loss: 4.2906e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00016\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0339e-05 - val_loss: 1.6048e-04\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00016\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2195e-05 - val_loss: 1.9601e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00016\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4600e-05 - val_loss: 1.5686e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00016\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4909e-05 - val_loss: 1.6636e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00016\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 3.1128e-05 - val_loss: 2.3637e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00016\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0360e-05 - val_loss: 1.6715e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00016\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6769e-05 - val_loss: 1.5804e-04\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00016\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1569e-05 - val_loss: 3.0284e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00016\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0587e-05 - val_loss: 3.3875e-04\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00016\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7942e-05 - val_loss: 3.1883e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00016\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5168e-05 - val_loss: 3.4890e-04\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00016\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3056e-05 - val_loss: 1.6517e-04\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00016\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3022e-05 - val_loss: 5.9205e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00016\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3210e-05 - val_loss: 1.6554e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00016\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8678e-05 - val_loss: 3.9567e-04\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00016\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8746e-05 - val_loss: 2.3092e-04\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00016\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5098e-05 - val_loss: 1.7736e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00016\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5742e-05 - val_loss: 1.6149e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00016\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8813e-05 - val_loss: 1.7865e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00016\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7186e-05 - val_loss: 1.8778e-04\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00016\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7851e-05 - val_loss: 1.6958e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00016\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9450e-05 - val_loss: 1.6465e-04\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00016\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9862e-05 - val_loss: 2.0996e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00016\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6806e-05 - val_loss: 1.7896e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00016\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2168e-05 - val_loss: 2.6125e-04\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00016\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1306e-05 - val_loss: 1.6142e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00016\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1231e-05 - val_loss: 1.5597e-04\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.00016 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(3).hdf5\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9770e-05 - val_loss: 2.0005e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00016\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2038e-05 - val_loss: 1.6385e-04\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00016\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0496e-05 - val_loss: 1.7383e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00016\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8359e-05 - val_loss: 1.9997e-04\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00016\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7989e-05 - val_loss: 1.8728e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00016\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8764e-05 - val_loss: 2.0776e-04\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00016\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2250e-05 - val_loss: 1.6566e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00016\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3238e-05 - val_loss: 1.7009e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00016\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7187e-05 - val_loss: 1.9614e-04\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00016\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9696e-05 - val_loss: 1.5833e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00016\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5046e-05 - val_loss: 2.1835e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00016\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0378e-05 - val_loss: 1.6995e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00016\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1926e-05 - val_loss: 1.6557e-04\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00016\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2022e-05 - val_loss: 3.3518e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00016\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5798e-05 - val_loss: 1.6399e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00016\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1883e-05 - val_loss: 1.7035e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00016\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 3.0525e-05 - val_loss: 1.9250e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00016\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6682e-05 - val_loss: 1.6886e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00016\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3873e-05 - val_loss: 1.7478e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00016\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00016\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.2471e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00016\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.6353e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00016\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9489e-05 - val_loss: 6.2074e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00016\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.9999e-05 - val_loss: 5.2613e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00016\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2368e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00016\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9174e-05 - val_loss: 4.7351e-04\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00016\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5612e-05 - val_loss: 7.9400e-04\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00016\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7531e-05 - val_loss: 8.5954e-04\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00016\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1390e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00016\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3071e-05 - val_loss: 6.9695e-04\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00016\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6380e-05 - val_loss: 5.2195e-04\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00016\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1545e-05 - val_loss: 6.4421e-04\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00016\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1311e-05 - val_loss: 6.6937e-04\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00016\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2959e-05 - val_loss: 3.5446e-04\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00016\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1888e-05 - val_loss: 2.6655e-04\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00016\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1811e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00016\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0795e-05 - val_loss: 5.1913e-04\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00016\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1922e-05 - val_loss: 6.2301e-04\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00016\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0601e-05 - val_loss: 2.7054e-04\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00016\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0657e-05 - val_loss: 2.8120e-04\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00016\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0433e-05 - val_loss: 3.3986e-04\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00016\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9326e-05 - val_loss: 4.0708e-04\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00016\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8018e-05 - val_loss: 4.8448e-04\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00016\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9955e-05 - val_loss: 3.0906e-04\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00016\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2419e-05 - val_loss: 2.0899e-04\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00016\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3674e-05 - val_loss: 3.9592e-04\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00016\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6346e-05 - val_loss: 2.5437e-04\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00016\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8625e-05 - val_loss: 5.4366e-04\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00016\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2276e-05 - val_loss: 3.3434e-04\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00016\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0528e-05 - val_loss: 3.7008e-04\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00016\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4365e-05 - val_loss: 2.2107e-04\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00016\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2435e-05 - val_loss: 1.8392e-04\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00016\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9779e-05 - val_loss: 4.0012e-04\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00016\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9121e-05 - val_loss: 4.5386e-04\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00016\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1130e-05 - val_loss: 2.3621e-04\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00016\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6795e-05 - val_loss: 3.9493e-04\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00016\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2569e-05 - val_loss: 1.7979e-04\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00016\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5722e-05 - val_loss: 3.4367e-04\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00016\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0352e-05 - val_loss: 3.2288e-04\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00016\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4037e-05 - val_loss: 3.4845e-04\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00016\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0029e-05 - val_loss: 2.6603e-04\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00016\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0717e-05 - val_loss: 6.4546e-04\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00016\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4948e-05 - val_loss: 1.6750e-04\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00016\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8354e-05 - val_loss: 3.2685e-04\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00016\n",
      "Epoch 188/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9354e-05 - val_loss: 1.9027e-04\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00016\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1730e-05 - val_loss: 1.6771e-04\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00016\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1527e-05 - val_loss: 1.6936e-04\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00016\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4930e-05 - val_loss: 2.1689e-04\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00016\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9737e-05 - val_loss: 2.4562e-04\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00016\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0526e-05 - val_loss: 2.9629e-04\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00016\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8280e-05 - val_loss: 2.7885e-04\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00016\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9459e-05 - val_loss: 2.2829e-04\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00016\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9006e-05 - val_loss: 1.6367e-04\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00016\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2984e-05 - val_loss: 1.6391e-04\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00016\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3410e-05 - val_loss: 1.6897e-04\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00016\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1818e-05 - val_loss: 1.9853e-04\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00016\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5959e-05 - val_loss: 1.9680e-04\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00016\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(3).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1984d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(3).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40548eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.014790596\n",
      "Test RMSE : 0.056570134612526676\n",
      "Test MAE : 0.037197143\n",
      "Test MSE : 0.0032001801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fce50",
   "metadata": {},
   "source": [
    "## MTEX(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8ab37529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_12 (Reshape)         (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0257 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00152, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8013e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00152 to 0.00120, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.9672e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00120 to 0.00106, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.6553e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00106 to 0.00104, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.4066e-04 - val_loss: 6.6454e-04\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00104 to 0.00066, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.2684e-04 - val_loss: 6.5819e-04\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00066 to 0.00066, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.0057e-04 - val_loss: 5.8935e-04\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00066 to 0.00059, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.0705e-04 - val_loss: 7.7709e-04\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00059\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.3750e-05 - val_loss: 5.1486e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00059 to 0.00051, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.1009e-04 - val_loss: 6.6094e-04\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00051\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.2819e-05 - val_loss: 3.3279e-04\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00051 to 0.00033, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9223e-05 - val_loss: 6.9809e-04\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00033\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.1879e-05 - val_loss: 4.5087e-04\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00033\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.8637e-05 - val_loss: 5.6516e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00033\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.6487e-05 - val_loss: 3.7648e-04\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00033\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.8516e-05 - val_loss: 5.8028e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00033\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0123e-05 - val_loss: 5.2536e-04\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00033\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4903e-05 - val_loss: 8.1442e-04\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00033\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.8293e-05 - val_loss: 2.9039e-04\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00033 to 0.00029, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.9901e-05 - val_loss: 2.5299e-04\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00029 to 0.00025, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9190e-05 - val_loss: 3.0881e-04\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00025\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7465e-05 - val_loss: 2.6554e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00025\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.3073e-05 - val_loss: 4.8141e-04\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00025\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.9341e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00025\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.5399e-05 - val_loss: 4.0817e-04\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00025\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2880e-05 - val_loss: 3.9788e-04\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00025\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3145e-05 - val_loss: 2.3960e-04\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00025 to 0.00024, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7181e-05 - val_loss: 4.4113e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00024\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4574e-05 - val_loss: 3.3226e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00024\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0507e-05 - val_loss: 2.2211e-04\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00024 to 0.00022, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8868e-05 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00022\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2063e-05 - val_loss: 3.0512e-04\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00022\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4727e-05 - val_loss: 7.9545e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00022\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1773e-05 - val_loss: 2.5116e-04\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00022\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5083e-05 - val_loss: 3.3314e-04\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00022\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7748e-05 - val_loss: 9.1895e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00022\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4989e-05 - val_loss: 2.1051e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_loss improved from 0.00022 to 0.00021, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.9888e-05 - val_loss: 2.8579e-04\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00021\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8459e-05 - val_loss: 3.0546e-04\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00021\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2075e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00021\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.0275e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00021\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4163e-05 - val_loss: 2.1151e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00021\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.5146e-05 - val_loss: 2.2897e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00021\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2764e-05 - val_loss: 2.1160e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00021\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7757e-05 - val_loss: 4.4198e-04\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00021\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3983e-05 - val_loss: 2.9311e-04\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00021\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2564e-05 - val_loss: 2.4994e-04\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00021\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5525e-05 - val_loss: 2.1500e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00021\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3348e-05 - val_loss: 1.9654e-04\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.00021 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6746e-05 - val_loss: 4.3668e-04\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00020\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1302e-05 - val_loss: 3.0091e-04\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00020\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6544e-05 - val_loss: 1.9703e-04\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00020\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.5692e-05 - val_loss: 6.5123e-04\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00020\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2961e-05 - val_loss: 5.6483e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00020\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5250e-05 - val_loss: 2.2505e-04\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00020\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2341e-05 - val_loss: 1.9296e-04\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.00020 to 0.00019, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5419e-05 - val_loss: 4.7558e-04\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00019\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3446e-05 - val_loss: 2.1128e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00019\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6496e-05 - val_loss: 1.9460e-04\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00019\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7811e-05 - val_loss: 1.9123e-04\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00019 to 0.00019, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8548e-05 - val_loss: 2.5110e-04\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00019\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5603e-05 - val_loss: 1.8976e-04\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00019 to 0.00019, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3325e-05 - val_loss: 5.1762e-04\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00019\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0786e-05 - val_loss: 4.8931e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00019\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1979e-05 - val_loss: 3.2330e-04\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00019\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0200e-05 - val_loss: 8.7652e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00019\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7285e-05 - val_loss: 2.0713e-04\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00019\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8988e-05 - val_loss: 6.7182e-04\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00019\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4268e-05 - val_loss: 2.6479e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00019\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4915e-05 - val_loss: 4.6897e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00019\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7007e-05 - val_loss: 3.4090e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00019\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0539e-05 - val_loss: 7.4500e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00019\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4419e-05 - val_loss: 1.9078e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00019\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7165e-05 - val_loss: 5.6148e-04\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00019\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3.8044e-05 - val_loss: 5.9129e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00019\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3492e-05 - val_loss: 2.0812e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00019\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7372e-05 - val_loss: 4.0874e-04\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00019\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3729e-05 - val_loss: 2.0525e-04\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00019\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4395e-05 - val_loss: 2.2401e-04\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00019\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4812e-05 - val_loss: 3.5225e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00019\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0583e-05 - val_loss: 2.3595e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00019\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0751e-05 - val_loss: 4.7000e-04\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00019\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4902e-05 - val_loss: 3.9163e-04\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00019\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4537e-05 - val_loss: 2.9913e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00019\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4718e-05 - val_loss: 4.3999e-04\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00019\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8647e-05 - val_loss: 3.2448e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00019\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0144e-05 - val_loss: 2.5946e-04\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00019\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3263e-05 - val_loss: 3.7572e-04\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00019\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4011e-05 - val_loss: 2.7164e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00019\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4699e-05 - val_loss: 2.2379e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00019\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7188e-05 - val_loss: 1.8225e-04\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.00019 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1962e-05 - val_loss: 3.6710e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00018\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9718e-05 - val_loss: 7.6145e-04\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00018\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3453e-05 - val_loss: 2.0848e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00018\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3.8944e-05 - val_loss: 2.0451e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00018\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9831e-05 - val_loss: 3.9537e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00018\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6089e-05 - val_loss: 3.4146e-04\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00018\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9050e-05 - val_loss: 6.1527e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00018\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1972e-05 - val_loss: 3.7664e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00018\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6140e-05 - val_loss: 4.6963e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00018\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1856e-05 - val_loss: 2.1237e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00018\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5702e-05 - val_loss: 2.5812e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00018\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2192e-05 - val_loss: 2.4550e-04\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00018\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8471e-05 - val_loss: 9.9438e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00018\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1817e-05 - val_loss: 1.7714e-04\n",
      "\n",
      "Epoch 00105: val_loss improved from 0.00018 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1099e-05 - val_loss: 1.6977e-04\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.00018 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3372e-05 - val_loss: 1.6904e-04\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.00017 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(4).hdf5\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2825e-05 - val_loss: 2.6053e-04\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00017\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5156e-05 - val_loss: 5.4237e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00017\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6514e-05 - val_loss: 1.6959e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00017\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5765e-05 - val_loss: 2.0651e-04\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00017\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0081e-05 - val_loss: 2.2192e-04\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00017\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1312e-05 - val_loss: 4.4330e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00017\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6454e-05 - val_loss: 2.5804e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00017\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1411e-05 - val_loss: 3.1905e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00017\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3019e-05 - val_loss: 3.8094e-04\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00017\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3673e-05 - val_loss: 2.5983e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00017\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3166e-05 - val_loss: 2.6116e-04\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00017\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3298e-05 - val_loss: 4.6378e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00017\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7380e-05 - val_loss: 2.0592e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00017\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7147e-05 - val_loss: 4.1937e-04\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00017\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0199e-05 - val_loss: 1.7765e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00017\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6697e-05 - val_loss: 3.0563e-04\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00017\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7091e-05 - val_loss: 4.8941e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00017\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0627e-05 - val_loss: 3.4112e-04\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00017\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4421e-05 - val_loss: 6.7546e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00017\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1810e-05 - val_loss: 3.9424e-04\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00017\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3031e-05 - val_loss: 2.4524e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00017\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3686e-05 - val_loss: 3.3176e-04\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00017\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2320e-05 - val_loss: 3.6343e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00017\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3509e-05 - val_loss: 6.1124e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00017\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1568e-05 - val_loss: 2.4457e-04\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00017\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8116e-05 - val_loss: 2.9811e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00017\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9747e-05 - val_loss: 4.6493e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00017\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0330e-05 - val_loss: 5.2868e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00017\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2728e-05 - val_loss: 2.5854e-04\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00017\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6080e-05 - val_loss: 7.6991e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00017\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2503e-05 - val_loss: 2.1661e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00017\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6418e-05 - val_loss: 2.9871e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00017\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0279e-05 - val_loss: 5.3396e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00017\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4129e-05 - val_loss: 2.7243e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00017\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2099e-05 - val_loss: 3.6088e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00017\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2507e-05 - val_loss: 3.0189e-04\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00017\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2213e-05 - val_loss: 6.4975e-04\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00017\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8214e-05 - val_loss: 2.7204e-04\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00017\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9212e-05 - val_loss: 3.0555e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00017\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2730e-05 - val_loss: 6.2191e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00017\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4035e-05 - val_loss: 2.2557e-04\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00017\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1665e-05 - val_loss: 2.2638e-04\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00017\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0376e-05 - val_loss: 2.9749e-04\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00017\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5347e-05 - val_loss: 1.9705e-04\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00017\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1768e-05 - val_loss: 5.9934e-04\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00017\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3312e-05 - val_loss: 2.2363e-04\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00017\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3797e-05 - val_loss: 6.6481e-04\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00017\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0794e-05 - val_loss: 2.9433e-04\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00017\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8212e-05 - val_loss: 4.6936e-04\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00017\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8665e-05 - val_loss: 3.7903e-04\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00017\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0837e-05 - val_loss: 3.9053e-04\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00017\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5219e-05 - val_loss: 3.6743e-04\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00017\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6584e-05 - val_loss: 3.7956e-04\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00017\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7683e-05 - val_loss: 2.9381e-04\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00017\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1116e-05 - val_loss: 2.4461e-04\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00017\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0748e-05 - val_loss: 1.9001e-04\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00017\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1356e-05 - val_loss: 2.8585e-04\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00017\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1727e-05 - val_loss: 8.3941e-04\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00017\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2285e-05 - val_loss: 2.1479e-04\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00017\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7770e-05 - val_loss: 1.8514e-04\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00017\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1704e-05 - val_loss: 2.1355e-04\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00017\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9135e-05 - val_loss: 1.8647e-04\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00017\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.5716e-05 - val_loss: 4.8947e-04\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00017\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1196e-05 - val_loss: 7.4148e-04\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00017\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7963e-05 - val_loss: 1.9648e-04\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00017\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2208e-05 - val_loss: 1.7728e-04\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00017\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2305e-05 - val_loss: 5.8532e-04\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00017\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0630e-05 - val_loss: 2.4405e-04\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00017\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4862e-05 - val_loss: 4.7423e-04\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00017\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8475e-05 - val_loss: 2.3659e-04\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00017\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6525e-05 - val_loss: 5.3741e-04\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00017\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1427e-05 - val_loss: 5.0072e-04\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00017\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2041e-05 - val_loss: 2.4190e-04\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00017\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9501e-05 - val_loss: 2.5715e-04\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00017\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3043e-05 - val_loss: 2.5517e-04\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00017\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5735e-05 - val_loss: 1.8696e-04\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00017\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4144e-05 - val_loss: 2.9822e-04\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00017\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1949e-05 - val_loss: 2.2740e-04\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00017\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1849e-05 - val_loss: 1.9172e-04\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00017\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9841e-05 - val_loss: 2.3400e-04\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00017\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0200e-05 - val_loss: 4.2070e-04\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00017\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4062e-05 - val_loss: 2.0202e-04\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00017\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0898e-05 - val_loss: 3.5161e-04\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00017\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9340e-05 - val_loss: 6.7798e-04\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00017\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0878e-05 - val_loss: 5.4699e-04\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00017\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1061e-05 - val_loss: 9.0641e-04\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00017\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2816e-05 - val_loss: 2.1686e-04\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00017\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9769e-05 - val_loss: 3.7989e-04\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00017\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6128e-05 - val_loss: 7.8324e-04\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00017\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7808e-05 - val_loss: 2.6996e-04\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00017\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0116e-05 - val_loss: 1.9294e-04\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00017\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1426e-05 - val_loss: 2.4178e-04\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00017\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5914e-05 - val_loss: 1.8698e-04\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00017\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(4).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ec3c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(4).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0adaedc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.017056068\n",
      "Test RMSE : 0.06182948127882918\n",
      "Test MAE : 0.042785965\n",
      "Test MSE : 0.0038228848\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002b225",
   "metadata": {},
   "source": [
    "## MTEX(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f34bc635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0131 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00159, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9129e-04 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00159 to 0.00150, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6230e-04 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00150 to 0.00140, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.3246e-04 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00140\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.9939e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00140 to 0.00119, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.6411e-04 - val_loss: 9.9894e-04\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00119 to 0.00100, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.4010e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00100\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.1185e-04 - val_loss: 9.0038e-04\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00100 to 0.00090, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.2634e-05 - val_loss: 8.8392e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00090 to 0.00088, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.0558e-05 - val_loss: 8.0882e-04\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00088 to 0.00081, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.9517e-05 - val_loss: 6.0295e-04\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00081 to 0.00060, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.6608e-05 - val_loss: 6.0179e-04\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00060 to 0.00060, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.3508e-04 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00060\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.2627e-05 - val_loss: 5.4496e-04\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00060 to 0.00054, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.6872e-05 - val_loss: 5.7897e-04\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00054\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.2371e-05 - val_loss: 4.9344e-04\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00054 to 0.00049, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.5720e-05 - val_loss: 5.0191e-04\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00049\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0175e-05 - val_loss: 5.1917e-04\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00049\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6046e-05 - val_loss: 6.7897e-04\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00049\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.0615e-05 - val_loss: 5.4628e-04\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00049\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.9232e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00049\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0481e-05 - val_loss: 5.7208e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00049\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0334e-05 - val_loss: 4.4688e-04\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00049 to 0.00045, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.4117e-05 - val_loss: 0.0026\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00045\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4671e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00045\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.3547e-05 - val_loss: 5.1061e-04\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00045\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.4632e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00045\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.8909e-05 - val_loss: 7.0473e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00045\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.0424e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00045\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0097e-05 - val_loss: 8.4628e-04\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00045\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.3099e-05 - val_loss: 5.2021e-04\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00045\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1123e-05 - val_loss: 9.0643e-04\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00045\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6991e-05 - val_loss: 4.0599e-04\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00045 to 0.00041, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7815e-05 - val_loss: 6.3403e-04\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00041\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1984e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00041\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1388e-05 - val_loss: 7.8173e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00041\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.5516e-05 - val_loss: 6.8780e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00041\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0561e-05 - val_loss: 3.7862e-04\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00041 to 0.00038, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0718e-05 - val_loss: 4.1226e-04\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00038\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8398e-05 - val_loss: 8.4830e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00038\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8440e-05 - val_loss: 3.9853e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00038\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3161e-05 - val_loss: 4.3848e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00038\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1467e-05 - val_loss: 4.5723e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00038\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0025e-05 - val_loss: 7.0700e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00038\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3337e-05 - val_loss: 5.3698e-04\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00038\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3487e-05 - val_loss: 6.3250e-04\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00038\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7252e-05 - val_loss: 7.2270e-04\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00038\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1895e-05 - val_loss: 4.3666e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00038\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.5835e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00038\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.1573e-04 - val_loss: 5.2376e-04\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00038\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4742e-05 - val_loss: 3.4017e-04\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.00038 to 0.00034, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4915e-05 - val_loss: 3.7081e-04\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00034\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5238e-05 - val_loss: 3.0010e-04\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.00034 to 0.00030, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0004e-05 - val_loss: 5.6562e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00030\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7403e-05 - val_loss: 4.7568e-04\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00030\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 3.9617e-05 - val_loss: 4.8845e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00030\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9231e-05 - val_loss: 6.6319e-04\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00030\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7627e-05 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00030\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7662e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00030\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0113e-05 - val_loss: 3.8106e-04\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00030\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9601e-05 - val_loss: 6.7030e-04\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00030\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0447e-05 - val_loss: 6.3359e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00030\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6763e-05 - val_loss: 4.3512e-04\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00030\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5406e-05 - val_loss: 4.2096e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00030\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4947e-05 - val_loss: 2.4087e-04\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00030 to 0.00024, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8963e-05 - val_loss: 3.3264e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00024\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2987e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00024\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.6739e-05 - val_loss: 8.0662e-04\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00024\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8753e-05 - val_loss: 3.8231e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00024\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6859e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00024\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4737e-05 - val_loss: 3.0612e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00024\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.5776e-05 - val_loss: 3.7241e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00024\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7819e-05 - val_loss: 9.8630e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00024\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0023e-05 - val_loss: 4.1938e-04\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00024\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3166e-05 - val_loss: 5.8915e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00024\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3.3107e-05 - val_loss: 3.9025e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00024\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8388e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00024\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2750e-05 - val_loss: 2.6264e-04\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00024\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7939e-05 - val_loss: 8.5400e-04\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00024\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9360e-05 - val_loss: 3.1950e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00024\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1626e-05 - val_loss: 3.3345e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00024\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3290e-05 - val_loss: 4.5026e-04\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00024\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5620e-05 - val_loss: 5.4628e-04\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00024\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0045e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00024\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1548e-05 - val_loss: 6.7298e-04\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00024\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0973e-05 - val_loss: 4.7243e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00024\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1563e-05 - val_loss: 4.1105e-04\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00024\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9790e-05 - val_loss: 6.4732e-04\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00024\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4920e-05 - val_loss: 5.5864e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00024\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1932e-05 - val_loss: 8.3449e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00024\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2340e-05 - val_loss: 4.7301e-04\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00024\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7445e-05 - val_loss: 4.9619e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00024\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0632e-05 - val_loss: 4.0806e-04\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00024\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0595e-05 - val_loss: 4.2558e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00024\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8100e-05 - val_loss: 5.7173e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00024\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1430e-05 - val_loss: 3.5713e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00024\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9377e-05 - val_loss: 5.5662e-04\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00024\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5224e-05 - val_loss: 9.7201e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00024\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6307e-05 - val_loss: 4.9097e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00024\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8816e-05 - val_loss: 6.4633e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00024\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5242e-05 - val_loss: 5.3397e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00024\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3625e-05 - val_loss: 4.4388e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00024\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2998e-05 - val_loss: 2.9844e-04\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00024\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1709e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00024\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4800e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00024\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6424e-05 - val_loss: 4.5514e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00024\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3543e-05 - val_loss: 5.1629e-04\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00024\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1954e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00024\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4637e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00024\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1690e-05 - val_loss: 4.3444e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00024\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6407e-05 - val_loss: 3.9836e-04\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00024\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6067e-05 - val_loss: 2.7485e-04\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00024\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2579e-05 - val_loss: 4.9888e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00024\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6108e-05 - val_loss: 4.4326e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00024\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5950e-05 - val_loss: 3.5797e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00024\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5605e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00024\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3295e-05 - val_loss: 5.7548e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00024\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0254e-05 - val_loss: 7.6216e-04\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00024\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0049e-05 - val_loss: 6.2670e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00024\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1008e-05 - val_loss: 9.5685e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00024\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0762e-05 - val_loss: 5.3351e-04\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00024\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1293e-05 - val_loss: 2.9562e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00024\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6039e-05 - val_loss: 5.2144e-04\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00024\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2323e-05 - val_loss: 4.1480e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00024\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4784e-05 - val_loss: 4.8588e-04\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00024\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6937e-05 - val_loss: 4.8401e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00024\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9824e-05 - val_loss: 2.1813e-04\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.00024 to 0.00022, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9802e-05 - val_loss: 3.3484e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00022\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3714e-05 - val_loss: 1.9656e-04\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.00022 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3807e-05 - val_loss: 2.1437e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00020\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3789e-05 - val_loss: 2.6968e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00020\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9327e-05 - val_loss: 7.9205e-04\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00020\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2763e-05 - val_loss: 3.0221e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00020\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4097e-05 - val_loss: 4.0440e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00020\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8433e-05 - val_loss: 2.5349e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00020\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2403e-05 - val_loss: 2.5986e-04\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00020\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3829e-05 - val_loss: 4.0919e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00020\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4653e-05 - val_loss: 5.3639e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00020\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1033e-05 - val_loss: 2.0096e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00020\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1852e-05 - val_loss: 3.1071e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00020\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0782e-05 - val_loss: 2.8884e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00020\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2054e-05 - val_loss: 8.2008e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00020\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9579e-05 - val_loss: 3.2853e-04\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00020\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5759e-05 - val_loss: 2.1563e-04\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00020\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0106e-05 - val_loss: 2.0706e-04\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00020\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4811e-05 - val_loss: 2.7629e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00020\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3371e-05 - val_loss: 2.8741e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00020\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2077e-05 - val_loss: 2.5138e-04\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00020\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8012e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00020\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 3.7052e-05 - val_loss: 3.8143e-04\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00020\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5500e-05 - val_loss: 4.7655e-04\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00020\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9497e-05 - val_loss: 2.4167e-04\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00020\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7629e-05 - val_loss: 3.0555e-04\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00020\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0159e-05 - val_loss: 3.3933e-04\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00020\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7246e-05 - val_loss: 4.7312e-04\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00020\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8618e-05 - val_loss: 2.8310e-04\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00020\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2142e-05 - val_loss: 2.3157e-04\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00020\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1268e-05 - val_loss: 2.9407e-04\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00020\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3792e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00020\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3541e-05 - val_loss: 7.6866e-04\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00020\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2631e-05 - val_loss: 3.7437e-04\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00020\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4435e-05 - val_loss: 3.9235e-04\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00020\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9464e-05 - val_loss: 1.7860e-04\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.00020 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0120e-05 - val_loss: 4.6561e-04\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00018\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4478e-05 - val_loss: 1.9157e-04\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00018\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1010e-05 - val_loss: 3.8196e-04\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00018\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7572e-05 - val_loss: 1.9114e-04\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00018\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9506e-05 - val_loss: 2.2090e-04\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00018\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9809e-05 - val_loss: 1.9262e-04\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00018\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2.9794e-05 - val_loss: 1.7799e-04\n",
      "\n",
      "Epoch 00170: val_loss improved from 0.00018 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(5).hdf5\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2256e-05 - val_loss: 1.9479e-04\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00018\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4138e-05 - val_loss: 1.8731e-04\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00018\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 3.1082e-05 - val_loss: 2.0757e-04\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00018\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8264e-05 - val_loss: 2.0317e-04\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00018\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4858e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00018\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9816e-05 - val_loss: 5.0829e-04\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00018\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6685e-05 - val_loss: 3.3520e-04\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00018\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9675e-05 - val_loss: 2.4214e-04\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00018\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3538e-05 - val_loss: 7.6821e-04\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00018\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 3.3625e-05 - val_loss: 1.8132e-04\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00018\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6877e-05 - val_loss: 1.9321e-04\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00018\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8370e-05 - val_loss: 7.2981e-04\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00018\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9574e-05 - val_loss: 2.8098e-04\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00018\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0153e-05 - val_loss: 3.9336e-04\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00018\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3950e-05 - val_loss: 2.6975e-04\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00018\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8930e-05 - val_loss: 2.6089e-04\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00018\n",
      "Epoch 187/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8881e-05 - val_loss: 3.4656e-04\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00018\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8350e-05 - val_loss: 3.7354e-04\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00018\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3727e-05 - val_loss: 2.8470e-04\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00018\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9259e-05 - val_loss: 2.9165e-04\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00018\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9712e-05 - val_loss: 2.3437e-04\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00018\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7295e-05 - val_loss: 1.9360e-04\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00018\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5894e-05 - val_loss: 3.9623e-04\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00018\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0138e-05 - val_loss: 2.0980e-04\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00018\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1815e-05 - val_loss: 3.8792e-04\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00018\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0401e-05 - val_loss: 3.2818e-04\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00018\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1452e-05 - val_loss: 2.1298e-04\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00018\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2700e-05 - val_loss: 5.8374e-04\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00018\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1024e-05 - val_loss: 1.9044e-04\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00018\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3.0980e-05 - val_loss: 3.8586e-04\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00018\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(5).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdc1d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(5).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "501ef5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.01753214\n",
      "Test RMSE : 0.0642410278139096\n",
      "Test MAE : 0.04446249\n",
      "Test MSE : 0.0041269097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecce386",
   "metadata": {},
   "source": [
    "## MTEX(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f38b35f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0200 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00187, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6107e-04 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00187\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.3806e-04 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00187 to 0.00158, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.0709e-04 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00158 to 0.00150, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.8120e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00150 to 0.00126, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.6076e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00126 to 0.00100, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.3398e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00100\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.1307e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00100\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.8781e-05 - val_loss: 7.7413e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00100 to 0.00077, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.3018e-05 - val_loss: 6.9197e-04\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00077 to 0.00069, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.5046e-05 - val_loss: 7.8310e-04\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00069\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.6509e-05 - val_loss: 6.1771e-04\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00069 to 0.00062, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.8740e-05 - val_loss: 7.3156e-04\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00062\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9067e-05 - val_loss: 6.5056e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00062\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1522e-05 - val_loss: 6.5001e-04\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00062\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9191e-05 - val_loss: 5.2145e-04\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00062 to 0.00052, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0676e-05 - val_loss: 5.5575e-04\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00052\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4125e-05 - val_loss: 5.2978e-04\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00052\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4379e-05 - val_loss: 6.2109e-04\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00052\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6840e-05 - val_loss: 6.9607e-04\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00052\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.0988e-05 - val_loss: 8.3099e-04\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00052\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7450e-05 - val_loss: 5.5466e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00052\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3855e-05 - val_loss: 6.9678e-04\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00052\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.3764e-05 - val_loss: 8.9264e-04\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00052\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2002e-05 - val_loss: 4.9229e-04\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00052 to 0.00049, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6834e-05 - val_loss: 5.0381e-04\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00049\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0409e-05 - val_loss: 6.4571e-04\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00049\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2586e-05 - val_loss: 6.2045e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00049\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.9348e-05 - val_loss: 6.6640e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00049\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9606e-05 - val_loss: 9.0283e-04\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00049\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1867e-05 - val_loss: 4.7697e-04\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00049 to 0.00048, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.3623e-05 - val_loss: 5.2383e-04\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00048\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4209e-05 - val_loss: 5.6237e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00048\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.0494e-05 - val_loss: 5.8941e-04\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00048\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6165e-05 - val_loss: 7.0381e-04\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00048\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4810e-05 - val_loss: 4.9082e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00048\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9074e-05 - val_loss: 3.7999e-04\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00048 to 0.00038, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.4020e-04 - val_loss: 8.8410e-04\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00038\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7551e-05 - val_loss: 4.3729e-04\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00038\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8374e-05 - val_loss: 3.9623e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00038\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.8952e-05 - val_loss: 8.9807e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00038\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4751e-05 - val_loss: 4.7991e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00038\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.4906e-05 - val_loss: 6.5227e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00038\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.5033e-05 - val_loss: 4.1885e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00038\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.6498e-05 - val_loss: 7.4955e-04\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00038\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5914e-05 - val_loss: 4.8439e-04\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00038\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5404e-05 - val_loss: 4.7802e-04\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00038\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3659e-05 - val_loss: 5.3260e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00038\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.8880e-05 - val_loss: 4.1078e-04\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00038\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.6077e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00038\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6643e-05 - val_loss: 6.9508e-04\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00038\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9389e-05 - val_loss: 3.7452e-04\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00038 to 0.00037, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.9050e-05 - val_loss: 5.1047e-04\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00037\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.1396e-05 - val_loss: 8.1226e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00037\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6172e-05 - val_loss: 4.3025e-04\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00037\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2284e-05 - val_loss: 3.9482e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00037\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9300e-05 - val_loss: 4.0938e-04\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00037\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5355e-05 - val_loss: 5.6011e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00037\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9860e-05 - val_loss: 3.5663e-04\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00037 to 0.00036, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5023e-05 - val_loss: 8.5534e-04\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00036\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1225e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00036\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.8255e-05 - val_loss: 3.9263e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00036\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4876e-05 - val_loss: 8.4831e-04\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00036\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0559e-05 - val_loss: 7.6795e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00036\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2402e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00036\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 6.9241e-05 - val_loss: 7.1312e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00036\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7712e-05 - val_loss: 7.7409e-04\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00036\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6586e-05 - val_loss: 2.9875e-04\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00036 to 0.00030, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8614e-05 - val_loss: 5.4637e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00030\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.8951e-05 - val_loss: 7.1084e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00030\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8462e-05 - val_loss: 8.2322e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00030\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8393e-05 - val_loss: 4.7820e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00030\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.5207e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00030\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9610e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00030\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2786e-05 - val_loss: 8.7523e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00030\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9275e-05 - val_loss: 9.9453e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00030\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6340e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00030\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8105e-05 - val_loss: 8.6342e-04\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00030\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9974e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00030\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7106e-05 - val_loss: 8.2729e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00030\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4775e-05 - val_loss: 7.1368e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00030\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0010e-05 - val_loss: 7.5079e-04\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00030\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2965e-05 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00030\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4402e-05 - val_loss: 3.8471e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00030\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 5.0081e-05 - val_loss: 4.2378e-04\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00030\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.4061e-05 - val_loss: 6.4819e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00030\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7565e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00030\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5040e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00030\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0734e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00030\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3029e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00030\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6860e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00030\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7972e-05 - val_loss: 5.1323e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00030\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.5297e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00030\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8508e-05 - val_loss: 7.3515e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00030\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0080e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00030\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3869e-05 - val_loss: 6.4010e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00030\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2575e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00030\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2080e-05 - val_loss: 4.9392e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00030\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2779e-05 - val_loss: 5.7459e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00030\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8031e-05 - val_loss: 8.1147e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00030\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0095e-05 - val_loss: 4.7080e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00030\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6541e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00030\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4589e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00030\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0670e-05 - val_loss: 3.6886e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00030\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3186e-05 - val_loss: 4.4868e-04\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00030\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3807e-05 - val_loss: 9.2411e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00030\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3181e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00030\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6613e-05 - val_loss: 9.0400e-04\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00030\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1817e-05 - val_loss: 8.8556e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00030\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5507e-05 - val_loss: 9.2877e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00030\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0531e-05 - val_loss: 8.1954e-04\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00030\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4187e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00030\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3054e-05 - val_loss: 5.9377e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00030\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1123e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00030\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2813e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00030\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3753e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00030\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7014e-05 - val_loss: 4.3401e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00030\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6741e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00030\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5796e-05 - val_loss: 4.4201e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00030\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.9557e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00030\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8913e-05 - val_loss: 4.4785e-04\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00030\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4690e-05 - val_loss: 8.5137e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00030\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7685e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00030\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5853e-05 - val_loss: 4.7322e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00030\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1009e-05 - val_loss: 5.5659e-04\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00030\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2474e-05 - val_loss: 8.5997e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00030\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5244e-05 - val_loss: 4.7098e-04\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00030\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3471e-05 - val_loss: 5.6446e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00030\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1194e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00030\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1266e-05 - val_loss: 6.0132e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00030\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7624e-05 - val_loss: 9.5309e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00030\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1359e-05 - val_loss: 8.9654e-04\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00030\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4768e-05 - val_loss: 7.6618e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00030\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3860e-05 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00030\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4221e-05 - val_loss: 3.7826e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00030\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9068e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00030\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3310e-05 - val_loss: 5.4048e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00030\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0965e-05 - val_loss: 4.5511e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00030\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5521e-05 - val_loss: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00030\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3833e-05 - val_loss: 6.4750e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00030\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9143e-05 - val_loss: 5.1503e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00030\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1349e-05 - val_loss: 7.3498e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00030\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9704e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00030\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1613e-05 - val_loss: 5.6847e-04\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00030\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8686e-05 - val_loss: 6.9260e-04\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00030\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8828e-05 - val_loss: 6.0724e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00030\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2346e-05 - val_loss: 9.7095e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00030\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4651e-05 - val_loss: 5.0089e-04\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00030\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5133e-05 - val_loss: 2.9524e-04\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.00030 to 0.00030, saving model to saved_weights/bitcoin_MTEX_onestep(6).hdf5\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8173e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00030\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6348e-05 - val_loss: 6.2372e-04\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00030\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4162e-05 - val_loss: 8.4380e-04\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00030\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7281e-05 - val_loss: 8.5837e-04\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00030\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3418e-05 - val_loss: 8.9672e-04\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00030\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1439e-05 - val_loss: 6.7716e-04\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00030\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2261e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00030\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4425e-05 - val_loss: 5.5414e-04\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00030\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3144e-05 - val_loss: 9.1735e-04\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00030\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5181e-05 - val_loss: 8.6219e-04\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00030\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4051e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00030\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1621e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00030\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2531e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00030\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6119e-05 - val_loss: 6.7131e-04\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00030\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1511e-05 - val_loss: 8.5125e-04\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00030\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1343e-05 - val_loss: 6.9330e-04\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00030\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3992e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00030\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5088e-05 - val_loss: 9.9217e-04\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00030\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2791e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00030\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6903e-05 - val_loss: 5.2273e-04\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00030\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1532e-05 - val_loss: 6.3979e-04\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00030\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3710e-05 - val_loss: 3.0421e-04\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00030\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0187e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00030\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1060e-05 - val_loss: 6.2847e-04\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00030\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2190e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00030\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2090e-05 - val_loss: 3.0957e-04\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00030\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1324e-05 - val_loss: 4.4533e-04\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00030\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6066e-05 - val_loss: 7.1263e-04\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00030\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1169e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00030\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4262e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00030\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4847e-05 - val_loss: 9.1841e-04\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00030\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0604e-05 - val_loss: 6.2879e-04\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00030\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5483e-05 - val_loss: 4.7814e-04\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00030\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2337e-05 - val_loss: 6.7545e-04\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00030\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.7422e-05 - val_loss: 5.9081e-04\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00030\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8900e-05 - val_loss: 7.8734e-04\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00030\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6145e-05 - val_loss: 5.6242e-04\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00030\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8739e-05 - val_loss: 6.0319e-04\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00030\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0374e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00030\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1185e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00030\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9430e-05 - val_loss: 7.3905e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00030\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2538e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00030\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7863e-05 - val_loss: 5.1613e-04\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00030\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.6852e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00030\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3174e-05 - val_loss: 8.6052e-04\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00030\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4267e-05 - val_loss: 7.9232e-04\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00030\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1752e-05 - val_loss: 7.8917e-04\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00030\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7269e-05 - val_loss: 5.6890e-04\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00030\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3430e-05 - val_loss: 8.8607e-04\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00030\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.7150e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00030\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9440e-05 - val_loss: 3.3900e-04\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00030\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(6).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c3639b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(6).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e1e31d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.016223548\n",
      "Test RMSE : 0.05872036446417071\n",
      "Test MAE : 0.04081203\n",
      "Test MSE : 0.0034480812\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c18329",
   "metadata": {},
   "source": [
    "## MTEX(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20b4abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 5ms/step - loss: 0.0186 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00113, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.9857e-04 - val_loss: 9.6273e-04\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00113 to 0.00096, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.8458e-04 - val_loss: 9.0047e-04\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00096 to 0.00090, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7523e-04 - val_loss: 8.3292e-04\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00090 to 0.00083, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.5613e-04 - val_loss: 8.0930e-04\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00083 to 0.00081, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4043e-04 - val_loss: 7.8831e-04\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00081 to 0.00079, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.2629e-04 - val_loss: 6.9673e-04\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00079 to 0.00070, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0880e-04 - val_loss: 6.4459e-04\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00070 to 0.00064, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0358e-04 - val_loss: 5.7963e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00064 to 0.00058, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 9.7286e-05 - val_loss: 7.7761e-04\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00058\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.7655e-05 - val_loss: 4.9695e-04\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00058 to 0.00050, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.8348e-05 - val_loss: 4.8678e-04\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00050 to 0.00049, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.6129e-05 - val_loss: 8.2536e-04\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00049\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.3235e-05 - val_loss: 5.3734e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00049\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.4186e-05 - val_loss: 4.6792e-04\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00049 to 0.00047, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.1224e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00047\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.7548e-05 - val_loss: 3.9694e-04\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00047 to 0.00040, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.1891e-05 - val_loss: 4.1736e-04\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00040\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.6858e-05 - val_loss: 9.5242e-04\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00040\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.5621e-05 - val_loss: 3.9784e-04\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00040\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2673e-05 - val_loss: 6.0818e-04\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00040\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.8867e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00040\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.4769e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00040\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.2340e-05 - val_loss: 9.4947e-04\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00040\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.3957e-05 - val_loss: 7.7018e-04\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00040\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.8324e-05 - val_loss: 5.1176e-04\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00040\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6839e-05 - val_loss: 6.4640e-04\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00040\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6442e-05 - val_loss: 6.8347e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00040\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.9993e-05 - val_loss: 6.3286e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00040\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.9033e-05 - val_loss: 4.3488e-04\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00040\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.0328e-05 - val_loss: 4.0773e-04\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00040\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8464e-05 - val_loss: 3.7508e-04\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.00040 to 0.00038, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7942e-05 - val_loss: 7.9958e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00038\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1468e-05 - val_loss: 3.2051e-04\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00038 to 0.00032, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7098e-05 - val_loss: 4.3095e-04\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00032\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1738e-05 - val_loss: 7.6826e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00032\n",
      "Epoch 37/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9014e-05 - val_loss: 4.9659e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00032\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5704e-05 - val_loss: 3.1361e-04\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.00032 to 0.00031, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6740e-05 - val_loss: 2.7754e-04\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00031 to 0.00028, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1971e-05 - val_loss: 7.0366e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00028\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4201e-05 - val_loss: 2.9295e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00028\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6074e-05 - val_loss: 3.2905e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00028\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7740e-05 - val_loss: 4.0656e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00028\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6962e-05 - val_loss: 3.9863e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00028\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2268e-05 - val_loss: 6.1935e-04\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00028\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8451e-05 - val_loss: 3.5488e-04\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00028\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0203e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00028\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6611e-05 - val_loss: 5.5462e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00028\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.0670e-05 - val_loss: 3.0855e-04\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00028\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4031e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00028\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9502e-05 - val_loss: 9.3507e-04\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00028\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6960e-05 - val_loss: 3.5787e-04\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00028\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.9706e-05 - val_loss: 9.5246e-04\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00028\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3386e-05 - val_loss: 5.7967e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00028\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5791e-05 - val_loss: 2.6498e-04\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00028 to 0.00026, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2419e-05 - val_loss: 2.8134e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00026\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6660e-05 - val_loss: 2.4067e-04\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00026 to 0.00024, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7013e-05 - val_loss: 5.4321e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00024\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0691e-05 - val_loss: 3.6837e-04\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00024\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9316e-05 - val_loss: 3.4097e-04\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00024\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8563e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00024\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.4042e-05 - val_loss: 4.9894e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00024\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8032e-05 - val_loss: 3.6031e-04\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00024\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7011e-05 - val_loss: 8.5502e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00024\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4993e-05 - val_loss: 7.7694e-04\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00024\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9881e-05 - val_loss: 3.4433e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00024\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7084e-05 - val_loss: 4.5657e-04\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00024\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1264e-05 - val_loss: 5.9738e-04\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00024\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4545e-05 - val_loss: 9.8280e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00024\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3664e-05 - val_loss: 6.3293e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00024\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4714e-05 - val_loss: 5.1362e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00024\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5627e-05 - val_loss: 9.6300e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00024\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7270e-05 - val_loss: 3.4378e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00024\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7325e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00024\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6146e-05 - val_loss: 5.4075e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00024\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3747e-05 - val_loss: 4.0835e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00024\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.0879e-05 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00024\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3690e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00024\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1132e-05 - val_loss: 4.7055e-04\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00024\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2271e-05 - val_loss: 3.4583e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00024\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4592e-05 - val_loss: 3.1100e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00024\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4677e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00024\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7251e-05 - val_loss: 6.4293e-04\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00024\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5932e-05 - val_loss: 4.7140e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00024\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8527e-05 - val_loss: 3.8242e-04\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00024\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1078e-05 - val_loss: 3.8085e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00024\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9680e-05 - val_loss: 5.1375e-04\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00024\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1589e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00024\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4041e-05 - val_loss: 3.5608e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00024\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9348e-05 - val_loss: 5.5797e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00024\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6966e-05 - val_loss: 6.0750e-04\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00024\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0598e-05 - val_loss: 7.4733e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00024\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0525e-05 - val_loss: 3.0748e-04\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00024\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6338e-05 - val_loss: 4.4200e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00024\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5396e-05 - val_loss: 9.1376e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00024\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9839e-05 - val_loss: 3.4654e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00024\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4830e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00024\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8983e-05 - val_loss: 6.5071e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00024\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1685e-05 - val_loss: 5.4765e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00024\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9928e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00024\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7536e-05 - val_loss: 2.0731e-04\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.00024 to 0.00021, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7410e-04 - val_loss: 2.1079e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00021\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1642e-05 - val_loss: 1.9863e-04\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.00021 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0818e-05 - val_loss: 2.6789e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00020\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3030e-05 - val_loss: 3.7717e-04\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00020\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5459e-05 - val_loss: 4.9383e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00020\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7493e-05 - val_loss: 1.9912e-04\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00020\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1631e-05 - val_loss: 1.8719e-04\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.00020 to 0.00019, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1564e-05 - val_loss: 2.5135e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00019\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1404e-05 - val_loss: 3.1935e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00019\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1436e-05 - val_loss: 1.8324e-04\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00019 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3216e-05 - val_loss: 4.2591e-04\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00018\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8680e-05 - val_loss: 3.4005e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00018\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1387e-05 - val_loss: 2.1909e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00018\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6065e-05 - val_loss: 2.9272e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00018\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2938e-05 - val_loss: 2.3551e-04\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00018\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5850e-05 - val_loss: 4.0134e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00018\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1587e-05 - val_loss: 1.7308e-04\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.00018 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1529e-05 - val_loss: 1.8943e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00017\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1804e-05 - val_loss: 3.6983e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00017\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0233e-05 - val_loss: 1.7288e-04\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.00017 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1016e-05 - val_loss: 4.4773e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00017\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5573e-05 - val_loss: 3.8927e-04\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00017\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9941e-05 - val_loss: 2.3886e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00017\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2289e-05 - val_loss: 2.3384e-04\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00017\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7591e-05 - val_loss: 1.8829e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00017\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9899e-05 - val_loss: 1.8434e-04\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00017\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1875e-05 - val_loss: 2.0274e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00017\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9703e-05 - val_loss: 2.1752e-04\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00017\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0930e-05 - val_loss: 1.8815e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00017\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3852e-05 - val_loss: 2.4744e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00017\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5340e-05 - val_loss: 2.0694e-04\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00017\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1357e-05 - val_loss: 2.6325e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00017\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0167e-05 - val_loss: 2.7345e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00017\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4816e-05 - val_loss: 3.1619e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00017\n",
      "Epoch 136/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8651e-05 - val_loss: 1.9650e-04\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00017\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0068e-05 - val_loss: 2.7638e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00017\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0853e-05 - val_loss: 2.4878e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00017\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5427e-05 - val_loss: 2.9376e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00017\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1214e-05 - val_loss: 4.8543e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00017\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3651e-05 - val_loss: 1.9357e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00017\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9273e-05 - val_loss: 3.0083e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00017\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0840e-05 - val_loss: 2.7225e-04\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00017\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1775e-05 - val_loss: 4.9877e-04\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00017\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3959e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00017\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3063e-05 - val_loss: 3.2273e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00017\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4652e-05 - val_loss: 2.6920e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00017\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8685e-05 - val_loss: 3.0194e-04\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00017\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8486e-05 - val_loss: 4.2219e-04\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00017\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1871e-05 - val_loss: 2.1940e-04\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00017\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3052e-05 - val_loss: 2.3000e-04\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00017\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1353e-05 - val_loss: 4.6974e-04\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00017\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2782e-05 - val_loss: 1.9381e-04\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00017\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2320e-05 - val_loss: 1.7218e-04\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.00017 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8631e-05 - val_loss: 2.9441e-04\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00017\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3785e-05 - val_loss: 1.9174e-04\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00017\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8489e-05 - val_loss: 1.6886e-04\n",
      "\n",
      "Epoch 00157: val_loss improved from 0.00017 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(7).hdf5\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0213e-05 - val_loss: 2.9604e-04\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00017\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1825e-05 - val_loss: 1.8737e-04\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00017\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1557e-05 - val_loss: 2.4880e-04\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00017\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9344e-05 - val_loss: 4.4234e-04\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00017\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1519e-05 - val_loss: 5.4932e-04\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00017\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1070e-05 - val_loss: 1.7786e-04\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00017\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9780e-05 - val_loss: 1.9814e-04\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00017\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9716e-05 - val_loss: 2.0236e-04\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00017\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9154e-05 - val_loss: 2.6798e-04\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00017\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1828e-05 - val_loss: 2.1724e-04\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00017\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1794e-05 - val_loss: 1.9651e-04\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00017\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0381e-05 - val_loss: 2.7055e-04\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00017\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8251e-05 - val_loss: 2.1413e-04\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00017\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7208e-05 - val_loss: 2.9496e-04\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00017\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0457e-05 - val_loss: 2.3267e-04\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00017\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9358e-05 - val_loss: 4.7219e-04\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00017\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3513e-05 - val_loss: 4.9353e-04\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00017\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8913e-05 - val_loss: 4.7473e-04\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00017\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2192e-05 - val_loss: 1.9108e-04\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00017\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9584e-05 - val_loss: 3.3475e-04\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00017\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8556e-05 - val_loss: 2.2067e-04\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00017\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1728e-05 - val_loss: 2.4017e-04\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00017\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2072e-05 - val_loss: 2.0586e-04\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00017\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0023e-05 - val_loss: 2.7838e-04\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00017\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.7486e-05 - val_loss: 2.1517e-04\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00017\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1618e-05 - val_loss: 2.0006e-04\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00017\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9034e-05 - val_loss: 3.2577e-04\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00017\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1562e-05 - val_loss: 3.4401e-04\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00017\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0649e-05 - val_loss: 2.7428e-04\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00017\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2080e-05 - val_loss: 6.5200e-04\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00017\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2091e-05 - val_loss: 5.4918e-04\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00017\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7026e-05 - val_loss: 4.4933e-04\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00017\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8108e-05 - val_loss: 2.8339e-04\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00017\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2101e-05 - val_loss: 4.5604e-04\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00017\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8572e-05 - val_loss: 2.1680e-04\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00017\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9836e-05 - val_loss: 2.7476e-04\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00017\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1432e-05 - val_loss: 1.9316e-04\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00017\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9563e-05 - val_loss: 1.9365e-04\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00017\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.7414e-05 - val_loss: 3.0959e-04\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00017\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8678e-05 - val_loss: 3.7266e-04\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00017\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1893e-05 - val_loss: 5.5456e-04\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00017\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6797e-05 - val_loss: 3.0576e-04\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00017\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9461e-05 - val_loss: 3.7410e-04\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00017\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(7).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c683b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(7).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97accbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.02007041\n",
      "Test RMSE : 0.09740856231120855\n",
      "Test MAE : 0.049821652\n",
      "Test MSE : 0.009488428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea557f",
   "metadata": {},
   "source": [
    "## MTEX(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34f9dace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_13 (Reshape)         (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0220 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00218, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6656e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00218 to 0.00116, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.0366e-04 - val_loss: 8.9115e-04\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00116 to 0.00089, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.5631e-04 - val_loss: 6.7261e-04\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00089 to 0.00067, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.2728e-04 - val_loss: 7.4034e-04\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00067\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.0649e-04 - val_loss: 4.5532e-04\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00067 to 0.00046, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.3252e-05 - val_loss: 4.3388e-04\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00046 to 0.00043, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.8039e-05 - val_loss: 3.5894e-04\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00043 to 0.00036, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.0200e-05 - val_loss: 3.4941e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00036 to 0.00035, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4199e-05 - val_loss: 3.0965e-04\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00035 to 0.00031, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.8697e-05 - val_loss: 4.1264e-04\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00031\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.6592e-05 - val_loss: 3.9908e-04\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00031\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.8402e-05 - val_loss: 3.1724e-04\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00031\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6314e-05 - val_loss: 2.9946e-04\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00031 to 0.00030, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6191e-05 - val_loss: 3.5689e-04\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00030\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1685e-05 - val_loss: 2.5722e-04\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00030 to 0.00026, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0778e-05 - val_loss: 2.5830e-04\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00026\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0514e-05 - val_loss: 2.5349e-04\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00026 to 0.00025, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.5508e-05 - val_loss: 2.8620e-04\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00025\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8386e-05 - val_loss: 2.4928e-04\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00025 to 0.00025, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.7001e-05 - val_loss: 5.3685e-04\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00025\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6034e-05 - val_loss: 2.4473e-04\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00025 to 0.00024, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9108e-05 - val_loss: 2.3381e-04\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00024 to 0.00023, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.9382e-05 - val_loss: 3.1180e-04\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00023\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.3931e-05 - val_loss: 2.5944e-04\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00023\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7249e-05 - val_loss: 2.3237e-04\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00023 to 0.00023, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1023e-05 - val_loss: 2.4276e-04\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00023\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.5060e-05 - val_loss: 9.8348e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00023\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3752e-05 - val_loss: 3.7856e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00023\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.9707e-05 - val_loss: 3.2534e-04\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00023\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1201e-05 - val_loss: 2.9604e-04\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00023\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7388e-05 - val_loss: 3.2628e-04\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00023\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7905e-05 - val_loss: 3.3021e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00023\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.9192e-05 - val_loss: 2.7734e-04\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00023\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0206e-05 - val_loss: 2.0191e-04\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00023 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2079e-05 - val_loss: 2.3034e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00020\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9698e-05 - val_loss: 2.5690e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00020\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2927e-05 - val_loss: 2.7494e-04\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00020\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9479e-05 - val_loss: 1.9681e-04\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00020 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8358e-05 - val_loss: 2.0466e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00020\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0938e-05 - val_loss: 2.2471e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00020\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2570e-05 - val_loss: 2.2781e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00020\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7606e-05 - val_loss: 1.9420e-04\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00020 to 0.00019, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0682e-05 - val_loss: 1.8636e-04\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00019 to 0.00019, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5855e-05 - val_loss: 3.2001e-04\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00019\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8117e-05 - val_loss: 2.6903e-04\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00019\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4132e-05 - val_loss: 2.0453e-04\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00019\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0796e-05 - val_loss: 1.7725e-04\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.00019 to 0.00018, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7457e-05 - val_loss: 4.5189e-04\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00018\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1918e-05 - val_loss: 2.4326e-04\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00018\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2793e-05 - val_loss: 2.0165e-04\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00018\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3664e-05 - val_loss: 1.7126e-04\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00018 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4788e-05 - val_loss: 1.9969e-04\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00017\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6673e-05 - val_loss: 2.5755e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00017\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.2220e-05 - val_loss: 5.5563e-04\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00017\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7766e-05 - val_loss: 1.7585e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00017\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5728e-05 - val_loss: 2.1555e-04\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00017\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8080e-05 - val_loss: 1.6683e-04\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.00017 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7232e-05 - val_loss: 2.0034e-04\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00017\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3199e-05 - val_loss: 1.6648e-04\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.00017 to 0.00017, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5196e-05 - val_loss: 2.6750e-04\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00017\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7634e-05 - val_loss: 2.1539e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00017\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4635e-05 - val_loss: 2.3887e-04\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00017\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7129e-05 - val_loss: 1.9582e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00017\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7438e-05 - val_loss: 1.8246e-04\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00017\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5324e-05 - val_loss: 1.9217e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00017\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2576e-05 - val_loss: 2.1920e-04\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00017\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6175e-05 - val_loss: 3.6330e-04\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00017\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9079e-05 - val_loss: 1.6652e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00017\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8167e-05 - val_loss: 4.1869e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00017\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5493e-05 - val_loss: 1.8360e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00017\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7295e-05 - val_loss: 1.5936e-04\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.00017 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6028e-05 - val_loss: 2.3042e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00016\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0480e-05 - val_loss: 1.7909e-04\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00016\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2565e-05 - val_loss: 1.6032e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00016\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0261e-05 - val_loss: 1.8284e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00016\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3129e-05 - val_loss: 1.6582e-04\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00016\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9215e-05 - val_loss: 1.6162e-04\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00016\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4897e-05 - val_loss: 4.1283e-04\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00016\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4553e-05 - val_loss: 1.6335e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00016\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1965e-05 - val_loss: 2.7799e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00016\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1683e-05 - val_loss: 2.1954e-04\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00016\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1427e-05 - val_loss: 3.8765e-04\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00016\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3213e-05 - val_loss: 3.5094e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00016\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.5090e-05 - val_loss: 1.5617e-04\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.00016 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2223e-05 - val_loss: 2.6401e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00016\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1005e-05 - val_loss: 1.5541e-04\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.00016 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4616e-05 - val_loss: 2.5902e-04\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00016\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4186e-05 - val_loss: 1.9844e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00016\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2544e-05 - val_loss: 8.2105e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00016\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1988e-05 - val_loss: 1.5567e-04\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00016\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3756e-05 - val_loss: 1.5822e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00016\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6726e-05 - val_loss: 1.5698e-04\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00016\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9810e-05 - val_loss: 1.8861e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00016\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4621e-05 - val_loss: 3.4736e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00016\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1610e-05 - val_loss: 3.8735e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00016\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5552e-05 - val_loss: 3.4835e-04\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00016\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4627e-05 - val_loss: 1.6844e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00016\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4673e-05 - val_loss: 2.0271e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00016\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1516e-05 - val_loss: 1.5878e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00016\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6129e-05 - val_loss: 1.8903e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00016\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3923e-05 - val_loss: 1.6955e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00016\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7536e-05 - val_loss: 2.1780e-04\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00016\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0179e-05 - val_loss: 1.5808e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00016\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.2454e-05 - val_loss: 1.6452e-04\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00016\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1852e-05 - val_loss: 2.3760e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00016\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3917e-05 - val_loss: 2.5581e-04\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00016\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0701e-05 - val_loss: 1.6181e-04\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00016\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1822e-05 - val_loss: 1.7814e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00016\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3548e-05 - val_loss: 1.5740e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00016\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1930e-05 - val_loss: 1.5521e-04\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.00016 to 0.00016, saving model to saved_weights/bitcoin_MTEX_onestep(8).hdf5\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8790e-05 - val_loss: 2.1786e-04\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00016\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4471e-05 - val_loss: 1.5815e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00016\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1274e-05 - val_loss: 2.7234e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00016\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9436e-05 - val_loss: 1.6429e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00016\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9638e-05 - val_loss: 2.1628e-04\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00016\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0191e-05 - val_loss: 1.6724e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00016\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3131e-05 - val_loss: 2.3433e-04\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00016\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0701e-05 - val_loss: 3.2687e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00016\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0905e-05 - val_loss: 1.9478e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00016\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8540e-05 - val_loss: 3.7897e-04\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00016\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4975e-05 - val_loss: 1.5916e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00016\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1755e-05 - val_loss: 1.5811e-04\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00016\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2879e-05 - val_loss: 2.0292e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00016\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5174e-05 - val_loss: 4.6943e-04\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00016\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8192e-05 - val_loss: 2.4186e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00016\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1367e-05 - val_loss: 2.1006e-04\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00016\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0859e-05 - val_loss: 2.9725e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00016\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3175e-05 - val_loss: 2.0330e-04\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00016\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3585e-05 - val_loss: 2.4667e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00016\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7957e-05 - val_loss: 1.7346e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00016\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5489e-05 - val_loss: 2.0801e-04\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00016\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2318e-05 - val_loss: 1.7568e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00016\n",
      "Epoch 134/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2579e-05 - val_loss: 2.4532e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00016\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8523e-05 - val_loss: 2.3034e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00016\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3732e-05 - val_loss: 1.8365e-04\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00016\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1459e-05 - val_loss: 1.7911e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00016\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2844e-05 - val_loss: 1.7011e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00016\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5856e-05 - val_loss: 2.0373e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00016\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0514e-05 - val_loss: 3.3855e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00016\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1201e-05 - val_loss: 1.7537e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00016\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2596e-05 - val_loss: 1.7257e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00016\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3239e-05 - val_loss: 2.5136e-04\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00016\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3.0817e-05 - val_loss: 2.0474e-04\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00016\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 3.3292e-05 - val_loss: 1.8477e-04\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00016\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6666e-05 - val_loss: 2.1035e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00016\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 3.3660e-05 - val_loss: 1.8454e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00016\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2134e-05 - val_loss: 2.6851e-04\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00016\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0236e-05 - val_loss: 2.2103e-04\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00016\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5244e-05 - val_loss: 3.0395e-04\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00016\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8658e-05 - val_loss: 1.6649e-04\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00016\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 2.9199e-05 - val_loss: 1.9906e-04\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00016\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3000e-05 - val_loss: 1.8925e-04\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00016\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2004e-05 - val_loss: 1.7655e-04\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00016\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7279e-05 - val_loss: 3.1242e-04\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00016\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4587e-05 - val_loss: 3.0573e-04\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00016\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 2.9165e-05 - val_loss: 2.0663e-04\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00016\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7594e-05 - val_loss: 1.6672e-04\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00016\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4808e-05 - val_loss: 6.0233e-04\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00016\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4729e-05 - val_loss: 1.9427e-04\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00016\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7395e-05 - val_loss: 2.1168e-04\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00016\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3470e-05 - val_loss: 2.0681e-04\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00016\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0549e-05 - val_loss: 1.9525e-04\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00016\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5110e-05 - val_loss: 1.7277e-04\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00016\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9412e-05 - val_loss: 1.6133e-04\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00016\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7039e-05 - val_loss: 2.1195e-04\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00016\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9863e-05 - val_loss: 1.6040e-04\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00016\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8094e-05 - val_loss: 1.7033e-04\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00016\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5200e-05 - val_loss: 1.6579e-04\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00016\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9409e-05 - val_loss: 1.6251e-04\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00016\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8438e-05 - val_loss: 1.6242e-04\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00016\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0857e-05 - val_loss: 1.7727e-04\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00016\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4790e-05 - val_loss: 1.7187e-04\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00016\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9610e-05 - val_loss: 2.0249e-04\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00016\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0760e-05 - val_loss: 1.7985e-04\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00016\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9056e-05 - val_loss: 1.8946e-04\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00016\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6888e-05 - val_loss: 2.9063e-04\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00016\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9698e-05 - val_loss: 1.7767e-04\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00016\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1639e-05 - val_loss: 1.6298e-04\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00016\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9409e-05 - val_loss: 1.6618e-04\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00016\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8221e-05 - val_loss: 1.9768e-04\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00016\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8845e-05 - val_loss: 1.6593e-04\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00016\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1931e-05 - val_loss: 1.6629e-04\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00016\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4760e-05 - val_loss: 6.1971e-04\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00016\n",
      "Epoch 185/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0503e-05 - val_loss: 1.6220e-04\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00016\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7016e-05 - val_loss: 1.7425e-04\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00016\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3708e-05 - val_loss: 1.9231e-04\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00016\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3149e-05 - val_loss: 1.6865e-04\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00016\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1048e-05 - val_loss: 1.7454e-04\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00016\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2401e-05 - val_loss: 2.6992e-04\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00016\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9155e-05 - val_loss: 1.9586e-04\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00016\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6367e-05 - val_loss: 3.2924e-04\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00016\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9983e-05 - val_loss: 2.5487e-04\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00016\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1258e-05 - val_loss: 1.7132e-04\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00016\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1586e-05 - val_loss: 1.7948e-04\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00016\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6936e-05 - val_loss: 1.8127e-04\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00016\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8243e-05 - val_loss: 1.8239e-04\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00016\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0003e-05 - val_loss: 1.8673e-04\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00016\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.5975e-05 - val_loss: 1.6707e-04\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00016\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3315e-05 - val_loss: 1.7558e-04\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00016\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(8).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5db12dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(8).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ba5e3538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.015750796\n",
      "Test RMSE : 0.07645098888333508\n",
      "Test MAE : 0.03909906\n",
      "Test MSE : 0.0058447537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a02404",
   "metadata": {},
   "source": [
    "## MTEX(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43fcdce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_19 (Reshape)         (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 5ms/step - loss: 0.0090 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00125, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.9805e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00125 to 0.00119, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.7189e-04 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00119 to 0.00103, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.4412e-04 - val_loss: 9.8323e-04\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00103 to 0.00098, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.1671e-04 - val_loss: 9.2339e-04\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00098 to 0.00092, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 1.0527e-04 - val_loss: 8.2379e-04\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00092 to 0.00082, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 9.1649e-05 - val_loss: 7.8004e-04\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00082 to 0.00078, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.2926e-05 - val_loss: 7.2965e-04\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00078 to 0.00073, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.3388e-05 - val_loss: 7.0207e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00073 to 0.00070, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 7.2870e-05 - val_loss: 7.4655e-04\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00070\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.9784e-05 - val_loss: 6.1119e-04\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00070 to 0.00061, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.4192e-05 - val_loss: 6.2256e-04\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00061\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.6565e-05 - val_loss: 6.0607e-04\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00061 to 0.00061, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.9794e-05 - val_loss: 5.3632e-04\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00061 to 0.00054, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.7173e-05 - val_loss: 7.0214e-04\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00054\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.6113e-05 - val_loss: 7.3136e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00054\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 5.1035e-05 - val_loss: 5.2586e-04\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00054 to 0.00053, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.8858e-05 - val_loss: 5.4082e-04\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00053\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.2889e-05 - val_loss: 5.2022e-04\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00053 to 0.00052, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.0521e-05 - val_loss: 5.0932e-04\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00052 to 0.00051, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1199e-05 - val_loss: 4.7939e-04\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00051 to 0.00048, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8896e-05 - val_loss: 4.6844e-04\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.00048 to 0.00047, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7059e-05 - val_loss: 4.9397e-04\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00047\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2935e-05 - val_loss: 5.6344e-04\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00047\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7293e-05 - val_loss: 7.1987e-04\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00047\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 6.3063e-05 - val_loss: 5.1765e-04\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00047\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.4228e-05 - val_loss: 4.8053e-04\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00047\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2183e-05 - val_loss: 5.2689e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00047\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5864e-05 - val_loss: 5.9572e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00047\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.4529e-05 - val_loss: 4.3993e-04\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.00047 to 0.00044, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1898e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00044\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 8.0703e-05 - val_loss: 4.6032e-04\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00044\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.9190e-05 - val_loss: 4.5159e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00044\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.6398e-05 - val_loss: 5.0106e-04\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00044\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1741e-05 - val_loss: 4.3307e-04\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00044 to 0.00043, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.4296e-05 - val_loss: 5.2658e-04\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00043\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4126e-05 - val_loss: 5.1343e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00043\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2885e-05 - val_loss: 4.4563e-04\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00043\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.1241e-05 - val_loss: 5.8339e-04\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00043\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.6921e-05 - val_loss: 9.9272e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00043\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6900e-05 - val_loss: 4.4195e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00043\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4413e-05 - val_loss: 4.4073e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00043\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0407e-05 - val_loss: 4.4351e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00043\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.7702e-05 - val_loss: 5.5608e-04\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00043\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.9854e-05 - val_loss: 4.8245e-04\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00043\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.8168e-05 - val_loss: 4.3105e-04\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00043 to 0.00043, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.9817e-05 - val_loss: 7.2186e-04\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00043\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5791e-05 - val_loss: 6.4829e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00043\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2317e-05 - val_loss: 5.8683e-04\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00043\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.0202e-05 - val_loss: 4.6147e-04\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00043\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7151e-05 - val_loss: 8.4459e-04\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00043\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1524e-05 - val_loss: 5.2800e-04\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00043\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7159e-05 - val_loss: 4.5458e-04\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00043\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8309e-05 - val_loss: 5.7991e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00043\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 7.6994e-05 - val_loss: 6.5710e-04\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00043\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2059e-05 - val_loss: 8.0695e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00043\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3497e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00043\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5674e-05 - val_loss: 5.4440e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00043\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3720e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00043\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3955e-05 - val_loss: 7.2856e-04\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00043\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5836e-05 - val_loss: 8.6910e-04\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00043\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1317e-05 - val_loss: 4.2642e-04\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.00043 to 0.00043, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3931e-05 - val_loss: 4.2365e-04\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00043 to 0.00042, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.4007e-05 - val_loss: 6.0427e-04\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00042\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2925e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00042\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1008e-05 - val_loss: 4.4324e-04\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00042\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 6.4235e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00042\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1573e-05 - val_loss: 9.3102e-04\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00042\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3890e-05 - val_loss: 4.5134e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00042\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7519e-05 - val_loss: 5.8841e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00042\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2435e-05 - val_loss: 8.0484e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00042\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7383e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00042\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8606e-05 - val_loss: 8.3509e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00042\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3970e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00042\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0834e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00042\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2435e-05 - val_loss: 8.6846e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00042\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2802e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00042\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.9536e-05 - val_loss: 5.1935e-04\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00042\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2802e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00042\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3495e-05 - val_loss: 4.6958e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00042\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5831e-05 - val_loss: 4.3414e-04\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00042\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4499e-05 - val_loss: 5.9238e-04\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00042\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0593e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00042\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5594e-05 - val_loss: 7.3362e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00042\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1192e-05 - val_loss: 5.9874e-04\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00042\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5336e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00042\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5651e-05 - val_loss: 7.3822e-04\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00042\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9071e-05 - val_loss: 2.8948e-04\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.00042 to 0.00029, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.7755e-05 - val_loss: 3.6736e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00029\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9186e-05 - val_loss: 5.2464e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00029\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2848e-05 - val_loss: 8.3381e-04\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00029\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2951e-05 - val_loss: 6.0229e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00029\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0577e-05 - val_loss: 3.2550e-04\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00029\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.6094e-05 - val_loss: 8.9264e-04\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00029\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7381e-05 - val_loss: 3.4463e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00029\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.3260e-05 - val_loss: 3.9683e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00029\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0206e-05 - val_loss: 3.7732e-04\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00029\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.1258e-05 - val_loss: 3.1409e-04\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00029\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.5348e-05 - val_loss: 5.3445e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00029\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9089e-05 - val_loss: 5.6005e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00029\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3744e-05 - val_loss: 8.8024e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00029\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9734e-05 - val_loss: 6.1994e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00029\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7307e-05 - val_loss: 4.0673e-04\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00029\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6913e-05 - val_loss: 9.9738e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00029\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5786e-05 - val_loss: 2.9970e-04\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00029\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2701e-05 - val_loss: 8.5572e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00029\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7901e-05 - val_loss: 4.6516e-04\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00029\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5488e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00029\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.8235e-05 - val_loss: 3.2555e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00029\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1083e-05 - val_loss: 4.8261e-04\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00029\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9601e-05 - val_loss: 9.9297e-04\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00029\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1561e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00029\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2003e-05 - val_loss: 5.1768e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00029\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0891e-05 - val_loss: 6.0478e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00029\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4231e-05 - val_loss: 5.5788e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00029\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6554e-05 - val_loss: 2.8289e-04\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.00029 to 0.00028, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4974e-05 - val_loss: 4.0685e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00028\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5324e-05 - val_loss: 3.5876e-04\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00028\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4271e-05 - val_loss: 9.6749e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00028\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8242e-05 - val_loss: 7.6303e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00028\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2800e-05 - val_loss: 5.7840e-04\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00028\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5767e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00028\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.2914e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00028\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5277e-05 - val_loss: 4.3894e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00028\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1354e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00028\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6806e-05 - val_loss: 6.1751e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00028\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9936e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00028\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9151e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00028\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8421e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00028\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1177e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00028\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5530e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00028\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7329e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00028\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8349e-05 - val_loss: 4.4024e-04\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00028\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.7825e-05 - val_loss: 5.4422e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00028\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3830e-05 - val_loss: 2.5779e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00135: val_loss improved from 0.00028 to 0.00026, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4777e-05 - val_loss: 4.8934e-04\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00026\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2613e-05 - val_loss: 4.9248e-04\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00026\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0833e-05 - val_loss: 6.4096e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00026\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0460e-05 - val_loss: 6.9721e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00026\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0112e-05 - val_loss: 4.6961e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00026\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1427e-05 - val_loss: 3.7128e-04\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00026\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4155e-05 - val_loss: 5.5333e-04\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00026\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.0530e-05 - val_loss: 8.3141e-04\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00026\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3385e-05 - val_loss: 2.8271e-04\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00026\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5881e-05 - val_loss: 6.8807e-04\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00026\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.4830e-05 - val_loss: 5.7776e-04\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00026\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0322e-05 - val_loss: 3.7168e-04\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00026\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8037e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00026\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6354e-05 - val_loss: 4.2263e-04\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00026\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1478e-05 - val_loss: 5.7905e-04\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00026\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2721e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00026\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 5.2668e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00026\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3114e-05 - val_loss: 4.9518e-04\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00026\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9472e-05 - val_loss: 2.5944e-04\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00026\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9177e-05 - val_loss: 6.6878e-04\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00026\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3102e-05 - val_loss: 2.6546e-04\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00026\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0447e-05 - val_loss: 5.5562e-04\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00026\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9058e-05 - val_loss: 4.2700e-04\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00026\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9159e-05 - val_loss: 3.4265e-04\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00026\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8961e-05 - val_loss: 5.4240e-04\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00026\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8170e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00026\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.6432e-05 - val_loss: 5.8406e-04\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00026\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2099e-05 - val_loss: 4.6173e-04\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00026\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1156e-05 - val_loss: 6.2359e-04\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00026\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5026e-05 - val_loss: 5.8307e-04\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00026\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0948e-05 - val_loss: 4.5683e-04\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00026\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1128e-05 - val_loss: 7.2166e-04\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00026\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0129e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00026\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.9383e-05 - val_loss: 4.9893e-04\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00026\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0369e-05 - val_loss: 3.8537e-04\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00026\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.7762e-05 - val_loss: 3.1813e-04\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00026\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8862e-05 - val_loss: 6.6087e-04\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00026\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2546e-05 - val_loss: 8.6875e-04\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00026\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1214e-05 - val_loss: 4.7034e-04\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00026\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1337e-05 - val_loss: 4.8818e-04\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00026\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.1381e-05 - val_loss: 1.9870e-04\n",
      "\n",
      "Epoch 00176: val_loss improved from 0.00026 to 0.00020, saving model to saved_weights/bitcoin_MTEX_onestep(9).hdf5\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5158e-05 - val_loss: 3.7822e-04\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00020\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.7567e-05 - val_loss: 5.4866e-04\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00020\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8097e-05 - val_loss: 2.4939e-04\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00020\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9115e-05 - val_loss: 6.2631e-04\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00020\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.4844e-05 - val_loss: 7.8397e-04\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00020\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0767e-05 - val_loss: 2.6010e-04\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00020\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1231e-05 - val_loss: 3.7254e-04\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00020\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3758e-05 - val_loss: 4.3505e-04\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00020\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0441e-05 - val_loss: 4.0378e-04\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00020\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8210e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00020\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0183e-05 - val_loss: 5.0489e-04\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00020\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9814e-05 - val_loss: 4.2126e-04\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00020\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8129e-05 - val_loss: 3.2872e-04\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00020\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 4.1139e-05 - val_loss: 6.5898e-04\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00020\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0054e-05 - val_loss: 9.5957e-04\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00020\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.8502e-05 - val_loss: 5.0250e-04\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00020\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.0231e-05 - val_loss: 6.1401e-04\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00020\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.7916e-05 - val_loss: 2.4109e-04\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00020\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.5502e-05 - val_loss: 9.2846e-04\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00020\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2524e-05 - val_loss: 6.4114e-04\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00020\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.8761e-05 - val_loss: 9.4404e-04\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00020\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.2226e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00020\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 3.3731e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00020\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 2.9568e-05 - val_loss: 5.9992e-04\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00020\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(9).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1247a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(9).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b3a6dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.021348638\n",
      "Test RMSE : 0.06990010799148408\n",
      "Test MAE : 0.05381504\n",
      "Test MSE : 0.004886025\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ced1b",
   "metadata": {},
   "source": [
    "## MTEX(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "374ebb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 24, 6, 1)\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 24, 6, 1)]        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 24, 6, 16)         144       \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 24, 6, 32)         2080      \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 24, 6, 1)          33        \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 24, 6)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 24, 64)            1216      \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                49184     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 52,690\n",
      "Trainable params: 52,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0101 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00184, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.2682e-04 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00184 to 0.00118, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.9662e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00118 to 0.00112, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.6756e-04 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00112\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.4504e-04 - val_loss: 9.8059e-04\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00112 to 0.00098, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.2747e-04 - val_loss: 9.5147e-04\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00098 to 0.00095, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.1858e-04 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00095\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 1.0605e-04 - val_loss: 8.3873e-04\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00095 to 0.00084, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.1817e-05 - val_loss: 9.1257e-04\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00084\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 9.3555e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00084\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.5141e-05 - val_loss: 8.7287e-04\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00084\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.1358e-05 - val_loss: 7.5275e-04\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00084 to 0.00075, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.5547e-05 - val_loss: 7.5373e-04\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00075\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.6723e-05 - val_loss: 7.7205e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00075\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.3593e-05 - val_loss: 7.0111e-04\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00075 to 0.00070, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.1267e-05 - val_loss: 8.8407e-04\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00070\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.4874e-05 - val_loss: 6.8929e-04\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.00070 to 0.00069, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.0758e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00069\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.6315e-05 - val_loss: 6.7344e-04\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00069 to 0.00067, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.3557e-05 - val_loss: 6.5250e-04\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00067 to 0.00065, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.8021e-05 - val_loss: 8.0528e-04\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00065\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0801e-05 - val_loss: 8.4996e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00065\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1530e-05 - val_loss: 9.7413e-04\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00065\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.2293e-05 - val_loss: 5.9751e-04\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.00065 to 0.00060, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1295e-05 - val_loss: 8.6672e-04\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00060\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9685e-05 - val_loss: 5.6411e-04\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.00060 to 0.00056, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1737e-05 - val_loss: 7.4034e-04\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00056\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7418e-05 - val_loss: 6.5982e-04\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00056\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0473e-05 - val_loss: 6.5249e-04\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00056\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.8714e-05 - val_loss: 6.1470e-04\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00056\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.9952e-05 - val_loss: 5.3216e-04\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.00056 to 0.00053, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3375e-05 - val_loss: 7.0710e-04\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00053\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4202e-05 - val_loss: 5.7242e-04\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00053\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 8.2813e-05 - val_loss: 9.8753e-04\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00053\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3596e-05 - val_loss: 5.1716e-04\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00053 to 0.00052, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.1169e-05 - val_loss: 7.5594e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00052\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.2358e-05 - val_loss: 7.5901e-04\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00052\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2190e-05 - val_loss: 5.6399e-04\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00052\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4633e-05 - val_loss: 4.4824e-04\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00052 to 0.00045, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0038e-05 - val_loss: 6.0667e-04\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00045\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3768e-05 - val_loss: 4.5122e-04\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00045\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 4.3151e-05 - val_loss: 4.7704e-04\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00045\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7177e-05 - val_loss: 4.5224e-04\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00045\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.6471e-05 - val_loss: 4.3669e-04\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00045 to 0.00044, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5114e-05 - val_loss: 4.2302e-04\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00044 to 0.00042, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1462e-05 - val_loss: 4.0237e-04\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.00042 to 0.00040, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5085e-05 - val_loss: 4.0767e-04\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00040\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1712e-05 - val_loss: 4.1715e-04\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00040\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3417e-05 - val_loss: 5.3407e-04\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00040\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.0518e-05 - val_loss: 4.4534e-04\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00040\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.8143e-05 - val_loss: 8.8685e-04\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00040\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2614e-05 - val_loss: 4.6612e-04\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.00040\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.4974e-05 - val_loss: 4.1288e-04\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00040\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4372e-05 - val_loss: 4.7900e-04\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00040\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0621e-05 - val_loss: 3.7944e-04\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.00040 to 0.00038, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8738e-05 - val_loss: 4.8332e-04\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00038\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3549e-05 - val_loss: 3.7912e-04\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.00038 to 0.00038, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0287e-05 - val_loss: 6.5365e-04\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00038\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7934e-05 - val_loss: 3.4037e-04\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.00038 to 0.00034, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 7.6351e-05 - val_loss: 3.6911e-04\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00034\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5831e-05 - val_loss: 3.4482e-04\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00034\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3769e-05 - val_loss: 4.7761e-04\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00034\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3784e-05 - val_loss: 3.3998e-04\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.00034 to 0.00034, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2400e-05 - val_loss: 3.3853e-04\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.00034 to 0.00034, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7539e-05 - val_loss: 3.3541e-04\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.00034 to 0.00034, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8703e-05 - val_loss: 3.2766e-04\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.00034 to 0.00033, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.3878e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00033\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7832e-05 - val_loss: 6.5404e-04\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.00033\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0815e-05 - val_loss: 9.3572e-04\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00033\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 4.5012e-05 - val_loss: 4.0478e-04\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00033\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0533e-05 - val_loss: 4.2327e-04\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00033\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2696e-05 - val_loss: 7.4780e-04\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00033\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7927e-05 - val_loss: 5.5520e-04\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00033\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.4196e-05 - val_loss: 9.6598e-04\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00033\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8746e-05 - val_loss: 3.7047e-04\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00033\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1224e-05 - val_loss: 6.7588e-04\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00033\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.0095e-05 - val_loss: 4.4103e-04\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00033\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 6.4078e-05 - val_loss: 3.8939e-04\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00033\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9749e-05 - val_loss: 4.0371e-04\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00033\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8691e-05 - val_loss: 5.6169e-04\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00033\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9478e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00033\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.1904e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00033\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1777e-05 - val_loss: 3.8649e-04\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00033\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6935e-05 - val_loss: 7.1157e-04\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00033\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6190e-05 - val_loss: 5.5896e-04\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00033\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7634e-05 - val_loss: 3.3561e-04\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00033\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1353e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00033\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9539e-05 - val_loss: 3.3719e-04\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00033\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7532e-05 - val_loss: 6.0387e-04\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00033\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0468e-05 - val_loss: 3.6962e-04\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00033\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6360e-05 - val_loss: 5.8731e-04\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00033\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2340e-05 - val_loss: 3.9309e-04\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00033\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2682e-05 - val_loss: 3.4257e-04\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00033\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5104e-05 - val_loss: 2.7162e-04\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.00033 to 0.00027, saving model to saved_weights/bitcoin_MTEX_onestep(10).hdf5\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2227e-05 - val_loss: 2.9954e-04\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00027\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8073e-05 - val_loss: 8.9009e-04\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00027\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2406e-05 - val_loss: 3.7749e-04\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00027\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.3891e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00027\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6417e-05 - val_loss: 5.0484e-04\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00027\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2787e-05 - val_loss: 3.8465e-04\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00027\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8992e-05 - val_loss: 4.8835e-04\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00027\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7233e-05 - val_loss: 4.0547e-04\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00027\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8543e-05 - val_loss: 8.1552e-04\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00027\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8731e-05 - val_loss: 4.1772e-04\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00027\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6464e-05 - val_loss: 3.3720e-04\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00027\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.5784e-05 - val_loss: 2.9682e-04\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00027\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.2940e-05 - val_loss: 4.8483e-04\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00027\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8532e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00027\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1076e-05 - val_loss: 3.8314e-04\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00027\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0215e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00027\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9247e-05 - val_loss: 7.2033e-04\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00027\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1436e-05 - val_loss: 3.0554e-04\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00027\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5160e-05 - val_loss: 8.2795e-04\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00027\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3516e-05 - val_loss: 4.7485e-04\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00027\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3944e-05 - val_loss: 4.1487e-04\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00027\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8538e-05 - val_loss: 7.7932e-04\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00027\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2644e-05 - val_loss: 8.3611e-04\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00027\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.8842e-05 - val_loss: 5.2740e-04\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00027\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2966e-05 - val_loss: 6.5257e-04\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.00027\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1960e-05 - val_loss: 3.3972e-04\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.00027\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7515e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.00027\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5188e-05 - val_loss: 3.7319e-04\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.00027\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 3.3321e-05 - val_loss: 7.1524e-04\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.00027\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9742e-05 - val_loss: 6.2989e-04\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.00027\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7991e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.00027\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1980e-05 - val_loss: 6.8772e-04\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.00027\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4672e-05 - val_loss: 6.3349e-04\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.00027\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.3981e-05 - val_loss: 3.8279e-04\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.00027\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6607e-05 - val_loss: 4.4633e-04\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.00027\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2592e-05 - val_loss: 6.1943e-04\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.00027\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7749e-05 - val_loss: 6.1159e-04\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.00027\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7172e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.00027\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9452e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.00027\n",
      "Epoch 134/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0834e-05 - val_loss: 6.1898e-04\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.00027\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0151e-05 - val_loss: 3.1269e-04\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.00027\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.6996e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.00027\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2445e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.00027\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2346e-05 - val_loss: 8.8645e-04\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.00027\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9866e-05 - val_loss: 8.7249e-04\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.00027\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8945e-05 - val_loss: 6.7598e-04\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.00027\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5055e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.00027\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4330e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.00027\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 5.7715e-05 - val_loss: 7.2753e-04\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.00027\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9621e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.00027\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.7913e-05 - val_loss: 8.3261e-04\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.00027\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0201e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.00027\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3855e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.00027\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1072e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.00027\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3547e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.00027\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.8140e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.00027\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3140e-05 - val_loss: 7.0691e-04\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.00027\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4277e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.00027\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2869e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.00027\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6159e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.00027\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9632e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.00027\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5043e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.00027\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5642e-05 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.00027\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6151e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.00027\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9786e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.00027\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0492e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.00027\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7235e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.00027\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 4.8639e-05 - val_loss: 8.8402e-04\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.00027\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6349e-05 - val_loss: 5.2462e-04\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.00027\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7717e-05 - val_loss: 9.6375e-04\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.00027\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3347e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.00027\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9165e-05 - val_loss: 0.0022\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.00027\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8210e-05 - val_loss: 9.2376e-04\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.00027\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1189e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.00027\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9186e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.00027\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9993e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.00027\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.6988e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.00027\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1278e-05 - val_loss: 0.0010\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.00027\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.9014e-05 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.00027\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5992e-05 - val_loss: 0.0030\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.00027\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.2175e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.00027\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2634e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.00027\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.8734e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.00027\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4836e-05 - val_loss: 0.0021\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.00027\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.1087e-05 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.00027\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9913e-05 - val_loss: 7.9231e-04\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.00027\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9482e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.00027\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.3461e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.00027\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7354e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.00027\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5636e-05 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.00027\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.5070e-05 - val_loss: 0.0018\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.00027\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2037e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.00027\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1417e-05 - val_loss: 0.0013\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.00027\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.9137e-05 - val_loss: 0.0025\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.00027\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0484e-05 - val_loss: 7.1460e-04\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.00027\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.7513e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.00027\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9357e-05 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.00027\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 3.2615e-05 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.00027\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.0842e-05 - val_loss: 0.0012\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.00027\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.4254e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.00027\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.7802e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.00027\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.6182e-05 - val_loss: 0.0017\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.00027\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.2632e-05 - val_loss: 0.0015\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.00027\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 2.9094e-05 - val_loss: 0.0016\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.00027\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 4.0433e-05 - val_loss: 0.0014\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.00027\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 3.1356e-05 - val_loss: 0.0019\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.00027\n"
     ]
    }
   ],
   "source": [
    "first_input = Input(shape=(24,6,1))\n",
    "\n",
    "in0=Conv2D(filters=16, kernel_size=(24//3,1),input_shape=(24,6,1), activation='relu',padding='same')(first_input)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "in1=Conv2D(filters=32, kernel_size=(24//5,1), activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "in1=Conv2D(filters=1, kernel_size=1, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "print(in1.shape)\n",
    "in1 = Reshape((24,6))(in1)\n",
    "in0=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in1)\n",
    "#in1= BatchNormalization(scale=False)(in1)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=2, activation='relu',padding='same')(in0)\n",
    "#in0= BatchNormalization(scale=False)(in0)\n",
    "#in1=Conv1D(filters=64, kernel_size=3, activation='relu',padding='same')(in0)\n",
    "\n",
    "#in1=Conv1D(filters=64, kernel_size=1, activation='relu',padding='same',name='extractor')(in1)\n",
    "#in1=add([in0,in1],name='extractor')\n",
    "#attn_layer = AttentionLayer(name='attention_layer')\n",
    "#attn_out, attn_states = attn_layer([in1,in1])\n",
    "\n",
    "    # Concat attention input and LSTM output, in original code it was decoder LSTM\n",
    "#concat_out = Concatenate(axis=-1, name='concat_layer')([in1, attn_out])\n",
    "#in1=MaxPooling1D(pool_size=2,name='extractor')(in1)\n",
    "\n",
    "#in1=add([in0,in1])\n",
    "in1=Flatten()(in0)\n",
    "#in1=Dense(50, activation='relu')(in1)\n",
    "#in1=LSTM(64)(in1)\n",
    "in1 = Dense(32,activation='relu')(in1)\n",
    "\n",
    "out=Dense(1)(in1)\n",
    "model=tf.keras.Model(inputs=[first_input],outputs=[out])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "checkpoint_path = \"saved_weights/bitcoin_MTEX_onestep(10).hdf5\"\n",
    "cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_best_only=True,\n",
    "                                        monitor='val_loss',mode='min',\n",
    "                                                 verbose=1)\n",
    "model.summary()\n",
    "# fit model9\n",
    "# history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78c3a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"saved_weights/bitcoin_MTEX_onestep(10).hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "355f154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAPE:  0.016643522\n",
      "Test RMSE : 0.05927973959227297\n",
      "Test MAE : 0.041648235\n",
      "Test MSE : 0.0035140875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "from math import *\n",
    "predict = model.predict(testX)\n",
    "test_rmse = sqrt(mean_squared_error(testY, predict))\n",
    "predicted = model.predict(testX)\n",
    "test_mae = mean_absolute_error(testY, predicted)\n",
    "test_mse = mean_squared_error(testY, predicted)\n",
    "mape=mean_absolute_percentage_error(testY, predicted)\n",
    "print('Test MAPE: ', mape)\n",
    "print('Test RMSE :', test_rmse)\n",
    "print('Test MAE :', test_mae)\n",
    "print('Test MSE :', test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07542681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b312a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83554d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(valX, valY), verbose=1, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a154d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962a48a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c676ccf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad08c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce994b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b57639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5a725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
